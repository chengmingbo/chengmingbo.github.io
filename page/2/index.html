<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chengmingbo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Mingbo">
<meta property="og:type" content="website">
<meta property="og:title" content="Mingbo">
<meta property="og:url" content="http://chengmingbo.github.io/page/2/index.html">
<meta property="og:site_name" content="Mingbo">
<meta property="og:description" content="Mingbo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Mingbo Cheng">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://chengmingbo.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Mingbo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mingbo</h1>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="/%5Bobject%20Object%5D" alt="Mingbo">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-slides"><a href="/slides/" rel="section"><i class="area-chart fa-fw"></i>slides</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mingbo Cheng</p>
  <div class="site-description" itemprop="description">Mingbo</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chengmingbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chengmingbo" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://deeplearningmath.org/supervised-machine-learning" title="https:&#x2F;&#x2F;deeplearningmath.org&#x2F;supervised-machine-learning" rel="noopener" target="_blank">deeplearningmath</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://lilianweng.github.io/archives/" title="https:&#x2F;&#x2F;lilianweng.github.io&#x2F;archives&#x2F;" rel="noopener" target="_blank">Lil'Log</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://arjun-sarkar786.medium.com/" title="https:&#x2F;&#x2F;arjun-sarkar786.medium.com&#x2F;" rel="noopener" target="_blank">Arjun Sarkar</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.zybuluo.com/codeep/note/163962" title="https:&#x2F;&#x2F;www.zybuluo.com&#x2F;codeep&#x2F;note&#x2F;163962" rel="noopener" target="_blank">mathjax grammar</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vividfree.github.io/" title="http:&#x2F;&#x2F;vividfree.github.io&#x2F;" rel="noopener" target="_blank">vividfree</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://colah.github.io/" title="http:&#x2F;&#x2F;colah.github.io&#x2F;" rel="noopener" target="_blank">colah</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.autonlab.org/tutorials" title="https:&#x2F;&#x2F;www.autonlab.org&#x2F;tutorials" rel="noopener" target="_blank">Andrew Moore</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://plot.ly/matlab/plot/" title="https:&#x2F;&#x2F;plot.ly&#x2F;matlab&#x2F;plot&#x2F;" rel="noopener" target="_blank">matlabplot</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.ryanzhang.info/blog/" title="http:&#x2F;&#x2F;www.ryanzhang.info&#x2F;blog&#x2F;" rel="noopener" target="_blank">Ryan’s Cabinet</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.cnblogs.com/jerrylead/tag/Machine%20Learning/" title="http:&#x2F;&#x2F;www.cnblogs.com&#x2F;jerrylead&#x2F;tag&#x2F;Machine%20Learning&#x2F;" rel="noopener" target="_blank">JerryLead</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://yxzf.github.io/" title="https:&#x2F;&#x2F;yxzf.github.io&#x2F;" rel="noopener" target="_blank">YXZF'S BLOG</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vonng.com/" title="http:&#x2F;&#x2F;vonng.com" rel="noopener" target="_blank">VONNG</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/05/21/decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/21/decision-tree/" class="post-title-link" itemprop="url">Decision Tree (ID3)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-21 15:10:50" itemprop="dateCreated datePublished" datetime="2017-05-21T15:10:50+02:00">2017-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Alogorithm/" itemprop="url" rel="index"><span itemprop="name">Alogorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="preface">Preface</h2>
<p>In April 9th, 2017, incident occurred in United Airlines where crew
of UA beat up a passenger and dragged him out of the plane before which
was about to take off attracted attention all around the world. Many
would gave out doubt: why a company being so rude to passengers can
exist in this world? Actually, UA is going well is just because they
have an extremely precise emergency situation procedure which is
calculate by compute depending on big-data analysis. Computer can help
us make decisions though, it has no emotions, which is effective in most
cases, but can not be approved by our human beings. Let's take a look at
how algorithm make a decision: <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-United%20Airlines.png" />
It is a decision tree, which simply represents the procedure of how UA
algorithm make the decision. First of all, before taking off, four
employees of UA need fly from Chicago to Kentucky. Then the algorithm
check if there is any seats left, if so, passengers were safe for the
moment. But UA3411 was full, the algorithm began assessing the
importance of employees or passengers. Obviously, the algorithm think
crew is more important due to business consideration. Then how to choose
who should be evicted from the plane. The algorithm was more complicated
than the tree I drew, however, Asian or not was one of the criterion.
But why? Because Asian are pushovers. The passenger agreed at first,
however, when he heard that he had to wait for one day, he realized that
he could not treat his patient, then he refused. Then he was beat up and
dragged off the plane.</p>
<p>As you have seen, it is a decision tree, which is similar to human
decision-making process. Decision tree is a simple but powerful
algorithm in machine learning. In fact, you are often using decision
tree theory when making decision, for example <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-homework.png" /></p>
<h2 id="introduction">Introduction</h2>
<p>Decision tree is a classification and regression algorithm, we build
a tree through statistics. Today we only talk about how to classify
dataset using Decision Tree. First we will introduce some information
theory background knowledge, then we use iris data build a decision tree
using IDC3 algorithm.</p>
<h2 id="iris-data">Iris data</h2>
<p><a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Iris">Iris
dataset</a> is a very famous dataset deposited on UCI machine learning
repository, which described three kinds of iris. there are four columns
corresponding for features as followed： * sepal length in cm * sepal
width in cm * petal length in cm * petal width in cm</p>
<p>The last column represents iris categories:</p>
<ul>
<li>Iris-setosa (n=50)</li>
<li>Iris-versicolor (n=50)</li>
<li>Iris-virginica (n=50)</li>
</ul>
<p>Here, our task is to use the dataset to train a model and generate a
decision tree. During the process we need calculate some statistics
values to decide how to generate a better one.</p>
<p>The dataset is very small so that you can easily download it and take
a look.</p>
<h2 id="entropy-and-information-gain">Entropy and Information Gain</h2>
<h4 id="entropy">Entropy</h4>
<p>Before Decision Tree, I'd like to talk about some concept in
Information Theory. Entropy is a concept from thermodynamics at first,
C.E.Shannon introduced which into information theory which represent
redundancy in 1948. It sounds a very strange concept. In fact, it is
very easy to understand. For example, during the knockout stages in
world Cup Games, there are 16 teams. Now I let you guess which team will
win the champion which assume I know the answer, how many times do you
need to get the outcome? First of all, you cut 16 teams to 8-8 parts,
you asked me if the team in first 8 teams or the other. I told you that
the team was in the other 8 teams. Then you cut the the 8 teams again,
you ask me if the team is in the first 4 teams or the other, I told you
that the champion would be in the first 4 teams, and so forth and so on.
And how many times is the entropy of who wining the champion.</p>
<p><span class="math display">\[ Entropy(champion) = {\rm log}_2^{16}=4
\]</span></p>
<p>That is, we can use 4 bits to represents which team will win the
game. Clever you may ask why we divide team to two parts other than
three or four parts. That is because we use binary represents the world
in computer world. $ 2^4=16 $ means we can use 4 bit represents 16
conditions. We can use entropy represent all information in this world.
And if you have known that which team will win the campion, the entropy
is 0, because, you do not need any more information to deduce the
outcome.</p>
<p>Entropy represents uncertainty indeed. Ancient China, we have to
record history on bamboo slips, which demanded us decrease words. That
means entropy of every single ancient Chinese character is higher than
words we are saying today. That is, if we lost just some of these words,
we would lose lots of stories. There are many songs starts with:"Yoo,
yoo, check now", which barely offer us information, which means we can
drop those words and interpret the these songs precisely as well. The
entropy of these sentence is low.</p>
<p>Assume <span class="math inline">\(X\)</span> is discrete random
variable, the distribution is: <span
class="math display">\[P(X=x_i)=p_i\]</span> then the entropy of X is:
<span class="math display">\[H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2
p_i\]</span> where if p_0=0, we define 0log0 = 0.</p>
<p>It seems that the equation has nothing to do with the entropy we have
calculated in the champion example. Now let's calculate the example.
First of all <span class="math inline">\(X\)</span> represents the
probability of each team which would win the game. we assume all teams
were at the same level, so we have <span
class="math display">\[p(X=x_1)=p(X=x_2)=p(X=x_3)=\cdots =
p(X=x_{16})=\frac{1}{16}\]</span> the entropy is <span
class="math display">\[H(X)=-\sum_{i=1}^{16}\frac{1}{16}{\rm log}_2
\frac{1}{16}=-16\times\frac{1}{16}\times {\rm log}_2
{2^{-4}}=4\]</span></p>
<p>Bingo, the the answer is same. In fact, if we know some more
information, the entropy is lower than 4. for example, the probability
of Germany is higher than some Asian teams. #### Entropy and Iris Data
Now we calculate entropy of Iris Data which will be used to fit a
decision tree in following sections. We concern about the
categories(setosa, versicolor and virginica). Remember the equation of
how to calculate entropy: <span
class="math display">\[H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2
p_i\]</span></p>
<p>Three kinds of flowers are all 50s, so the probability of each
category is the same: <span
class="math display">\[p_1=p_2=p_3=\frac{50}{50+50+50}=\frac{1}{3}\]</span>
Then, the entropy is pretty easy to calculate <span
class="math display">\[H(X)=-1\times (\frac{1}{3}{\rm
log}_2\frac{1}{3}+\frac{1}{3}{\rm log}_2\frac{1}{3}+\frac{1}{3}{\rm
log}_2\frac{1}{3})=1.5850\]</span> #### Conditional Entropy The meaning
of Conditional Entropy is as its name. With respect with random
variable<span class="math inline">\((X, Y)\)</span>, the joint
distribution is <span class="math display">\[P(X=x_i, Y=y_j)=p_{ij},
i=1,2,3\cdots m; j=1,2,3,\cdots n\]</span> Conditional Entropy H(Y|X)
represents that given we have known random variable <span
class="math inline">\(X\)</span> , the disorder or uncertainty of <span
class="math inline">\(Y\)</span>. The definition is as followed: <span
class="math display">\[H(Y|X)=\sum_{i=1}^m p_i H(Y|X=x_i)\]</span> Here,
<span class="math inline">\(p_i=P(X=x_i)\)</span>.</p>
<h4 id="conditional-entropy-and-iris-data">Conditional Entropy and Iris
Data</h4>
<p>We calculate some Conditional Entropy as examples. First of all, I
random choose 15 columns of sepal length with respect to their
categories. the result is as followed：</p>
<table>
<thead>
<tr class="header">
<th>No.</th>
<th>sepal length in cm</th>
<th>categories</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>2</td>
<td>7.20</td>
<td>Iris-virginica</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5.00</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>4</td>
<td>5.00</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>5</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.70</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>7</td>
<td>5.20</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.50</td>
<td>Iris-versicolor</td>
</tr>
<tr class="odd">
<td>9</td>
<td>4.80</td>
<td>Iris-setosa</td>
</tr>
<tr class="even">
<td>10</td>
<td>4.60</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>11</td>
<td>6.50</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>12</td>
<td>5.20</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>13</td>
<td>7.70</td>
<td>Iris-virginica</td>
</tr>
<tr class="even">
<td>14</td>
<td>6.40</td>
<td>Iris-virginica</td>
</tr>
<tr class="odd">
<td>15</td>
<td>6.00</td>
<td>Iris-versicolor</td>
</tr>
</tbody>
</table>
<p>The octave code is <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%% octave</span><br><span class="line">[a,b,c,d, cate] = textread(&quot;iris.data&quot;, &quot;%f%f%f%f%s&quot;,&quot;delimiter&quot;, &quot;,&quot;);</span><br><span class="line"></span><br><span class="line">for i=1:15</span><br><span class="line">  x = floor(rand()*150);</span><br><span class="line">  fprintf(&#x27;%f %s\n&#x27;, a(x), cate&#123;x&#125; );</span><br><span class="line">end</span><br></pre></td></tr></table></figure> We just take this 15 items for
examples, I assume that we divide sepal length into two parts: greater
than mean and less than mean. The mean is <span
class="math display">\[mean = (5.90+7.2+\cdots+6.00)/15 =
5.7733\]</span> There are 8 elements less then 5.7733 and 7 bigger ones.
That is</p>
<table>
<thead>
<tr class="header">
<th>mean</th>
<th>idx of greater than mean</th>
<th>idx of less than mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5.7733</td>
<td>1,2,5,11,13,14,15</td>
<td>3,4,6,7,8,9,10,12</td>
</tr>
</tbody>
</table>
<p>We let <span
class="math inline">\(x_1=greater\)</span>(1,2,5,11,13,14,15), <span
class="math inline">\(x_2=less\)</span>(3,4,6,7,8,9,10,12) then <span
class="math display">\[H(Y|X=x_1)=-(p_1 {\rm log}_2 p_1 + p_2 {\rm
log}_2 p_2 + p_3 {\rm log}_2 p_3)=\frac{4}{7}{\rm
log}_2\frac{4}{7}+\frac{3}{7}{\rm log}_2\frac{3}{7}+0{\rm log}_2
0=0.98523\]</span> <span class="math display">\[H(Y|X=x_2)=-(p_1 {\rm
log}_2 p_1 + p_2 {\rm log}_2 p_2+p_3 {\rm log}_2 p_3)=\frac{3}{8}{\rm
log}_2\frac{3}{8}+0{\rm log}_2 0+\frac{5}{8}{\rm
log}_2\frac{5}{8}=0.95443\]</span></p>
<p>The Conditional Entropy then is <span
class="math display">\[H(Y|X)=\sum_{i=1}^{2}p_i
H(Y|x_i)=\frac{7}{15}\times 0.98523+\frac{8}{15}\times
0.95443=0.96880\]</span> #### Information Gain Just as its name implies,
Information Gain means the information we have gained after adding some
features. That is, we can vanish some uncertainty when we add some
information. For example, I want you to guess an NBA player, the
uncertainty is very high, however, there are only several persons in the
list if I tell you that he is a Chinese. You gained information after
knowing the Chinese feature to decrease the uncertainty. The calculation
of Information Gain is <span class="math display">\[IG(Y, X)=
H(Y)-H(Y|X)\]</span> Here, we want to decide <span
class="math inline">\(Y\)</span> with feature <span
class="math inline">\(X\)</span>. It is easy, just Entropy of <span
class="math inline">\(Y\)</span> minus Conditional Entropy <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(X\)</span>. The meaning is obvious too: <span
class="math inline">\(H(Y)\)</span> represents uncertainty, <span
class="math inline">\(H(Y|X)\)</span> represents uncertainty of <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(X\)</span>, the difference is the Information
Gain. #### Information Gain and Iris Data In this section, I will apply
Information Gain equations to the whole Iris data. First of all, let
<span class="math inline">\(Y\)</span> represent categories of iris, and
<span class="math inline">\(X_1,X_2,X_3, X_4\)</span> represent sepal
length, sepal width, petal length petal width respectively.</p>
<p>We have computed that <span
class="math inline">\(H(Y)=1.0986\)</span>, next, we will calculate 4
Conditional Entropy <span
class="math inline">\(H(Y|X_1),H(Y|X_2),H(Y|X_3),H(Y|X_4)\)</span>. In
light of continuousness of <span class="math inline">\(X\)</span>, we
divide them by mean of each feature. Then <span
class="math display">\[\overline{X_1}=5.8433,\,\overline{X_2}=3.0540,\,\overline{X_3}=3.7587,\,\overline{X_4}=1.1987\]</span></p>
<p><span class="math display">\[H(Y|X_1)=-\sum_{i=1}^3 p_i
H(Y|X_{1i})=-(\frac{70}{150}(\frac{0}{70}{\rm
log}_2\frac{0}{70}+\frac{26}{70}{\rm log}_2\frac{26}{70}
+\frac{44}{70}{\rm log}_2\frac{44}{70})+\frac{80}{150}(\frac{50}{80}{\rm
log}_2\frac{50}{80}+\frac{24}{80}{\rm
log}_2\frac{24}{80}+\frac{6}{80}{\rm
log}_2\frac{6}{80}))=1.09757\]</span></p>
<p><span class="math display">\[H(Y|X_2)=-\sum_{i=1}^3 p_i
H(Y|X_{2i})=-(\frac{67}{150}(\frac{42}{67}{\rm
log}_2\frac{42}{67}+\frac{8}{67}{\rm
log}_2\frac{8}{67}+\frac{17}{67}{\rm
log}_2\frac{17}{67}+\frac{83}{150}(\frac{8}{83}{\rm
log}_2\frac{8}{83}+\frac{42}{83}{\rm
log}_2\frac{42}{83}+\frac{33}{83}{\rm
log}_2\frac{33}{83}))=1.32433\]</span></p>
<p><span class="math display">\[H(Y|X_3)=-\sum_{i=1}^3 p_i
H(Y|X_{3i})=-(\frac{93}{150}(\frac{0}{93}{\rm
log}_2\frac{0}{93}+\frac{43}{93}{\rm
log}_2\frac{43}{93}+\frac{50}{93}{\rm
log}_2\frac{50}{93}+\frac{57}{150}(\frac{50}{57}{\rm
log}_2\frac{50}{57}+\frac{7}{57}{\rm log}_2\frac{7}{57}+\frac{0}{57}{\rm
log}_2\frac{0}{57}))=0.821667\]</span></p>
<p><span class="math display">\[H(Y|X_4)=-\sum_{i=1}^3 p_i
H(Y|X_{4i})=-(\frac{90}{150}(\frac{0}{90}{\rm
log}_2\frac{0}{90}+\frac{40}{90}{\rm
log}_2\frac{40}{90}+\frac{50}{90}{\rm
log}_2\frac{50}{90}+\frac{60}{150}(\frac{50}{60}{\rm
log}_2\frac{50}{60}+\frac{10}{60}{\rm
log}_2\frac{10}{60}+\frac{0}{60}{\rm log}_2\frac{0}{60}))=0.854655
\]</span> Information Gains is easy to get <span
class="math display">\[IG(Y,
X_1)=H(Y)-H(Y|X_1)=1.5850-1.09757=0.487427\]</span></p>
<p><span class="math display">\[IG(Y,
X_2)=H(Y)-H(Y|X_2)=1.5850-1.32433=0.260669\]</span></p>
<p><span class="math display">\[IG(Y,
X_3)=H(Y)-H(Y|X_3)=1.5850-0.821667=0.763333\]</span></p>
<p><span class="math display">\[IG(Y,
X_4)=H(Y)-H(Y|X_4)=1.5850-0.854655=0.730345\]</span> By now, we find
that <span class="math inline">\(IG(Y, X_3)\)</span> is bigger than
others, which means feature <span class="math inline">\(X_3\)</span>
supplies more information.</p>
<h2 id="id3iterative-dichotomiser-3">ID3(Iterative Dichotomiser 3)</h2>
<p>ID3 algorithm was developed by Ross Quinlan in 1986, which is a very
classic algorithm as well as C4.5 and CART. We First apply Information
Gain of each feature with respect to iris data. Then to choose the
maximum to divide data into 2 parts. For each part we apply Information
Gain recursively until we put all parents data to one node. Now that we
have know Information Gain from the last section, obviously we choose X3
as the feature dividing data into 2 parts in the first place.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-17-2.png" /></p>
<p>Let's take a look at the first cut using feature <span
class="math inline">\(X_3\)</span>. We have 150 items at first, after
comparing if <span class="math inline">\(X_3&gt;3.7587\)</span>, we
divide data into two parts, one has 93 items, the other got 57. From the
data, we know that there is no setosa in node B, meanwhile, no virginica
in node C, which means that this feature is very good for split data due
to exclude setosa and virginica.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Node B</th>
<th>Node C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>setosa</td>
<td>0</td>
<td>50</td>
</tr>
<tr class="even">
<td>versicolor</td>
<td>43</td>
<td>7</td>
</tr>
<tr class="odd">
<td>virginica</td>
<td>50</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The end condition of the algorithm is decided by IG. When IG is less
then some threshold or if there is only one category left, we can end
the algorithm. If IG less than some value(e.g. 0.01) and more than one
category left simultaneously, we have to choose a final category to be
the leaf, the rule is to set the category having samples more than the
others.</p>
<p>Take Node H for example, we set IG threshold to 0.01 in the first
place. Then we calculate the Information Gain for each feature, the
biggest IG from feature 2(sepal width in cm), which is 0.003204 and less
than 0.01. So we have to set H as a leaf. There are 0 Iris-setosa, 25
Iris-versicolor and 44 Iris-virginica in the leaf, so we set the bigger
one(i.e. Iris-virginica) to the leaf.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-19-2.png" /></p>
<h2 id="summary">Summary</h2>
<p>Today we have talked about what is decision tree algorithm. Firstly,
I introduce three background concept Entropy, Conditional Entropy and
Information Gain. Next we apply ID3 algorithm to Iris data to build a
decision.</p>
<p>One of the most significant advantages of decision tree is that we
can explain the result. If the algorithm decided UA should beat the
their passengers, they could trace the tree to find the path of reason
chain. It is very useful to tell consumers why we recommend them
something, under such circumstance, we can use decision tree to train a
model.</p>
<p>There is a shortcoming that Information Gain tends to use feature
with more values. In order to resolve the problem, Ross Quinlan improved
the algorithm through Information Gain Rate Rather than IG. <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Leo_Breiman">Breiman</a> introduced
CART algorithm subsequently, which can be applied to classification as
well as regression. Recently, Scientists have developed more powerful
algorithm such as Random Forest and Gradient Boosting Decision Tree
etc.</p>
<h2 id="reference">Reference</h2>
<ol type="1">
<li>《统计学习方法》，李航</li>
<li>《数学之美》，吴军</li>
<li>http://www.shogun-toolbox.org/static/notebook/current/DecisionTrees.html</li>
<li>https://en.wikipedia.org/</li>
</ol>
<h2 id="appendix-code">Appendix code</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%% octave main function file</span><br><span class="line">%% iris data dowload link: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</span><br><span class="line">[a,b,c,d, cate] = textread(&quot;iris.data&quot;, &quot;%f%f%f%f%s&quot;,&quot;delimiter&quot;, &quot;,&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%for i=1:15</span><br><span class="line">%	x = floor(rand()*150);</span><br><span class="line">%	fprintf(&#x27;%f %s\n&#x27;, a(x), cate&#123;x&#125; );</span><br><span class="line">%end;</span><br><span class="line"></span><br><span class="line">features = [a, b, c, d];</span><br><span class="line">for i=1:length(features(1, :))</span><br><span class="line">	col = features(:, i);</span><br><span class="line">	me = mean(col);</span><br><span class="line">	disp(me);</span><br><span class="line">	feat(i).greater = find(col &gt; me);</span><br><span class="line">	feat(i).less = find(col &lt;= me);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">total = (1:150)&#x27;;</span><br><span class="line">decision(feat, length(features(1, :)), cate, total);</span><br><span class="line">fprintf(&#x27;\n&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">%% octave: decsion tree file</span><br><span class="line">function decision(feat, feat_size, cate, total)</span><br><span class="line">	if length(total) == 0</span><br><span class="line">		return</span><br><span class="line">	end</span><br><span class="line">	fprintf(&#x27;(-%d-)&#x27;, length(total));</span><br><span class="line">	%plogp = @(x)[x*log2(x)];</span><br><span class="line">	function e = plogp(pi)</span><br><span class="line">		if pi == 0</span><br><span class="line">			e = 0;</span><br><span class="line">		else</span><br><span class="line">			e = pi*log2(pi);</span><br><span class="line">		end</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	function d = div(a, b)</span><br><span class="line">		if b == 0</span><br><span class="line">			d = 0;</span><br><span class="line">		else</span><br><span class="line">			d = a/b;</span><br><span class="line">		end</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	debug = 0;</span><br><span class="line"></span><br><span class="line">	function m = maxc(cate, cates, total)</span><br><span class="line">		maxidx = 1;</span><br><span class="line">		max_c = 0;</span><br><span class="line">		for i=1:length(cates)</span><br><span class="line">			c =find(strcmp(cate, cates&#123;i&#125;));</span><br><span class="line">			cl = length(intersect(c, total));</span><br><span class="line">			if debug == 1 fprintf(&#x27;\n%d##%d  %s###&#x27;,i, cl, char(cates&#123;i&#125;)) end</span><br><span class="line">			%if (debug == 1 &amp;&amp; cl &lt;10 &amp;&amp; cl &gt;0) disp(intersect(c, total)&#x27;) end</span><br><span class="line">			if cl &gt; max_c</span><br><span class="line">				max_c = cl;</span><br><span class="line">				maxidx = i;</span><br><span class="line">			end</span><br><span class="line">		end</span><br><span class="line">		if debug == 1 fprintf(&#x27;\n****%d    %d******\n&#x27;, maxidx, max_c) end</span><br><span class="line">		%m = cates(maxidx);</span><br><span class="line">		m = maxidx;</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	% compute h(y)</span><br><span class="line">	cates = unique(cate);</span><br><span class="line">	hx = 0;</span><br><span class="line">	for i = 1:length(cates)</span><br><span class="line">		c = find(strcmp(cate, cates&#123;i&#125;));</span><br><span class="line">		rc = intersect(c, total);</span><br><span class="line">		hx -= plogp(length(rc)/length(total));</span><br><span class="line">	end</span><br><span class="line">	%fprintf(&#x27;hx = %f\n&#x27;, hx)			</span><br><span class="line">	% compute h(y|x)</span><br><span class="line">	max_feature = 1;</span><br><span class="line">	max_ig = 0;</span><br><span class="line"></span><br><span class="line">	max_left = intersect(feat(1).greater, total);</span><br><span class="line">	max_right = intersect(feat(1).less, total);</span><br><span class="line">	for i=1:feat_size</span><br><span class="line">		hxh = 0;</span><br><span class="line">		hxl = 0;</span><br><span class="line">		feat_greater = intersect(feat(i).greater, total);</span><br><span class="line">		feat_less = intersect(feat(i).less, total);</span><br><span class="line">		ge = length(feat_greater);</span><br><span class="line">		le = length(feat_less);</span><br><span class="line"></span><br><span class="line">		if (ge+le) == 0</span><br><span class="line">			continue</span><br><span class="line">		end</span><br><span class="line"></span><br><span class="line">		for j = 1:length(cates);</span><br><span class="line">			c = find(strcmp(cate, cates&#123;j&#125;));</span><br><span class="line">			xh = length(intersect(feat_greater, c));</span><br><span class="line">			xl = length(intersect(feat_less, c));</span><br><span class="line">			hxh -= plogp(div(xh, ge));</span><br><span class="line">			hxl -= plogp(div(xl, le));</span><br><span class="line">		end</span><br><span class="line">		% compute hx - h(y|x)</span><br><span class="line">		hxy = (ge/(ge+le))*hxh + ((le)/(ge+le))*hxl;</span><br><span class="line">		ig = hx - hxy;</span><br><span class="line"></span><br><span class="line">		if ig &gt; max_ig</span><br><span class="line">			max_ig = ig;</span><br><span class="line">			max_feature = i;</span><br><span class="line">			max_left= feat_less;</span><br><span class="line">			max_right = feat_greater;</span><br><span class="line">		end</span><br><span class="line"></span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	left = max_left;</span><br><span class="line">	right = max_right;</span><br><span class="line">	%fprintf(&#x27;feature:ig  %d %f %d %d ------ \n&#x27;, max_feature, max_ig, length(left), length(right));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	if debug == 1 printf(&quot;\033[0;32;1m-ig--%f \033[0m&quot;,  max_ig); end</span><br><span class="line">	if(max_ig &lt; 0.01)</span><br><span class="line">		%fprintf(&#x27;&lt;%s&gt;&#x27;, char(maxc(cate, cates, total)))</span><br><span class="line">		printf(&quot;\033[0;31;1m&lt;%d&gt;\033[0m&quot;,  maxc(cate, cates, total));</span><br><span class="line">		return</span><br><span class="line">	end</span><br><span class="line">	fprintf(&quot;\033[0;34;1m#%d \033[0m&quot;,  max_feature);</span><br><span class="line">	fprintf(&#x27;&#123;&#x27; )</span><br><span class="line">	decision(feat, feat_size, cate, left);</span><br><span class="line">	decision(feat, feat_size, cate, right);</span><br><span class="line">	fprintf(&#x27;&#125;&#x27;)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/" class="post-title-link" itemprop="url">A Tutorial on Singular Value Decomposition</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-01 21:49:00" itemprop="dateCreated datePublished" datetime="2017-05-01T21:49:00+02:00">2017-05-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Alogorithm/" itemprop="url" rel="index"><span itemprop="name">Alogorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="preface">Preface</h3>
<p>Under some circumstance, we want to compress data to save storage
space. For example, when iPhone7 was released, many were trapped in a
dilemma: Should I buy a 32G iPhone without enough free space or that of
128G with a lot of storage being wasted? I had been trapped in such
dilemma indeed. I still remember that I only had 8G storage totally when
I was using my first Android phone. What annoyed me most was my
thousands of photos. Well, I confess that I was being always a mad
picture taker. I knew that there were some technique which could
compress a picture through reducing pixel. However, it is not enough,
because, as you know, in some arbitrary position in a picture, we can
tell that the picture share the same color. An extreme Example: if we
have a pure color picture, what we just need know is the RGB value and
the size, then reproducing the picture is done without extra effort.
What I was dreaming is done perfectly by Singular Value
Decomposition(SVD).</p>
<h3 id="introduction">Introduction</h3>
<p>Before SVD, in this article, I will introduce some mathmatical
concepts in the first place which cover Linear transformation and
EigenVector&amp;EigenValue. This Background knowledge is meant to make
SVD straightforward. You can skip if you are familar with this
knowledge.</p>
<h3 id="linear-transformation">Linear transformation</h3>
<p>Given a matrice <span class="math inline">\(A\)</span> and vector
<span class="math inline">\(\vec{x}\)</span>, we want to compute the
mulplication of <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span></p>
<p><span
class="math display">\[\vec{x}=\begin{pmatrix}1\\3\end{pmatrix}\qquad
A=\begin{pmatrix}2 &amp; 1 \\ -1 &amp; 1
\end{pmatrix}\qquad\vec{y}=A\vec{x}\]</span></p>
<p>But when we do this multiplication, what happens? Acutually, when we
multiply <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span>, we are changing the coordinate
axes of the vector <span class="math inline">\(x\)</span> to another new
axes. Begin with a simpler example, we let</p>
<p><span class="math display">\[A=\begin{pmatrix}1 &amp; 0\\ 0
&amp;1\end{pmatrix}\]</span></p>
<p>then we have <span class="math display">\[A\vec{x}=\begin{pmatrix}1
&amp; 0\\ 0
&amp;1\end{pmatrix}\begin{pmatrix}1\\3\end{pmatrix}=\begin{pmatrix}1\\3\end{pmatrix}\]</span></p>
<p>You may have noticed that we can always get the same <span
class="math inline">\(\vec{x}\)</span> after left multiply by A. In this
case, we use coordinate axes <span
class="math inline">\(i=\begin{pmatrix}1 \\ 0\end{pmatrix}\)</span> and
<span class="math inline">\(j=\begin{pmatrix}0 \\
1\end{pmatrix}\)</span> as the figure below demonstrated. That is, if we
want to represent <span
class="math inline">\(\begin{pmatrix}1\\3\end{pmatrix}\)</span> under
the coordination, we can calculate the transformation as followed:</p>
<p><span class="math display">\[\begin{align} A\vec{x}=1\cdot i + 3\cdot
j = 1\cdot \begin{pmatrix}1 \\ 0\end{pmatrix} + 3\cdot \begin{pmatrix}0
\\
1\end{pmatrix}=\begin{pmatrix}1\\3\end{pmatrix}\end{align}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-073041.jpg" /></p>
<p>As we know, we can put a vector to anywhere in space, and if we want
to calculate sum of two vectors, the simplest way is to connect the to
vector from one's head to the other's tail. Our example, we compute
<span class="math inline">\(A\vec{x}\)</span> means add two vector(green
imaginary lines) up. And the answer is still <span
class="math inline">\(\begin{pmatrix}1\\3\end{pmatrix}\)</span>.</p>
<p>Now we change <span class="math inline">\(i=\begin{pmatrix}2\\
-1\end{pmatrix}\)</span> and <span
class="math inline">\(j=\begin{pmatrix}1\\1\end{pmatrix}\)</span> as the
coordinate axes(the red vectors), which means <span
class="math inline">\(A=\begin{pmatrix}2 &amp; 1 \\ -1 &amp;
1\end{pmatrix}\)</span>. I put vectors(black ones) to this figure as
well. We can see what happens when we change a new coordinate axes.</p>
<p>First of all, we multiply <span class="math inline">\(j\)</span> by
<span class="math inline">\(3\)</span> and <span
class="math inline">\(i\)</span> by 1. Then we move vector j and let the
head of <span class="math inline">\(i\)</span> connect the tail of <span
class="math inline">\(3\cdot j\)</span>. We can now find what is the
coordination of <span class="math inline">\(1\cdot i+3\cdot
j\)</span>(the blue one). We now verify the result using mutiplication
of <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span>:</p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}2 &amp; 1 \\ -1
&amp; 1\end{pmatrix}\begin{pmatrix}1\\3\end{pmatrix}=1\cdot
\begin{pmatrix}2 \\ -1\end{pmatrix} + 3\cdot  \begin{pmatrix}1 \\
1\end{pmatrix}=\begin{pmatrix}5\\2\end{pmatrix}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-013710.jpg" />
Here, you can imagine that matrice <span
class="math inline">\(A\)</span> is just like a function <span
class="math inline">\(f(x)\rightarrow y\)</span>, when you subsitute
<span class="math inline">\(x\)</span>, we get the exact <span
class="math inline">\(y\)</span> using the principle <span
class="math inline">\(f(x)\rightarrow y\)</span>. In fact, the
multiplication is tranform the vector from one coordination to
another.</p>
<h4 id="exercise">Exercise</h4>
<ol type="1">
<li><span class="math inline">\(A=\begin{pmatrix}1 &amp; 2 \\ 3 &amp;
4\end{pmatrix}\)</span>, draw the picture to stretch and rotate <span
class="math inline">\(x=\begin{pmatrix}1\\3\end{pmatrix}\)</span>.</li>
<li>Find a <span class="math inline">\(A\)</span> matrix to rotate <span
class="math inline">\(\vec{x}=\begin{pmatrix}1\\3\end{pmatrix}\)</span>
to <span class="math inline">\(90^{\circ}\)</span> and <span
class="math inline">\(180^{\circ}\)</span>.</li>
<li>what if <span class="math inline">\(A=\begin{pmatrix}1 &amp; 2 \\ 2
&amp; 4\end{pmatrix}\)</span>.</li>
</ol>
<h3 id="eigenvector-and-eigenvalue">EigenVector and EigenValue</h3>
<p>EigenVector and EigenValue is an extremely important concept in
linear algebra, and is commonly used everywhere including SVD we are
talking today. However, many do not know how to interpret it. In fact,
EigenVector and EigenValue is very easy as long as we know about what is
linear transformation.</p>
<h4 id="a-problem">A Problem</h4>
<p>Before start, let's take a look at a question: if we want to multiply
matrices for 1000 times, how to calculate effectively? <span
class="math display">\[AAA\cdots A= \begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp; 2\end{pmatrix}\cdots
\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\]</span></p>
<h4 id="intuition">Intuition</h4>
<p>Last section, we have talked that if we multiply a vector by a matrix
<span class="math inline">\(A\)</span>, means that we use <span
class="math inline">\(A\)</span> to stretch and rotate the vector in
order to represent the vector in a new coordinate axes. However, there
are some vectors for <span class="math inline">\(A\)</span>, they can
only be stretched but can not be rotated. Assume <span
class="math inline">\(A=\begin{pmatrix}3 &amp; 1 \\ 0 &amp;
2\end{pmatrix}\)</span>, let <span
class="math inline">\(\vec{x}=\begin{pmatrix}1 \\
-1\end{pmatrix}\)</span>. When we multiply <span
class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span></p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}3 &amp; 1 \\ 0
&amp; 2\end{pmatrix}\begin{pmatrix}1 \\ -1\end{pmatrix}=\begin{pmatrix}2
\\ -2\end{pmatrix}=2\cdot \begin{pmatrix}1 \\
-1\end{pmatrix}\]</span></p>
<p>It turns out we can choose any vector along <span
class="math inline">\(\vec{x}\)</span>, the outcome is the same, for
example:</p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}3 &amp; 1 \\ 0
&amp; 2\end{pmatrix}\begin{pmatrix}-3 \\
3\end{pmatrix}=\begin{pmatrix}-6 \\ -6\end{pmatrix}=2\cdot
\begin{pmatrix}-3 \\ 3\end{pmatrix}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-052135.jpg" /></p>
<p>We name vectors like <span class="math inline">\(\begin{pmatrix}-3 \\
3\end{pmatrix}\)</span> and <span class="math inline">\(\begin{pmatrix}1
\\ -1\end{pmatrix}\)</span> <strong>EigenVectors</strong> and 2 the
conresponse <strong>EigenValues</strong>. In practice, we usually choose
unit eigenvectors(length equals to 1) given that there are innumerable
EigenVectors along the line.</p>
<p>I won't cover how to compute these vectors and vaules and just list
the answer as followed</p>
<p><span class="math display">\[\begin{align}&amp;\begin{pmatrix}3 &amp;
1 \\0&amp; 2\end{pmatrix}
\begin{pmatrix}{-1}/{\sqrt(2)} \\ {1}/{\sqrt(2)}\end{pmatrix}=
2\begin{pmatrix}{-1}/{\sqrt(2)} \\ {1}/{\sqrt(2)}\end{pmatrix}
\qquad\qquad\vec{x_1}=\begin{pmatrix}{-1}/{\sqrt(2)} \\
{1}/{\sqrt(2)}\end{pmatrix} &amp;\lambda_1=2\\
&amp;\begin{pmatrix}3 &amp; 1 \\0&amp; 2\end{pmatrix}
\begin{pmatrix}1 \\ 0\end{pmatrix}\qquad=\qquad
3\begin{pmatrix}1 \\
0\end{pmatrix}\qquad\qquad\quad\,\,\,\vec{x_2}=\begin{pmatrix}1 \\
0\end{pmatrix}
&amp;\lambda_2=3
\end{align}\]</span> Notice that <span
class="math inline">\(|\vec{x_1}|=1\)</span> and <span
class="math inline">\(|\vec{x_2}|=1\)</span> #### EigenValue
Decomposition If we put two EigenVectors and corresponding EigenValues
together, we can get the following equation: <span
class="math display">\[AQ=\begin{pmatrix}3 &amp; 1 \\0&amp;
2\end{pmatrix}
\begin{pmatrix}
{-1}/{\sqrt(2)}&amp;1\\
{1}/{\sqrt(2)}&amp;0
\end{pmatrix}=
\begin{pmatrix}
{-1}/{\sqrt(2)}&amp;1 \\ {1}/{\sqrt(2)}&amp;0
\end{pmatrix}
\begin{pmatrix}
2 &amp; 0\\
0 &amp; 3
\end{pmatrix}=Q\Lambda
\]</span> Then we have <span class="math inline">\(AQ=Q\Lambda\)</span>,
the conclusion is still right if we introduce more dimensions, that is
<span class="math display">\[\begin{align}
A\vec{x_1}=\lambda\vec{x_1}\\
A\vec{x_2}=\lambda\vec{x_2}\\
\vdots\qquad\\
A\vec{x_k}=\lambda\vec{x_k}
\end{align}\]</span></p>
<p><span class="math display">\[Q=
\begin{pmatrix}
    x_{11}&amp; x_{21} &amp;\cdots x_{k1}&amp;\\
    x_{12}&amp; x_{22} &amp;\cdots x_{k2}&amp;\\
    &amp;\vdots&amp;&amp;\\
    x_{1m}&amp; x_{22} &amp;\cdots x_{km}&amp;
\end{pmatrix}
\qquad\Lambda=
\begin{pmatrix}
\lambda_1 &amp; 0 &amp; \cdots&amp;0\\
0 &amp;\lambda_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots\\
0&amp;\cdots&amp;\cdots&amp;\lambda_k
\end{pmatrix}\]</span></p>
<p>If we do something on the equation <span
class="math inline">\(AQ=Q\Lambda\)</span>, then we have: <span
class="math display">\[AQQ^{-1}=A=Q\Lambda Q^{-1}\]</span> It is
EigenVaule Decomposition. #### Resolution Now, Let's look at the
question in the beginning of this section <span
class="math display">\[\begin{align}
AAA\cdots A&amp;= \begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp; 2\end{pmatrix}\cdots
\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\\
AAA\cdots A &amp;= Q\Lambda Q^{-1}Q\Lambda Q^{-1}Q\Lambda Q^{-1}\cdots
Q\Lambda Q^{-1}=Q\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}Q^{-1}\\
AAA\cdots A &amp;=Q\Lambda\Lambda\cdots \Lambda Q^{-1}=
Q\begin{pmatrix}2^{1000} &amp; 0 \\0 &amp; 3^{1000}\end{pmatrix}Q^{-1}
\end{align}\]</span> The calculation is extremely simple using EVD.</p>
<h4 id="exercise-1">Exercise</h4>
<ol type="1">
<li>Research how to compute EigenVectors and EigenValues, then
compute<span class="math inline">\(\begin{pmatrix}1 &amp; 2 &amp; 3\\4
&amp; 5 &amp;6\\7 &amp; 8 &amp; 9\end{pmatrix}\)</span>.</li>
<li>Think about the decisive factor affects how many EigenValues we can
get.</li>
</ol>
<h3 id="singular-value-decompositon">Singular Value Decompositon</h3>
<p>Notice that EigenVector Decomposition is applied to decompose square
matrices. Is there any approach to decompose non-square matrices? The
answer is a YES, and the name is Singular Value Decompositon.</p>
<h4 id="intuition-1">Intuition</h4>
<p>First of all, let's take a look at what SVD looks like <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-rectangle1.png" />
From the picture, we can find that matrice <span
class="math inline">\(A\)</span> is decomposed to 3 components: <span
class="math inline">\(U\)</span>, <span
class="math inline">\(\Sigma\)</span> and <span
class="math inline">\(V^{T}\)</span>. <span
class="math inline">\(U\)</span> and<span
class="math inline">\(V^T\)</span> are both sqaure matrices and <span
class="math inline">\(\Sigma\)</span> has the same size as <span
class="math inline">\(A\)</span>. Still, I want to emphasize that <span
class="math inline">\(U\)</span> and<span
class="math inline">\(V^T\)</span> are both unitary matrix, which means
the Determinant of <span class="math inline">\(U\)</span> and <span
class="math inline">\(V^T\)</span> is 1 and <span
class="math inline">\(U^T=U^{-1}\quad V^T=V^{-1}\)</span>.</p>
<h4 id="deduction">Deduction</h4>
<p>In the Linear Transformation section, we can transform a vector to
another coordinate axes. Assume you have a non-square matrice, and you
want to transform A from vectors <span
class="math inline">\(V=(\vec{v_1},
\vec{v_2},\cdots,\vec{v_n})^T\)</span> to antoher coordinate axes which
is <span class="math inline">\(U=(\vec{u_1},
\vec{u_2},\cdots,\vec{u_n})^T\)</span>, the thing is, <span
class="math inline">\(\vec{v_i}\)</span> and <span
class="math inline">\(\vec{u_i}\)</span> have unit length, and all
directions are perpendicular, that is, each of <span
class="math inline">\(\vec{v_i}\)</span> are at right angles to other
<span class="math inline">\(\vec{v_j}\)</span>, we name such matrices as
orthogonal matrices. In addition, I need add a factor <span
class="math inline">\(\Sigma=(\sigma_1,\sigma_2,
\sigma_3,\cdots,\sigma_n)\)</span> which represent the times of each
direction of <span class="math inline">\(\vec{u_i}\)</span>, i.e., We
need transform A from <span class="math inline">\(V=(\vec{v_1},
\vec{v_2},\cdots,\vec{v_n})^T\)</span> to <span
class="math inline">\((\sigma_1 \vec{u_1},\sigma_2 \vec{u_2}, \sigma_3
\vec{u_3},...\sigma_n \vec{u_n})^T\)</span>. From the picture below we
can find that we want to transform from the circle coordinate axes to
the ellipse axes. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-circle.png" />
<span class="math display">\[\begin{align}
\vec{v_1} \vec{v_2} \vec{v_3},...\vec{v_n} \qquad\rightarrow \qquad
&amp;\vec{u_1},\vec{u_2},\vec{u_3},...\vec{u_n}\\
&amp;\sigma_1,\sigma_2, \sigma_3,...\sigma_n
\end{align}\]</span></p>
<p>Recall that we can transform <span class="math inline">\(A\)</span>
at every direction, then generate another direction as new coordinate
direction. So we have <span class="math display">\[ A \vec{v_1}=\sigma_1
\vec{u_1}\\
A \vec{v_2}=\sigma_2 \vec{u_2}\\
\vdots\\
A \vec{v_j}=\sigma_j \vec{u_j}\]</span></p>
<p><span class="math display">\[\begin{align}
&amp;\begin{pmatrix}\\A\\\end{pmatrix}\begin{pmatrix}\\
\vec{v_1},\vec{v_2},\cdots,\vec{v_n}\\\end{pmatrix}=\begin{pmatrix}\\
\vec{u_1}, \vec{u_2},\cdots,\vec{u_n}\\ \end{pmatrix}\begin{pmatrix}
\sigma_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_n
\end{pmatrix}\\
&amp;C^{m\times n}\qquad\quad C^{n\times n}\qquad\qquad\qquad C^{m\times
n}\qquad \qquad \qquad C^{n\times n}
\end{align}\]</span> Which is <span class="math display">\[A_{m\times
n}V_{n\times n} = \hat{U}_{m\times n}\hat{\Sigma}_{n\times
n}\]</span></p>
<p><span class="math display">\[\begin{align}
A_{m\times n}V_{n\times n} &amp;= \hat{U}_{m\times
n}\hat{\Sigma}_{n\times n}\\
(A_{m\times n}V_{n\times n}V_{n\times n}^{-1} &amp;= \hat{U}_{m\times
n}\hat{\Sigma}_{n\times n}V_{n\times n}^{-1}\\
A_{m\times n}&amp;=\hat{U}_{m\times n}\hat{\Sigma}_{n\times n}V_{n\times
n}^{-1}\\&amp;=\hat{U}_{m\times n}\hat{\Sigma}_{n\times n}V_{n\times
n}^{T}
\end{align}\]</span></p>
<p>We need do something to the equation in order to continue the
deduction. First we stretch matrice <span
class="math inline">\(\hat{\Sigma}\)</span> vertically to <span
class="math inline">\(m \times n\)</span> size. Then stretch <span
class="math inline">\(\hat{U}\)</span> horizonly to <span
class="math inline">\(m\times m\)</span>, we can set any value to the
right entries. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-27-RSVD.png" /></p>
<p>Due to the fact we need calculate <span
class="math inline">\(U^{-1}\)</span> and <span
class="math inline">\(V^{-1}\)</span>, the equation is adjusted to <span
class="math display">\[A_{m\times n} = U_{m\times m}\Sigma_{m\times
n}V^T_{n\times n}\]</span> For furture convenience, we need sort all
<span class="math inline">\(\sigma s\)</span>, which means: <span
class="math display">\[\sigma_1\geq\sigma_2\geq\sigma_3 \geq\cdots\geq
\sigma_m\]</span>. #### How to calculate <span
class="math inline">\(U\)</span>, <span
class="math inline">\(V^T\)</span> and <span
class="math inline">\(\Sigma\)</span> To Decompose matrice <span
class="math inline">\(A\)</span>, we need calculate <span
class="math inline">\(U\)</span>, <span
class="math inline">\(V^T\)</span> and <span
class="math inline">\(\Sigma\)</span>. Remember that <span
class="math inline">\(U^T = U^{-1}\)</span> and <span
class="math inline">\(V^T = V^{-1}\)</span>, we will use the property
next.</p>
<p><span class="math display">\[\begin{align}
A &amp;= U\Sigma V^T\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
AA^T&amp;=U\Sigma V^T(U\Sigma V^T)^T\\
&amp;=U\Sigma V^TV\Sigma^T U^T\\
&amp;=U\Sigma V^{-1}V\Sigma^T U^T\\
&amp;=U\Sigma I\Sigma^T U^T\\
&amp;=U\Sigma^2 U^T
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
(AA^T)U&amp;=(U\Sigma^2 U^T)U\\
&amp;=(U\Sigma^2 )U^{-1}U\\
&amp;=U\Sigma^2
\end{align}\]</span></p>
<hr />
<p><span class="math display">\[\begin{align}
A^TA
&amp;=(U\Sigma V^T)^TU\Sigma V^T\\
&amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T U^{-1}U\Sigma V^T\\
&amp;=V\Sigma^T I\Sigma V^T\\
&amp;=V\Sigma^2 V^T\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}(A^TA)V&amp;=(V\Sigma^2
V^T)V\\
&amp;=(V\Sigma^2)V^{-1}V\\
&amp;=V\Sigma^2
\end{align}\]</span></p>
<h3 id="image-compression">Image Compression</h3>
<p>Firstly, let's look at the process of compressing a picture, the left
picture is original grayscale image. On the right, under different
compress rate, we can see pictures after reproducing. Before compress,
the size of the picture is 1775K byte. Then the picture is almost the
same, when we compress which into 100K byte size, which means we can
save 90% storage space <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-29-image.gif" /></p>
<p>To compress a piture, you just decompose the matrice through SVD,
then instead of using the original <span
class="math inline">\(U_{m\times m}\)</span>, <span
class="math inline">\(\Sigma_{m\times n}\)</span> and <span
class="math inline">\(U_{n\times n}\)</span>, we shrink every matrice to
new size <span class="math inline">\(U_{m\times r}\)</span>, <span
class="math inline">\(\Sigma_{r\times r}\)</span> and <span
class="math inline">\(U_{r\times n}\)</span>. The final <span
class="math inline">\(size(R)\)</span> is still <span
class="math inline">\(m\times n\)</span>, but we abandon some entries
since these entries are not so important than these we have reserved.
<img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-30-rect.png" /></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%% octave: core code of svd compressions</span><br><span class="line">X = imread(filename);  </span><br><span class="line">[U S V] = svd(double(X));</span><br><span class="line">R = U(:,1:r)*S(1:r,1:r)*V(:,1:r)&#x27;;    </span><br></pre></td></tr></table></figure>
<h3 id="summary">Summary</h3>
<p>Today we have learned mathmatics backgroud on SVD, including linear
transformation and EigenVector&amp;EigenVaule. Before SVD, we first
talked about EigenValue Decomposition. Finally, Singular Vaule
Decomposition is very easy to be deduced. In the last section, we took
an example see how SVD be applied to image compression field.</p>
<p>Now, it comes to the topic how to save our storage of a 32G iPhone7,
the coclusion is obvious: using SVD compress image to shrink the size of
our photos.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>https://www.youtube.com/watch?v=EokL7E6o1AE</li>
<li>https://www.youtube.com/watch?v=cOUTpqlX-Xs</li>
<li>https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw</li>
<li>https://yhatt.github.io/marp/</li>
<li>https://itunes.apple.com/cn/itunes-u/linear-algebra/id354869137</li>
<li>http://www.ams.org/samplings/feature-column/fcarc-svd</li>
<li>https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/04/16/eigenvector-and-eigenvalue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/04/16/eigenvector-and-eigenvalue/" class="post-title-link" itemprop="url">怎样理解特征向量和特征值（翻译）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-04-16 21:03:38" itemprop="dateCreated datePublished" datetime="2017-04-16T21:03:38+02:00">2017-04-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文地址：<a
target="_blank" rel="noopener" href="http://math.stackexchange.com/a/23325">stackexchange</a></p>
<p>原文答案作者主页：<a
target="_blank" rel="noopener" href="http://math.stackexchange.com/users/742/arturo-magidin">Arturo
Magidin</a></p>
<h4 id="版权声明">版权声明</h4>
<p>本译文首发于我的个人博客chengmingbo.github.io, 版权属于原作者。 ####
简短的答案
特征向量可以让线性变换的理解变得简单。它们是沿着坐标轴（方向）的线性变换包括简单的伸/缩以及翻转；特征值提供的是这些线性变换影响因子。
如果你理解越多沿着坐标轴（方向）的线性变换行为，理解线性变换就变得越简单；所以你要做的是有足够多的线性无关的特征向量与单因素线性变换产生联系。</p>
<h4 id="长一点儿的答案">长一点儿的答案</h4>
<p>这个世界上有非常多的问题可以通过线性变换来建模，而特征向量提供了非常简单的解决方案。例如，考虑线性微分方程:
<span class="math display">\[\frac{\mathrm d x}{\mathrm d t} = ax +
by\]</span> <span class="math display">\[\frac{\mathrm d y}{\mathrm d t}
= cx + dy\]</span></p>
<p>可以找到很多描述此微分方程的系统，比如，两个物种数量的增长相互影响。具体来说，可能物种<span
class="math inline">\(x\)</span>是物种<span
class="math inline">\(y\)</span>的捕食者；周围越多的物种<span
class="math inline">\(x\)</span>，意味着越少的物种<span
class="math inline">\(y\)</span>可以得到繁衍壮大；问题是周围的物种<span
class="math inline">\(y\)</span>越少，那么对于物种<span
class="math inline">\(x\)</span>来说食物就会越少，所以物种<span
class="math inline">\(x\)</span>的繁衍就会越少；但是接下来因为物种<span
class="math inline">\(x\)</span>对物种<span
class="math inline">\(y\)</span>的生存压力降低，很快会导致<span
class="math inline">\(y\)</span>物种数量的增长；但是这就意味这物种<span
class="math inline">\(x\)</span>的食物变多了，所以物种xx的数量也跟着增长；如此这般，循环往复。特定的物理现象也能形成这样的系统，比如粒子在运动的流体中，粒子的速度矢量取决于其所处的流体中位置。</p>
<p>直接解决这种系统是非常复杂的。但是，假设如果你可以不用去关注变量<span
class="math inline">\(x\)</span>和变量<span
class="math inline">\(y\)</span>而是转而关注<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>（这里<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>与<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>线性相关，也就是说，<span
class="math inline">\(z=\alpha x + \beta y\)</span>, <span
class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>是常量，同时<span
class="math inline">\(w=\gamma x + \delta y\)</span>， <span
class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\delta\)</span>也是常量）。这样，我们的系统就变换成了如下的形式：
<span class="math display">\[\frac{\mathrm d z}{\mathrm d t} = \kappa
w\]</span> <span class="math display">\[\frac{\mathrm d w}{\mathrm d t}
= \lambda z\]</span></p>
<p>也就是说，你对系统做了<strong>解耦</strong>，这样你就可以单独的处理各个独立函数了。接下来就这个问题就变得非常简单：<span
class="math inline">\(z=Ae^{\kappa t}\)</span>，以及<span
class="math inline">\(w=Be^{\lambda t}\)</span>。下一步就是用<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>的公式，算出<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>。</p>
<p>这能做到么？事实上，这等于我们精确的找到了矩阵<span
class="math inline">\(\begin{pmatrix}a &amp; b\\
c&amp;d\end{pmatrix}\)</span>线性独立的两个特征向量！<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>是其特征向量，而<span
class="math inline">\(\kappa\)</span>和<span
class="math inline">\(\lambda\)</span>为相对应的特征值。通过使用一个表达式把<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span><strong>混合</strong>
起来，然后解耦成两个互相独立的函数，问题现在变得非常简单了。</p>
<p>这就是我们希望使用特征向量及特征值的本质：通过线性变换把问题<strong>解耦</strong>
成一系列沿着各个隔离<strong>方向</strong>的操作，使得各个方向问题都可独立解决。</p>
<p>大量的问题归根结底是解决<strong>线性独立操作</strong>，理解这些可以实实在在的帮助你理解矩阵/线性变换到底在做什么。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/04/05/pca-translation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/04/05/pca-translation/" class="post-title-link" itemprop="url">主成分分析（PCA）简明教程（翻译）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-04-05 21:58:33" itemprop="dateCreated datePublished" datetime="2017-04-05T21:58:33+02:00">2017-04-05</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>作者：Lindsay I Smith</p>
<p>时间：2002.2.26</p>
<p>译者：程明波</p>
<p><a target="_blank" rel="noopener" href="http://facepress.net/pdf/734.pdf">英文文章地址</a></p>
<p><a
target="_blank" rel="noopener" href="http://chengmingbo.githu.io/2017/04/05/pca-translation/">译文地址</a></p>
<h4 id="版权声明">版权声明</h4>
<p>本译文首发于我的个人博客chengmingbo.github.io, 版权属于原作者。</p>
<h3 id="第一章">第一章</h3>
<h4 id="前言">前言</h4>
<p>这是一篇帮助读者理解主成分分析（PCA）的教程。PCA是一种统计技术，在人脸识别和图像压缩等领域都有应用。同时，PCA也是一种高维数据模式发现的一种常用方法。</p>
<p>在讲PCA之前，本文先介绍了PCA用到的一些数学概念。其中包括标准差、协方差、特征向量和特征值等。这些背景知识意在帮助我们理解PCA部分，如果你对这些概念已经非常清晰可以跳过此部分。</p>
<p>示例贯穿于整个教程，以便于通过直观的例子讨论概念。如果你还想了解更多内容，霍华德.安东著有约翰威立国际出版公司出版的数学课本《Elementary
Linear Algebra 5e》提供了非常好的这方面数学背景知识。</p>
<h3 id="第二章">第二章</h3>
<h4 id="数学背景知识">数学背景知识</h4>
<p>本章试图介绍便于理解主成分分析计算过程所需的基本数学知识。各个主题互相独立，同时各主题会举例说明。理解为什么使用这些技术以及一些关于数据计算结果所告诉我们的比记住枯燥的数学原理更重要。尽管不是所有这些知识都应用于PCA，一些看起来不是直接相关知识是这些最重要技术的基石。</p>
<p>我安排其中的一节介绍统计学知识，主要着眼于分布的度量，或者说数据是怎样离散的，其他的部分主要讲矩阵代数的一些知识，包括特征值、特征向量以及一些PCA所需的重要矩阵性质。
#### 2.1 统计知识
整个统计学都基于你有一个很大数据集的前提，以及你想分析关于数据集中各个数据点的关系。我会介绍一些量度这些数据的一些方法，帮助你理解这些数据本身。</p>
<h5 id="标准差">2.1.1 标准差</h5>
<p>要了解标准差，我们需要一个数据集。统计学经常会使用总体中的一些采样。以选举为例，总体就是一个国家的所有人，因此一个采样就是统计学家用来度量的总体的一个子集。统计学伟大之处是我们只需度量（例如电话调查等）总体中的采样，你就可以计算出最接近所有整体的度量。</p>
<p>本节我假设我们的数据集是某个很大总体的采样。本节后面会提供总体以及采样的更多信息。这是一个示例数据集：
<span class="math display">\[X=[1\, 2\,4\, 6\, 12\, 15\, 25\, 45\, 68\,
67\, 65\, 98]\]</span> 我们简单地假设字符<span
class="math inline">\(X\)</span>表示包含这些所有数字的集合。如果我想表示这个集合中某个单独的数字，我会用<span
class="math inline">\(X\)</span>加上下标表示某个具体的数。例：<span
class="math inline">\(X_3\)</span>表示<span
class="math inline">\(X\)</span>集合中的第三个数，也就是数字<span
class="math inline">\(4\)</span>。注意，有些书用<span
class="math inline">\(X_0\)</span>表示第一个数字，我们这里用<span
class="math inline">\(X_1\)</span>。另外，我们用字符<span
class="math inline">\(n\)</span>表示集合中元素的数量。</p>
<p>我们可以计算一个集合的很多维度，比如，我们可以计算样本的均值。这里我假设读者明白什么是一个样本的均值。这里仅给出公式：
<span class="math display">\[\overline{X} =\frac{\sum_{i=1}^n
X_i}{n}\]</span> 注意，我们用字符<span
class="math inline">\(\overline{X}\)</span>标识集合的均值。这个公式表示：把所有的数字加起来再除以他们的数量。</p>
<p>很不幸，均值除了告诉我们某种中心以外，并没有告诉关于数据的更多信息。比如以下两个数据集合的均值(10)完全一样，但是显然它们区别很大。
<span class="math display">\[[0\, 8\, 12\, 20]\quad 和 \quad[8\, 9\,
11\, 12]\]</span></p>
<p>那么，这两个集合有何不同呢？这两个集合的离散程度是不同的。一个数据集的标准差（Standard
Deviation,
缩写SD）是衡量这个集合数据离散程度的一个指标。怎么计算呢？SD的定义是这样的：每个数据点到这份数据均值点的平均距离。计算每一个数据点到均值点的距离平方，然后相加，再除以<span
class="math inline">\(n-1\)</span>，再开方，公式如下： <span
class="math display">\[s=\sqrt{\frac{\sum_{i=1}^n
(X-\overline{X})^2}{(n-1)}}\]</span> 这里<span
class="math inline">\(s\)</span>常用来标识样本方差。可能有人会问：“为啥分母除以<span
class="math inline">\(n-1\)</span>而不是<span
class="math inline">\(n\)</span>呢？”
答案有点儿复杂，大体来说，如果你的数据集合是一个采样，比如，你取样于真实世界（比如调查500人的选举情况）得到一个子集，那么你就必须用<span
class="math inline">\(n-1\)</span>，因为这个结果比你用<span
class="math inline">\(n\)</span>更接近于你用全部的整体算出的标准差。但是，如果你不是计算一个样本的而是整体的标准差，这种情况下，你就应该除以<span
class="math inline">\(n\)</span>而不是<span
class="math inline">\(n-1\)</span>。如果想了解更多的关于标准差的内容，可以访问<a
target="_blank" rel="noopener" href="http://mathcentral.uregina.ca/RR/database/RR.09.95/weston2.html">这里</a>,链接文章用类似的方法讲了标准差，提供了不同分母计算的区别实验，同时还探讨了采样和总体的异同。</p>
<p>数据集1 <span class="math display">\[\begin{array}{lrr}
X &amp; (X-\overline{X}) &amp; (X-\overline{X})^2 \\
\hline
0 &amp; -10 &amp; 100\\
8 &amp; -2 &amp; 4\\
12 &amp; 2 &amp; 4\\
20 &amp; 10 &amp; 100\\
\hline
\bf{总计} &amp; &amp; 208\\
\hline
\bf{除以(n-1)} &amp; &amp; 69.333\\
\hline
\bf{平方根} &amp; &amp; 8.3266\\
\hline
\end{array}\]</span></p>
<p>数据集2 <span class="math display">\[\begin{array}{lrr}
X &amp; (X-\overline{X}) &amp; (X-\overline{X})^2 \\
\hline
8 &amp; -2 &amp; 4\\
9 &amp; -1 &amp; 1\\
11 &amp; 1 &amp; 1\\
12 &amp; 2 &amp; 4\\
\hline
\bf{总计} &amp; &amp; 10\\
\hline
\bf{除以(n-1)} &amp; &amp; 3.333\\
\hline
\bf{平方根} &amp; &amp; 1.8.257\\
\hline
\end{array}\]</span></p>
<p><span class="math display">\[\bf{表2.1 标准差计算}\]</span></p>
<p>从上表2.1，我们可以看到标准差的计算过程。</p>
<p>因此，和我们预想的一样，第一个数据的的标准差要比第二个大得多。原因是数据离散于均值点的程度更高。再举一个例子，数据集:
<span class="math display">\[[10\, 10\, 10\, 10]\]</span>
的均值也是10，但是它的标准差是0，
因为所有的数字是相同的。没有任何数据点偏离均值。</p>
<h5 id="方差">2.1.2 方差</h5>
<p>方差是数据离散程度的另一个度量。实际上，它和标准差几乎相同，公式如下：
<span class="math display">\[s^2=\frac{\sum_{i=1}^n
(X-\overline{X})^2}{(n-1)}\]</span>
你会注意到方差就是标准差的平方，标识上也有<span
class="math inline">\(s\)</span>(<span
class="math inline">\(s^2\)</span>)。<span
class="math inline">\(s^2\)</span>经常用来标识一个数据集的方差。方差和标准差都是用来衡量数据的离散程度。标准差使用的更普遍，方差也常使用。之所以介绍方差是因为下一节我们介绍的协方差是基于方差的。</p>
<h5 id="练习">练习</h5>
<p>计算下列数据集的均值、标准差和方差。</p>
<p>[12 23 34 44 59 70 98]</p>
<p>[12 15 25 27 32 88 99]</p>
<p>[15 35 78 82 90 95 97]</p>
<h5 id="协方差">2.1.3 协方差</h5>
<p>我们之前介绍的前两种度量方式只针对纯1维情况。1维数据集合可能是这种形式：屋里所有人的身高，或者上学期计算机科目101的成绩等等。但许多数据集是大于1维的情况，
对于这种数据集，统计分析的目标经常是分析不同的维度之间的关系。例如，我们可能有个数据集同时包含了课堂上学生的身高，以及他们论文的分数。接着我们就可以利用统计分析工具来观察学生的身高对他们的成绩是否有影响。</p>
<p>标准差和方差只是对单一维度的计算，因此，你只能对数据的每一个维度单独计算标准差。然而，如果有一种类似的度量能找到各维度相互在偏离均值的变化关系会非常有用。</p>
<p>协方差就是这样一种度量。协方差总是用来度量两个维度，如果你计算一个维度和他自己维度的协方差，这时协方差就退化为这一个维度的方差。因此，如果你有一个3维数据集<span
class="math inline">\((x,y,z)\)</span>,协方差的计算公式与方差非常相似。方差的计算公式也可以这样表示：
<span
class="math display">\[var(X)=\frac{\sum_{i=1}^n(X_i-\overline{X_i})(X_i-\overline{X_i})}{(n-1)}\]</span>
这里我简单的对平方项进行了展开。有了以上知识，我们现在可以写出协方差的公式了：
<span
class="math display">\[cov(X,Y)=\frac{\sum_{i=1}^n(X_i-\overline{X_i})(Y_i-\overline{Y_i})}{(n-1)}\]</span></p>
<p>除了第二个括号中的<span
class="math inline">\(X\)</span>全部被替换成了<span
class="math inline">\(Y\)</span>以外，协方差和方差的公式完全一样。我们可以这么表述：“对于每个数据项，把每个<span
class="math inline">\(x\)</span>和<span
class="math inline">\(x\)</span>均值的差与每个<span
class="math inline">\(y\)</span>和<span
class="math inline">\(y\)</span>均值的差相乘，再加和除以<span
class="math inline">\((n-1)\)</span>”。协方差是怎样的一种工作机理呢？我们这里用一些数据来举例。想象你通过调查得到一个2维数据。假设我们问了一堆学生他们花在科目COSC241的总小时数，以及他们的学期末成绩。现在我们有了两个维度，第一个维度是<span
class="math inline">\(H\)</span>，标识学习的小时数，第二个维度是M，标识学生的成绩。<strong>图2.2</strong>展示了我们假设的数据以及两个维度学习小时数和成绩之间的协方差<span
class="math inline">\(cov(H,M)\)</span>。</p>
<p>这张图告诉我们什么呢？协方差的值没有它的符号重要（正或负）。如果值是正的，比如我们这里，那么意味着两个维度一起增减。即，一般来说，如果学习的小时数增加，那么这个学生最后取得的成绩就会高。</p>
<p>但是如果协方差的值是负的，那么如果其中一个维度增加，另一个维度就会减少。如果我们刚刚计算的协方差的结果是负值。那我们的说法就变成了随着学习小时数的增加，期末成绩会降低。</p>
<p>最后一种情况，如果协方差是<span
class="math inline">\(0\)</span>，那么说明两个维度是相互独立的。</p>
<p>我们很容易画一张图如图2.1.3，得出结论：学习成绩随着学习的小时数增加而增加。但是，只有两维或三维这种低维的奢侈情况，我们才能通过可视化观察趋势。由于在一个数据集中可以计算任意两个维度的协方差，这种技术经常是高维数据可视化非常困难的情况下寻找维度之间关系的一种方法。</p>
<p>你可能会问，<span class="math inline">\(cov(X, Y)\)</span>与<span
class="math inline">\(cov(Y,X)\)</span>是否相等？简单一看我们就会发现，它们是完全相等的，因为两个式子计算的唯一不同是在<span
class="math inline">\(cov(Y,X)\)</span>中<span
class="math inline">\((X_i-\overline{X_i})(Y_i-\overline{Y_i})\)</span>被替换成了<span
class="math inline">\((Y_i-\overline{Y_i})(X_i-\overline{X_i})\)</span>。我们知道乘法满足交换率，也就是说，无论乘数和被乘数的位置怎么变化，结果都是一样，也就是说这两个协方差结果是相同的。</p>
<h5 id="section">2.1.4</h5>
<p>我们知道，协方差总是用来计算两个维度之间的关系。如果我们有一个超过2维的数据集合，那么我们要计算的协方差的值的数量就不止一个了。比如，一个三维的数据集（<span
class="math inline">\(x,y,z三个维度\)</span>)。你可以计算的协方差就有<span
class="math inline">\(cov(x,y)\)</span>、<span
class="math inline">\(cov(x,z)\)</span> 和 <span
class="math inline">\(cov(x,z)\)</span>。事实上，对于一个<span
class="math inline">\(n\)</span>维的数据集，你可以计算<span
class="math inline">\(\frac{n!}{(n-2)! * 2}\)</span>不同的值。</p>
<p>数据： <span class="math display">\[\begin{array}{lrr}
&amp;小时数(H)&amp;成绩(M)\\
\hline
数据 &amp; 9 &amp; 39\\
&amp; 15 &amp;56 \\
&amp; 25 &amp;93 \\
&amp; 14 &amp;61 \\
&amp; 10 &amp;50 \\
&amp; 18 &amp;75 \\
&amp; 0 &amp;32 \\
&amp; 16 &amp;85 \\
&amp; 5 &amp;42 \\
&amp; 19 &amp;70 \\
&amp; 16 &amp;66 \\
&amp; 20 &amp;80 \\
\hline
总数&amp;167&amp;749\\
\hline
平均 &amp; 13.92&amp; 62.42\\
\hline
\end{array}\]</span></p>
<p>协方差： <span class="math display">\[\begin{array}{cc|c|c|c}
H &amp; M &amp; (H_i - \overline{H}) &amp; (M_i-\overline{M}) &amp;
(H_i-\overline{H})(M_i-\overline{M})\\
\hline
9 &amp; 39 &amp; -4.92&amp; -23.42 &amp;115.23\\
15 &amp; 56 &amp; 1.08&amp; -6.42 &amp;-6.93\\
25 &amp; 93 &amp; 11.08&amp; -30.58 &amp;338.83\\
14 &amp; 61 &amp; 0.08&amp; -1.42 &amp;-0.11\\
10 &amp; 50 &amp; -3.92&amp; -12.42 &amp;48.69\\
18 &amp; 75 &amp; 4.08&amp; 12.58 &amp;51.33\\
0 &amp; 32 &amp; -13.92&amp; -30.42 &amp;423.45\\
16 &amp; 85 &amp; 2.08&amp; -22.58 &amp;46.97\\
5 &amp; 42 &amp; -8.92&amp; -20.42 &amp;182.15\\
19 &amp; 70 &amp; 5.08&amp; -7.58 &amp;38.51\\
16 &amp; 66 &amp; 2.08&amp; -3.58 &amp;7.45\\
20 &amp; 80 &amp; 6.08&amp; 17.58 &amp;106.89\\
\hline
总数 &amp; &amp; &amp; &amp; 1149.89\\
\hline
平均 &amp; &amp; &amp; &amp; 104.54\\
\end{array}\]</span></p>
<p>想求出所有不同维度的协方差，非常有用的方法是把他们全计算出来然后放入矩阵。我假设你对矩阵比较熟悉，以及矩阵怎样定义。因此，对于一个<span
class="math inline">\(n\)</span>维的数据集的协方差矩阵： <span
class="math display">\[C^{n\times n}=(c_{i,j}, c_{i,j}=cov(Dim_i,
Dim_j))\]</span>, 这里<span class="math inline">\(C^{n\times
n}\)</span>是一个<span class="math inline">\(n\)</span>行<span
class="math inline">\(n\)</span>列的矩阵，<span
class="math inline">\(Dim_x\)</span> 是第<span
class="math inline">\(x\)</span>维。上面非常不美观的公式说的是，如果你有一个<span
class="math inline">\(n\)</span>维数据集，那么协方差矩阵就是一个<span
class="math inline">\(n\)</span>行<span
class="math inline">\(n\)</span>列的矩阵，矩阵的每一个元素是两个维度之间的协方差计算结果。例如，矩阵的第2行第三列就是维度2和维度3之间的协方差计算结果。</p>
<p>一个例子。我们假设有一个3维的数据集，分别使用<span
class="math inline">\(x\)</span>,<span
class="math inline">\(y\)</span>,<span
class="math inline">\(z\)</span>表示3个维度。那么协方差矩阵是一个3行3列的矩阵，矩阵中的元素就是：
<span class="math display">\[\begin{pmatrix}
cov(x,x) &amp; cov(x,y) &amp; cov(x,z) \\
cov(y,x) &amp; cov(y,y) &amp; cov(y,z) \\
cov(z,x) &amp; cov(z,y) &amp; cov(z,z) \\
\end{pmatrix}\]</span></p>
<p>几个需要注意：主对角线计算的某一维和它自己的协方差，也就是这些维度的方差。剩下的元素，因为<span
class="math inline">\(cov(a,b)=cov(b,a)\)</span>，所以矩阵关于主对角线对称。</p>
<h5 id="练习-1">练习</h5>
<ol type="1">
<li>计算以下关于<span class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>的2维数据集的协方差，然后描述一下协方差结果可能推导出数据什么方面的结论。
<span class="math display">\[\begin{array}{c|c|c|c|c|c}
项目id &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\
\hline
x &amp; 10 &amp; 39 &amp; 19 &amp; 23 &amp; 28\\
y &amp; 43 &amp; 13 &amp; 32 &amp; 21 &amp; 20\\
\hline
\end{array}\]</span></li>
<li>计算下列3维数据的协方差矩阵： <span
class="math display">\[\begin{array}{c|c|c|c}
项目id &amp; 1 &amp; 2 &amp; 3 \\
\hline
x &amp; 1 &amp; -1 &amp; 4\\
y &amp; 2 &amp; 1 &amp; 3\\
z &amp; 1 &amp; 3 &amp; -1\\
\hline
\end{array}\]</span></li>
</ol>
<h4 id="矩阵代数">2.2 矩阵代数</h4>
<p>本节会介绍PCA所用到的一些矩阵代数的背景知识，我将重点介绍对给定矩阵计算特征向量和特征值的相关知识。这里我假设你了解矩阵的基本知识。
<span class="math display">\[\begin{align}\begin{pmatrix}
2 &amp; 3\\
2 &amp; 1\\
\end{pmatrix}\times
\begin{pmatrix}
1\\\\
3
\end{pmatrix}=
\begin{pmatrix}
11\\\\
5
\end{pmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}\begin{pmatrix}
2 &amp; 3\\
2 &amp; 1\\
\end{pmatrix}\times
\begin{pmatrix}
3\\\\
2
\end{pmatrix}=
\begin{pmatrix}
12\\\\
8
\end{pmatrix}=4\times
\begin{pmatrix}
3\\\\
2
\end{pmatrix}
\end{align}\]</span> <span
class="math display">\[\bf{图2.2：非特征向量和1个特征向量}\]</span>
<span class="math display">\[\begin{align}
2\times
\begin{pmatrix}
3\\\\
2
\end{pmatrix}=
\begin{pmatrix}
6\\\\
4
\end{pmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}\begin{pmatrix}
2 &amp; 3\\
2 &amp; 1\\
\end{pmatrix}\times
\begin{pmatrix}
6\\\\
4
\end{pmatrix}=
\begin{pmatrix}
24\\\\
16
\end{pmatrix}=4\times
\begin{pmatrix}
6\\\\
4
\end{pmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[\bf{图2.3:
缩放特征向量后仍为特征向量}\]</span></p>
<h5 id="特征向量">2.2.1 特征向量</h5>
<p>如你所知，只要两个矩阵的大小相容，你就可以将两个矩阵相乘。特征向量是矩阵相乘的的特殊形式。我们现在考虑如图2.2所示的矩阵和向量相乘的情况。</p>
<p>第一个例子中，计算结果不是整数与原始矩阵相乘的形式，但到了第二的例子，计算结果的就是一个整数乘以与左边完全相同的一个向量。为何能产生这样的结果呢？实际上，向量就是2维空间的一个矢量。向量<span
class="math inline">\(\begin{pmatrix}3\\2\end{pmatrix}\)</span>(第二个相乘的例子)代表从原点<span
class="math inline">\((0,0)\)</span>一个指向<span
class="math inline">\((3,2)\)</span>的一个箭头，另一个矩阵可以被认为是变换矩阵。如果你在向量的左边乘以一个矩阵，结果就是把这个向量从其原始位置进行了变换。</p>
<p>上面说得就是变换就是特这向量的本质。想象一个变换矩阵，以及一个在直线<span
class="math inline">\(y=x\)</span>上的向量，矩阵左乘这个向量。如果你发现结果仍然位于<span
class="math inline">\(y=x\)</span>这条直线上，那么这就向是量的自反射。这个向量（所有的乘子，因为我们不关心向量的大小）就是这个变换矩阵的一个特征向量。</p>
<p>这些特征向量有什么性质呢？第一你要知道的就是只有方矩阵才有特征向量。其次是不是所有的方矩阵都有特征向量。最后，如果一个<span
class="math inline">\(n\times n\)</span>矩阵只要有，那么就一定有<span
class="math inline">\(n\)</span>个特征向量。如果一个<span
class="math inline">\(3\times
3\)</span>的矩阵有特征向量，那就有3个。</p>
<p>特征向量的另一个性质是：如果我在相乘之前对其缩放一定量，那么我可以仍然得到相同的乘积形式（如图2.3）。这是因为如果你缩放一个的向量，你做的仅仅是把这个向量变长，而没有改变其方向。最后，一个矩阵的所有的特征向量都是<em>垂直</em>的。也就是说，无论你有多少维的向量，他们都是互相形成直角。另一个更数学化的说法叫<em>正交</em>。这么描述非常重要，原因是我们可以更方便表述这些垂直的正交向量，而不用在<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴的坐标系中描述。在PCA介绍部分我们会用到这些。</p>
<p>另一点重要的是，数学家们在寻找特征向量时，他们总喜欢找长度为1的特征向量。原因我们已经知道，向量的长度并不是影响因素，方向才是。所以为了使特征向量有标准形式，我们的一般做法是将其缩放成长度为1的向量。这样，所有的特征向量就都有相同的长度了。下面我们把例子中的向量标准化。
<span class="math display">\[\begin{pmatrix}
3\\
2
\end{pmatrix}
\]</span> 是一个特征向量，这个向量的长度为： <span
class="math display">\[\sqrt{(3^2+2^2)}=\sqrt{13}\]</span>
所以我们把原始的向量除以这个长度，就得到了长度唯一的特征向量。 <span
class="math display">\[\begin{pmatrix}
3\\
2
\end{pmatrix}\div\sqrt{13}=
\begin{pmatrix}
{3}/{\sqrt{13}}\\
{2}/{\sqrt{13}}
\end{pmatrix}
\]</span></p>
<p>怎么找到这些神秘的特征向量呢？很不幸，只有当矩阵足够小时，特征向量才好找，比如不超过<span
class="math inline">\(3\times
3\)</span>的矩阵。如果矩阵大小再变大，通常的做法是用复杂的迭代方式求解，这些方法此教程不会讲解。如果你想在程序中使用计算特征向量的方法，很多数学库都有实现，<a
target="_blank" rel="noopener" href="http://webnz.com/robert/">一个有用的数学库包</a>。</p>
<p>如果想进一步了解特征向量和特征值以及正交等内容，请参考霍华德.安东著有约翰威立国际出版公司出版的数学课本《Elementary
Linear Algebra 5e》，ISBN 0-471-85223-6。</p>
<h5 id="特征值">2.2.2 特征值</h5>
<p>特征值和特征向量高度相关，其实我们已经在图2.2看到过特征值。还记得被矩阵缩放以后的特征向量有相同的大小么？在那个例子中，这个值是4。这里4就是特征向量相关的特征值。无论我们对特征向量怎么缩放，我们始终得到的特征值都一直是一样的，如图2.3的例子特征值一直是4。</p>
<p>现在我们发现特征向量和特征值总是成对出现。如果你现在需要某个编程库计算特征向量，通常特征值也被同时计算出来了。</p>
<h4 id="练习-2">练习</h4>
<p>对于下面的矩阵 <span class="math display">\[\begin{pmatrix}
3&amp;0&amp;-1\\
-4&amp;1&amp;2\\
-6&amp;0&amp;-2
\end{pmatrix}
\]</span> 判断下面是否有此矩阵的特征向量，如果有，请求出对应的特征值。
<span class="math display">\[
\begin{pmatrix}
2\\
2\\
-1
\end{pmatrix}\
\begin{pmatrix}
-1\\
0\\
2
\end{pmatrix}\
\begin{pmatrix}
-1\\
1\\
3
\end{pmatrix}\
\begin{pmatrix}
0\\
1\\
0
\end{pmatrix}\
\begin{pmatrix}
3\\
2\\
1
\end{pmatrix}
\]</span></p>
<h3 id="第三章-主成分分析principal-components-analysis">第三章
主成分分析（Principal Components Analysis）</h3>
<p>终于到了主成分分析（PCA）部分了，PCA可以在数据中识别模式，并通过此种方式突出数据中相似和不同的部分。由于很难用图像表示高维数据，也就意味着在高维数据中寻找模式变得非常困难，这时，PCA就成了极为强大的数据分析工具。</p>
<p>另一个PCA的优点是，如果你通过其找到了数据中的模式，你还可以用来压缩数据，即，在不损失太多信息的前提下降低数据的维度。这个技术被用于图像压缩，我们在稍后的章节中会有涉及。</p>
<p>本章我们将针对一个数据集，一步一步实现PCA计算。这里我不准备描述<em>为什么</em>
PCA表现出色。我做的是为你提供每一步都发生了什么，这样，将来如果你想使用此技术时，就会有足够多的知识帮助你做决策。</p>
<h4 id="方法">3.1 方法</h4>
<h5 id="第一步数据集">第一步：数据集</h5>
<p>在我们这个简单的例子中，我会使用我编造的一个数据集。这个数据集只有两维，之所以选择这份数据是因为我可以通过画出图形来分析PCA的每一步都发生了什么。
##### 第二步：减掉均值
如果想实现PCA，我们首先要把每一维的数据减掉均值。就是说要对每一维求平均值，接着把每一维的每个数据都减掉均值。我们这里所有的<span
class="math inline">\(x\)</span>值都要减掉<span
class="math inline">\(\overline{x}\)</span>(<span
class="math inline">\(x\)</span>维度所有数据的均值)，所有的<span
class="math inline">\(y\)</span>值都减掉<span
class="math inline">\(\overline{y}\)</span>。这样我们就构造了一个均值为0的数据集。
<span class="math display">\[\begin{align}
\bf{数据}=
\begin{array}{c|c}
x&amp; y\\
\hline
2.5 &amp; 2.4\\
0.5 &amp; 0.7\\
2.2 &amp; 2.9\\
1.9 &amp; 2.2\\
3.1 &amp; 3.0\\
2.3 &amp; 2.7\\
2 &amp; 1.6\\
1 &amp; 1.1\\
1.5 &amp; 1.6\\
1.1 &amp; 0.9\\
\end{array}\bf{调整后的数据=}
\begin{array}{c|c}
x&amp; y\\
\hline
0.69 &amp; 0.49\\
-1.31 &amp; -1.21\\
0.39 &amp; 0.99\\
0.09 &amp; 0.29\\
1.29 &amp; 1.09\\
0.49 &amp; 0.79\\
0.19 &amp; -0.31\\
-0.81 &amp; -0.81\\
-0.31 &amp; -0.31\\
-0.71 &amp; -1.01\\
\end{array}
\end{align}\]</span> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-062821.jpg" />
<span
class="math display">\[图3.1：PCA示例数据，左边为原始数据，右边为减掉均值的数据\]</span></p>
<h5 id="第三步计算协方差矩阵">第三步：计算协方差矩阵</h5>
<p>协方差矩阵我们在2.1.4小节已经讨论过。由于我们的数据是2维的，所有协方差矩阵就是<span
class="math inline">\(2\times
2\)</span>。协方差矩阵计算没特别说明的，我直接给出结果： <span
class="math display">\[cov=\begin{pmatrix}
0.616555556&amp;0.615444444\\
0.615444444&amp;0.716555556
\end{pmatrix}\]</span>
由于协方差矩阵非对角线元素都是正值，所以我们可以预期<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>一起增减。 #####
第四步：计算协方差矩阵的特征向量和特征值
协方差矩阵是方阵，所以我们可以计算其特征向量和特征值。这极为重要，因为他们可以告诉我们关于数据的有用信息。我一会儿会说明原因，现在我们来看一下特征值和特征向量：
<span class="math display">\[\begin{align}
\bf{特征值}=\begin{pmatrix}
0.0490833989\\
1.28402771
\end{pmatrix}\bf{\bf    特征向量}=\begin{pmatrix}
-0.7351178656 &amp; -0.6778873399\\
0.677873399 &amp; -0.735178656
\end{pmatrix}
\end{align}\]</span>
一定要注意到两个特征向量都是单位向量。也就是说他们的长度都是1。这个结果对PCA非常重要，幸运的是，大部分数学工具包计算特征向量提供的都是单位向量。</p>
<p>那么，这些计算结果都是什么意思呢？如果你观察图3.2的数据点，你会发现这些数据有非常强的模式。和我们用协方差预期的一致，这些数据确实一起增减。我利用数据同时也画了两个特征向量，这两个特征向量看起来像图3.2的对角线。我们在特征向量小节介绍过，两个特征向量是相互垂直的。但是，更重要的是特征向量为我们提供了数据中的模式信息。可以看出，其中一条线（译者注：大概45度倾角的这条线）看起来像画了拟合这些数据点的一条线。这个特征向量告诉我们两个数据维度沿着线的相关性（译者注：原文是两个数据集，我认为是两个数据维度）。第2个特征向量（译者注：大概135度倾角的这条线）给我们提供了另外一些重要性稍低的数据中的模式，数据点分布在线的两边。</p>
<p>因此，通过从特征矩阵中取出特征向量进行分析，我们已经提取出了刻画数据特点的线。接下来的步骤会包括数据变换以便于用我们这些线来表达数据。
##### 第五步：选择成分及构建特征的向量
现在我们来讨论数据压缩和降维的概念。如果你学习了前面小节的特征向量和特征值的相关信息，你会注意到特征向量是之间有很大不同。事实上，具有更大特征值对应的特征向量是数据集的主成分(Principal
Component)。在我们这个例子中，对应更大特征值特的特征向量是基本拟合数据点的这条线。这维特征向量描述了数据维度之间最重要的关系。
<img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-081716.jpg" />
<span
class="math display">\[图3.2:\bf{标准化（减掉均值）后的数据图以及协方差矩阵中特征向量图}\]</span></p>
<p>一般来说，一旦从协方差矩阵中找出特征向量，下一步我们要做的就是把他们对应的特征值从高到低排列。排序表明了成分（component）的重要性高低。现在，如果你如果愿意，可以忽略那些重要性没那么高的成分，你就会丢失一些信息，但是如果特征值非常小，你丢失的信息并不会太多。如果你扔掉一些成分，最终的数据集的维度会低于原始数据的维度。具体来说，如果你的原始数据有<span
class="math inline">\(n\)</span>维，因此你可以计算出<span
class="math inline">\(n\)</span>个特征向量和<span
class="math inline">\(n\)</span>个特征值，接下来如果你只选取前<span
class="math inline">\(p\)</span>维特征向量，那么最终的数据就变成了<span
class="math inline">\(p\)</span>维的数据集。</p>
<p>现在你需要做的是构造一个特征(feature)的向量，其实就是一个向量的矩阵。矩阵是通过挑选你希望留下的特征向量，组成一个每列1个特征向量的矩阵(译者注：最后一维我认为是第p维更好，可以与上面一段对应)。
<span
class="math display">\[\bf{特征的向量}=(eig_1,eig_2,eig_3,...,eig_p)\]</span></p>
<p>来看我们的例子，现在我们有2个特征向量，我们现在有两个选择，第一选择是两个特征向量都被用于构造特征的向量：
<span class="math display">\[cov=\begin{pmatrix}
-0.77873399&amp;-0.735178656\\
-0.735178656&amp;0.677873399
\end{pmatrix}\]</span>
或者，我们可以扔掉不重要的成分，那么特征的向量只有1列： <span
class="math display">\[cov=\begin{pmatrix}
-0.677873399\\
-0.735178656
\end{pmatrix}\]</span> 下一节我们将针对上面两种新的数据集进行讨论。
##### 第六步：生成新数据集
这是PCA的最后同时是最简单的一步。一旦我们选择了我们希望保留到成分（特征向量集），我们只需把特征的矩阵转置，左乘调整后的数据（原始数据减掉均值），然后再转置。
<span class="math display">\[\bf{最终数据=行特征的向量} \times
\bf{行调整后的数据}\]</span></p>
<p>这里<span
class="math inline">\(\bf{行特征的向量}\)</span>是特征的向量组成的矩阵进行转置，也就是说现在特征向量现在是以行的形式排列，最重要的特征向量在第一行。<span
class="math inline">\(\bf{行调整后的数据}\)</span>是经过均值调整后的数据，也进行了转置，也就是说数据项在每一列，而每行是一个独立的维度。抱歉数据转置可能来得有点儿突然，但是如果我们现在对特征向量的矩阵和数据进行转置，后面的公式就会简单很多，而不是一直带着个转置的上标符号<span
class="math inline">\(T\)</span>。<span
class="math inline">\(\bf{最终数据}\)</span>是最终的数据集合，其中每一列是一个数据项，每一行是一个维度。</p>
<p>做完这些我们可以得到什么呢？我们可以得到和我们选择向量完全相关的原始数据。我们的原始数据有<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴两个坐标的坐标系，所以我们的数据与这两个坐标的坐标系相关。其实你可以用任何你喜欢的两个坐标轴的坐标系来表示你的数据。如果坐标轴互相垂直，这种表示方法是最高效的，这就是为何特征向量间互相垂直这么重要。现在我们已经把我们的数据从跟<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴相关改为2个特征向量组成的坐标系相关。如果说我们已经通过降维构造了新的数据集，也就是说我们扔掉了一些特征向量，那么新数据只跟我们留下的特征向量相关。</p>
<p>为了展示我们的数据，我已经把两种可能的特征的向量都对数据做了变换。我已经对每种情况的结果进行的了转置，这样我就把数据恢复成表结构的组织形式。同时，我也把最终的数据点画了出来，这样我们就可以观察这些数据点与这些成分之间的关系。</p>
<p>两个特征向量都保留的情况转换后的结果见图3.3。这个图其实就是原始数据旋转后，这样特征向量就成了坐标轴。这种情况很好理解，因为我们在分解的过程中并没有丢失任何信息。</p>
<p>另外一种变换，我们只保留有最大特征值的特征向量，我们可以从图3.4中看的数据的结果。和预期的一样，这个数据只有一维。如果你用这份数据与两维特征向量都用变换后的数据对比，你会注意到，这个数据就是另一份数据的第一列。所以，如果你画出这个数据的图，这份数据只有一维，那么结果其实就是图3.3数据点<span
class="math inline">\(x\)</span>的坐标点。我们其实就是高效的抛弃了其他的坐标轴，也就是其他的特征向量。</p>
<p>那么我究竟做了什么呢?
本质上我们把数据进行了变换，使之可以用相关的模式进行表示，这些模式就是一些最适合描述这些数据之间关系的线。这么做非常有用，因为我们现在已经把数据点对每条线的贡献进行分类，然后进行组合。首先，我们仅仅有<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴，这还不错，但是每个<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>的数据点其实并无法告诉我们，每个数据点和其他数据点之间的关系。现在数据点的值可以精确告诉我们数据点处于趋势线的位置（上面或者下面）。如果是两个特征向量都用的情况，我们仅仅是把数据转换以便于我们使这些数据与特征向量相关，而不是<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴。但是只留一维特征向量的分解移除了较小特征向量的贡献，是我们的数据只与保留的一维数据相关。
##### 3.1.1 把旧数据找回来
显然，如果你用PCA对数据进行压缩，你一定想把原始数据恢复回来。（下一章我们看到例子）这些内容来自于<a
target="_blank" rel="noopener" href="http://www.vision.auc.dk/sig/Teaching/Flerdim/Current/hotelling/hotelling.html">这里</a>。
<span class="math display">\[\begin{align}
\bf{转换后的数据}=\begin{array}{c|c}
x&amp;y\\
\hline
-0.827970186 &amp; -0.175115307\\
1.77758033 &amp; 0.142857227\\
-0.992197494 &amp; 0.384374989\\
-0.274210416 &amp; 0.1304117207\\
-1.67580142 &amp; -0.209498461\\
-0.912949103 &amp; 0.17528282444\\
0.0991094375 &amp; -0.349824698\\
1.14457216 &amp; 0.0464172582\\
0.438046137 &amp; 0.0177646297\\
1.22382056 &amp; -0.162675287\\
\end{array}
\end{align}\]</span> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-134547.jpg" />
<span
class="math display">\[图3.3：应用了PCA分析后并使用了两个特征向量的数据表以及绘制的新数据点\]</span></p>
转换后的数据(单一特征向量)
<span class="math display">\[\begin{array}{c}
x\\
\hline
-0.827970186\\
1.77758033\\
-0.992197494\\
-0.274210416\\
-1.67580142\\
-0.912949103\\
1.14457216\\
0.438046137\\
1.22382056
\end{array}\]</span>
<p><span
class="math display">\[图3.4：只用最重要的特征向量数据转换的数据\]</span></p>
<p>所以，我们怎么把原来数据恢复回来？在我们进行恢复原始数据之前，回忆只有我们将所有特征向量进行转换才能精确的把数据恢复回来。如果我们在最后转换时减少特征向量，那么恢复的数据已经失去很多信息。
回想一下，最后的变换是： <span
class="math display">\[\bf{最终数据=行特征的向量} \times
\bf{行调整后的数据}\]</span> 我们可以把公式反转过来，进而得到原始数据，
<span
class="math display">\[\bf{行调整后的数据}=\bf{行特征的向量}^{-1}\times\bf{最终数据}\]</span>
这里，<span
class="math inline">\(\bf{行特征的向量}^{-1}\)</span>是的<span
class="math inline">\(\bf{行特征的向量}\)</span>的逆。由于我们讨论的是特征向量，组成的特征的向量，所以，<span
class="math inline">\(\bf{行特征的向量}^{-1}\)</span>其实就是<span
class="math inline">\(\bf{行特征的向量}\)</span>的转置。当然，只有在矩阵中的所有元素是由单位特征向量组成是才成立。这样，恢复原始数据又变得容易了很多，现在公式变成了：
<span
class="math display">\[\bf{行调整后的数据}=\bf{行特征的向量}^{T}\times\bf{最终数据}\]</span>
这个公式在我们只保留部分特征向量的情况下仍然成立。也就是说，就算你扔掉了一下特征向量，上面的公式仍然成立。</p>
<p>我不会演示用所有特征向量恢复原始数据，因为这样计算的结果和开始的数据一模一样。但是，我们一起来看一下只保留了一维特征向量的情况下，是怎样损失信息的。图3.5展示了丢失信息的情况。我们把图中的数据点与图3.1对比一下就会发现，沿着主特征向量的变化被保留下来了（见图3.2特征向量及数据）沿着其他成分（另一个特征向量被扔掉了）的变化丢失了。</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-30-003805.jpg" />
<span class="math display">\[图3.5
从单独一维特征向量重新构造的数据\]</span></p>
<h4 id="练习-3">练习</h4>
<ol type="1">
<li>协方差矩阵的特征向量为我们提供了什么呢？</li>
<li>我们在PCA计算的过程中，哪一步可以决定压缩数据，压缩可以起到什么效果呢？</li>
<li>举例说明PCA在图像处理怎样用主成分表示，同时调研一下人脸识别中“特征脸”(Eigenfaces)主题。</li>
</ol>
<h3 id="第四章-计算机视觉应用">第四章 计算机视觉应用</h3>
<p>本章我们将简单的PCA在计算机视觉领域的应用，首先我们看一下图像是怎么表示的，然后我们我们看看能怎样用PCA处理这些图像。本章关于人脸识别的的信息主要来自于1997年IEEE
9月Vol 85, No. 9《Face Recognition: Eigenface, Elastic Matching, and
Neural Nets》。图像表示来自于爱迪生-韦斯利出版社1987年出版的由Rafael C.
Gonzalez 和Paul Wintz合著的《Digital Image
Processing》想了解更多信息，KL变换相关知识也是非常好的参考。图像压缩相关知识来自于<a
target="_blank" rel="noopener" href="http://www.vision.auc.dk/sig/Teaching/Flerdim/Current/hotelling/hotelling.html">这里</a>，此网站还提供了大量用不同数量特征向量重新构造图像的方法。</p>
<h4 id="表示">4.1 表示</h4>
<p>在我们把一系列矩阵技术应用于计算机视觉上时，我们必须考虑图像的表示方法。一个正方形，<span
class="math inline">\(N\times N\)</span>的图像可以被表示成<span
class="math inline">\(N^2\)</span>维的向量。 <span
class="math display">\[X=(x_1,x_2,x_3,...,x_{N^2})\]</span>
这里，第一行前<span class="math inline">\(N\)</span>个<span
class="math inline">\((x_1--x_n)\)</span>一个挨着一个的像素点组成了1维的图像，下<span
class="math inline">\(N\)</span>个元素是下一行，以此类推。每个像素点的值代表图像三原色的亮度，也可能是只是灰度图像，那么只需要1个单独的值即可表示。
#### 4.2 PCA寻找模式
假设我们有20个图像。每个图像的像素非常高。对每个图像，我们都建立一个图像向量表示相应图像。接着我们就可以把所有的图像放到一个像这样的大矩阵中：
<span class="math display">\[图像矩阵=
\begin{pmatrix}
ImageVec_1\\
ImageVec_2\\
\vdots\\
ImageVec_{20}\\
\end{pmatrix}\]</span>
我们现在就可以开始以这条图像矩阵为基始，应用PCA，先构造协方差矩阵，然后得到原始数据相关的特征向量。为什么用PCA分析有用呢？假设我们要做人脸识别，那我们的原始数据就是很多人脸。接下来的问题是，给一张我新的图片，那么这是原始人脸数据中谁的人脸呢（注意，这新的图片不是我们开始给的20个人脸图片）？计算机视觉的处理方法是衡量新的图片和原始图片的差别，但并不是在原始坐标系进行对比，而是在PCA分析的生成的坐标系下衡量。</p>
<p>在实际应用中，PCA生成的坐标系下识别人脸会好非常多，因为PCA分析已经提供了原始图片中不同和相似等相关性。主成分分析已经识别出了数据中的统计学模式。</p>
<p>因为所有的向量都是<span
class="math inline">\(N^2\)</span>维的，我们最后会得到<span
class="math inline">\(N^2\)</span>个特征向量，在实践中，我们可以扔掉其中不重要的一些特征向量，识别效果仍然非常好。
#### 4.3 PCA图像压缩
使用PCA做图像压缩常常也被称作霍特林变换或者是KL变换（Karhunen-Leove
transform）. 如果我们有20个图像，每个图像有<span
class="math inline">\(N^2\)</span>个像素，所以我们就可以构造<span
class="math inline">\(N^2\)</span>个向量，每个向量20维。每个向量由每个图片的相同像素点的图片亮度值组成。这与我们之前的例子不同，因为之前我们是有一个图像的向量，向量里的每项都是不同的像素。然而我们现在是有一个每个像素的向量，向量里的每项是都是来自于不同的图片。</p>
<p>如果现在我们在一个数据集上应用PCA，那么，我们将会得到20个特征向量，因为，每个向量都是20维的。如果想要压缩数据，我们可以选择只用其中一部分特征向量变换，假设是15个特征向量。这样我得到的最终数据只有15维，达到了节省空间的目的。但是，当要恢复原始数据是，图像已经丢了一些信息。这种压缩技术叫做有损压缩，因为解压后的图片已经不是和原始图片完全一样的图片了，一般来说会变差。</p>
<h3 id="附录-a">附录 A</h3>
<h4 id="实现代码">实现代码</h4>
<p>这份代码用于可替换Matlab的自由软件Scilab。我用这份代码生成了文章的所有例子。除了第一个宏，剩下的都是我(原文作者)写的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">// This macro taken from</span><br><span class="line">// http://www.cs.montana.edu/ ̃harkin/courses/cs530/scilab/macros/cov.sci // No alterations made</span><br><span class="line">// Return the covariance matrix of the data in x, where each column of x</span><br><span class="line">// is one dimension of an n-dimensional data set.  That is, x has x columns</span><br><span class="line">// and m rows, and each row is one sample.</span><br><span class="line">//</span><br><span class="line">// For example, if x is three dimensional and there are 4 samples.</span><br><span class="line">// x=[123;456;789;101112]</span><br><span class="line">// c=cov(x)</span><br><span class="line">function [c]=cov (x)</span><br><span class="line">// Get the size of the array</span><br><span class="line">sizex=size(x);</span><br><span class="line">// Get the mean of each column</span><br><span class="line">meanx = mean (x, &quot;r&quot;);</span><br><span class="line">// For each pair of variables, x1, x2, calculate</span><br><span class="line">// sum ((x1 - meanx1)(x2-meanx2))/(m-1)</span><br><span class="line">for var = 1:sizex(2),</span><br><span class="line">        x1 = x(:,var);</span><br><span class="line">        mx1 = meanx (var);</span><br><span class="line">        for ct = var:sizex (2),</span><br><span class="line">                x2 = x(:,ct);</span><br><span class="line">                mx2 = meanx (ct);</span><br><span class="line">                v = ((x1 - mx1)’ * (x2 - mx2))/(sizex(1) - 1);</span><br><span class="line">end, c=cv;</span><br><span class="line">end,</span><br><span class="line">cv(var,ct) = v;</span><br><span class="line">cv(ct,var) = v;</span><br><span class="line">// do the lower part of c also.</span><br><span class="line">// This a simple wrapper function to get just the eigenvectors</span><br><span class="line">// since the system call returns 3 matrices</span><br><span class="line">function [x]=justeigs (x)</span><br><span class="line">// This just returns the eigenvectors of the matrix</span><br><span class="line">[a, eig, b] = bdiag(x);</span><br><span class="line">x= eig;</span><br><span class="line">// this function makes the transformation to the eigenspace for PCA</span><br><span class="line">// parameters:</span><br><span class="line">// adjusteddata = mean-adjusted data set</span><br><span class="line">// eigenvectors = SORTED eigenvectors (by eigenvalue)</span><br><span class="line">// dimensions  = how many eigenvectors you wish to keep</span><br><span class="line">//</span><br><span class="line">// The first two parameters can come from the result of calling</span><br><span class="line">// PCAprepare on your data.</span><br><span class="line">// The last is up to you.</span><br><span class="line">function [finaldata] = PCAtransform(adjusteddata,eigenvectors,dimensions) finaleigs = eigenvectors(:,1:dimensions);</span><br><span class="line">prefinaldata = finaleigs’*adjusteddata’;</span><br><span class="line">finaldata = prefinaldata’;</span><br><span class="line">// This function does the preparation for PCA analysis</span><br><span class="line">// It adjusts the data to subtract the mean, finds the covariance matrix,</span><br><span class="line">// and finds normal eigenvectors of that covariance matrix.</span><br><span class="line">// It returns 4 matrices</span><br><span class="line">// meanadjust = the mean-adjust data set</span><br><span class="line">// covmat = the covariance matrix of the data</span><br><span class="line">// eigvalues = the eigenvalues of the covariance matrix, IN SORTED ORDER</span><br><span class="line">// normaleigs = the normalised eigenvectors of the covariance matrix,</span><br><span class="line">// IN SORTED ORDER WITH RESPECT TO</span><br><span class="line">// THEIR EIGENVALUES, for selection for the feature vector.</span><br><span class="line">//</span><br><span class="line">// NOTE: This function cannot handle data sets that have any eigenvalues</span><br><span class="line">// equal to zero. It’s got something to do with the way that scilab treats</span><br><span class="line">// the empty matrix and zeros.</span><br><span class="line">//</span><br><span class="line">function [meanadjusted,covmat,sorteigvalues,sortnormaleigs] = PCAprepare (data) // Calculates the mean adjusted matrix, only for 2 dimensional data</span><br><span class="line">means = mean(data,&quot;r&quot;);</span><br><span class="line">meanadjusted = meanadjust(data);</span><br><span class="line">covmat = cov(meanadjusted);</span><br><span class="line">eigvalues = spec(covmat);</span><br><span class="line">normaleigs = justeigs(covmat);</span><br><span class="line">sorteigvalues = sorteigvectors(eigvalues’,eigvalues’);</span><br><span class="line">sortnormaleigs = sorteigvectors(eigvalues’,normaleigs);</span><br><span class="line">// This removes a specified column from a matrix</span><br><span class="line">// A = the matrix</span><br><span class="line">// n = the column number you wish to remove</span><br><span class="line">function [columnremoved] = removecolumn(A,n)</span><br><span class="line">inputsize = size(A);</span><br><span class="line">numcols = inputsize(2);</span><br><span class="line">temp = A(:,1:(n-1));</span><br><span class="line">for var = 1:(numcols - n)</span><br><span class="line">        temp(:,(n+var)-1) = A(:,(n+var));</span><br><span class="line">columnremoved = temp;</span><br><span class="line">// This finds the column number that has the</span><br><span class="line">// highest value in it’s first row.</span><br><span class="line">function [column] = highestvalcolumn(A)</span><br><span class="line">inputsize = size(A);</span><br><span class="line">numcols = inputsize(2);</span><br><span class="line">maxval = A(1,1);</span><br><span class="line">maxcol = 1;</span><br><span class="line">for var = 2:numcols</span><br><span class="line">        if A(1,var) &gt; maxval</span><br><span class="line">                maxval = A(1,var);</span><br><span class="line">end,</span><br><span class="line">        end,</span><br><span class="line">column = maxcol</span><br><span class="line">maxcol = var;</span><br><span class="line">25</span><br><span class="line">end,</span><br><span class="line">// This sorts a matrix of vectors, based on the values of</span><br><span class="line">// another matrix</span><br><span class="line">//</span><br><span class="line">// values = the list of eigenvalues (1 per column)</span><br><span class="line">// vectors = The list of eigenvectors (1 per column)</span><br><span class="line">//</span><br><span class="line">// NOTE:  The values should correspond to the vectors</span><br><span class="line">// so that the value in column x corresponds to the vector</span><br><span class="line">// in column x.</span><br><span class="line">function [sortedvecs] = sorteigvectors(values,vectors)</span><br><span class="line">inputsize = size(values);</span><br><span class="line">numcols  = inputsize(2);</span><br><span class="line">highcol = highestvalcolumn(values);</span><br><span class="line">sorted = vectors(:,highcol);</span><br><span class="line">remainvec = removecolumn(vectors,highcol);</span><br><span class="line">remainval = removecolumn(values,highcol);</span><br><span class="line">for var = 2:numcols</span><br><span class="line">        highcol = highestvalcolumn(remainval);</span><br><span class="line">        sorted(:,var) = remainvec(:,highcol);</span><br><span class="line">        remainvec = removecolumn(remainvec,highcol);</span><br><span class="line">        remainval = removecolumn(remainval,highcol);</span><br><span class="line">end,</span><br><span class="line">sortedvecs = sorted;</span><br><span class="line">// This takes a set of data, and subtracts</span><br><span class="line">// the column mean from each column.</span><br><span class="line">function [meanadjusted] = meanadjust(Data)</span><br><span class="line">inputsize = size(Data);</span><br><span class="line">numcols = inputsize(2);</span><br><span class="line">means = mean(Data,&quot;r&quot;);</span><br><span class="line">tmpmeanadjusted = Data(:,1) - means(:,1);</span><br><span class="line">for var = 2:numcols</span><br><span class="line">        tmpmeanadjusted(:,var) = Data(:,var) - means(:,var);</span><br><span class="line">meanadjusted = tmpmeanadjusted</span><br><span class="line">end,</span><br></pre></td></tr></table></figure>
<p><a
target="_blank" rel="noopener" href="http://cmb.oss-cn-qingdao.aliyuncs.com/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89.pdf">本文PDF</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/03/18/Naive-Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/03/18/Naive-Bayes/" class="post-title-link" itemprop="url">Naive Bayes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-03-18 21:06:08" itemprop="dateCreated datePublished" datetime="2017-03-18T21:06:08+01:00">2017-03-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="introduction">Introduction</h3>
<p>You are sitting in front your sceen, annoyed by a bunch of spam
mails. You wonder if there are any appoaches to get rid of so much many
offended emails. Last time you doped out a extremely good idea. You set
a series of words to identify those emails: every mail invovled by words
"coupon" was trown to trash. However, on one hand, there were only about
10% spam including "coupon", one the other hand, you had trashed two
significant emails, due to which, you lost two business valued about two
million dollars. The thing was that, your inbox seems being overrun by
those spams. Who can rescue you from endless deleting spams
everyday?</p>
<h3 id="intuition">Intuition</h3>
<p>Actually, you were close to the answer when you were putting all
emails to trash which included the word "coupon". Today, we learn about
an efficient method to solve the problem systematically. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-18-110227.jpg" />
Maybe we can search through internet, find all words which are about
advertisment in email. You can restrict that if and only if those which
includes more than 4 word in the list of spam words can be put into
trash can. Maybe finally you can design a rule system to recoginize
those spams without miss important ones. But it seems so boring a job to
do this, moreover, it may be not personalized. If some those who are
working for saling discount stuffs, he/she may find it doesn't work
through applying your effective rules. How about thinking of probablity
of those words emerge in all your inbox. Some words such as "coupon" may
contribute more but not entirety, simultaneously, affordable may
contribute less but not none. Notice that profit emerge in spam emails
and normal emails both sometimes. We let <span
class="math inline">\(y=0\)</span> denote an normal email while <span
class="math inline">\(y=1\)</span> the opposite. And if a word such as
"coupon" emerges in a mail, we set <span
class="math inline">\(coupon=1\)</span>, otherwise <span
class="math inline">\(coupon=0\)</span>. Suppose you have an email, we
define the probability to : <span class="math display">\[\begin{align}
&amp;p1=p(y=0|free=1, discount=1, affordable=1, customer=0, KPI=0,
budget=0,...,bias=0)=?\\
&amp;p2=p(y=1|free=1, discount=1, affordable=1, customer=0, KPI=0,
budget=0,...,bias=0)=?
\end{align}\]</span> Our aim is to decide which is bigger <span
class="math inline">\(p1\)</span> or <span
class="math inline">\(p2\)</span>. Let's describe the problem as
followed: &gt; We want to decide the probality of a mail spam or normal
when word "free" is in the mail, "discount" is in the mail, "affordable"
is in the mail, customer is not in the mail, ..., "bias" is not.</p>
<p>The description above is only about one email. For some other emails,
maybe "free" and "discount" both did not emerge at all.</p>
<h3 id="definitioin">Definitioin</h3>
<p>To generalize the problem, we let <span
class="math inline">\(x_i\)</span> denote a word in emails. Suppose we
now know all words in your inbox, say 10 thousand words. Then we have
<span class="math inline">\(x_1\)</span> denote if "free" is in a mail,
<span class="math inline">\(x_1=0\)</span> denotes negative while <span
class="math inline">\(x_1=1\)</span> denotes positive. So we have <span
class="math inline">\(x_1,x_2,x_3,...,x_{10000}\)</span>, which denotes
the status of each word in a mail. Then the problem is transferred as
followed: <span class="math display">\[\begin{align}
&amp;p1=p(y=0|x_1=1, x_2=1, x_3=1, x_4=0, x_5=0,
x_6=0,...,x_{10000}=0)=?\\
&amp;p2=p(y=1|x_1=1, x_2=1, x_3=1, x_4=0, x_5=0,
x_6=0,...,x_{10000}=0)=?
\end{align}\]</span> Suppose we have N words in your inbox, then we want
to decide: <span class="math display">\[\begin{align}
&amp;p1=p(y=0|x_1, x_2,...,x_{N})=?\\
&amp;p2=p(y=1|x_1, x_2,...,x_{N})=?
\end{align}\]</span> So how to sovle the probability problem?</p>
<h3 id="naive-bayes">Naive Bayes</h3>
<p>Recall Bayes rules: <span class="math display">\[\begin{equation}
p(y|x)=\frac{p(x,y)}{p(x)}=\frac{p(x|y)p(y)}{p(x)}
\end{equation}\]</span> we apply the equation to our problem, then we
have: <span class="math display">\[\begin{align}
&amp;p(y=1|x_1,x_2,...,x_N)\\
&amp; =\frac{p(x_1,x_2,...,x_N,y=1)}{p(x_1,x_2,...,x_N)}\\
&amp; =\frac{p(x_1,x_2,...,x_N|y)p(y=1)}{p(x_1,x_2,...,x_N)}\\
\\
&amp;p(y=0|x_1,x_2,...,x_N)\\
&amp; =\frac{p(x_1,x_2,...,x_N,y=0)}{p(x_1,x_2,...,x_N)}\\
&amp; =\frac{p(x_1,x_2,...,x_N|y)p(y=0)}{p(x_1,x_2,...,x_N)}
\end{align}\]</span> Notice that {p(x_1,x_2,...,x_N) is positive, and a
constant as well. so our aim is transferred to: <span
class="math display">\[\begin{align}
&amp;Max(p(y=1|x_1,x_2,...,x_N), p(y=0|x_1,x_2,...,x_N)\\
&amp;=Max(p(x_1,x_2,...,x_N|y=1)p(y=1),p(x_1,x_2,...,x_N|y=0)p(y=0)
\end{align}\]</span></p>
<p>First of all, we talk about how to get <span
class="math inline">\(p(y=0)\)</span> and <span
class="math inline">\(p(y=1)\)</span>. We have <span
class="math inline">\(N=10000\)</span>, suppose there are <span
class="math inline">\(900\)</span> spams and <span
class="math inline">\(91000\)</span> normal emails, then: <span
class="math display">\[\begin{align}
&amp;p(y=0)=\frac{count(spam\ email)}{count(all\
emails)}=\frac{900}{10000}=0.09\\
&amp;p(y=1)=\frac{count(normal\ email)}{count(all\
emails)}=\frac{9100}{10000}=0.91
\end{align}\]</span> And right now our task left is to compute <span
class="math inline">\(p(x_1,x_2,...,x_N|y=0)\)</span> and <span
class="math inline">\(p(x_1,x_2,...,x_N|y=1)\)</span>, that means we
want to know if a mail is a normal one or not, what is the probability
of the combination of these <span class="math inline">\(N=10000\)</span>
words. In other word, <span class="math inline">\(x_1=0\ or\
1\)</span>,<span class="math inline">\(x_2=0\ or\ 1\)</span> and so on.
So we have to compute <span
class="math inline">\(2*2^{10000}=2*1.995*10^{3010}\)</span>
probabilities under our circumstance. It seems the scale is so large
that we can not handle it. So we have an extremely adventurous
assumption: &gt; Each x in an email only can be decided by y.</p>
<p>Then we have: <span class="math display">\[\begin{align}
p(x_1,x_2,...,x_N|y=0)=p(x_1|y=0)\cdot p(x_2|y=0)\cdots p(x_N|y=0)\\
p(x_1,x_2,...,x_N|y=1)=p(x_1|y=1)\cdot p(x_2|y=1)\cdots p(x_N|y=1)
\end{align}\]</span> Now, we just need compute 2*10000 probabilities, it
is surely a mission possible now. So how to compute <span
class="math inline">\(p(x_i=0\ or\ 1|y=0\ or\ 1)\)</span>? Take word
"free" for example, we want to compute: <span
class="math display">\[\begin{align}
&amp;p(free=0|y=0)\\
&amp;=\frac{p(free=0, y=0)}{p(y=0)}\\
&amp;=\frac{count\ of\ emails\ have\ no\ word\ &quot;free&quot;\ in\
normal\ emails}{count\ of\ normal\ emails}\\
&amp;p(free=0|y=1)\\
&amp;=\frac{p(free=0, y=1)}{p(y=1)}\\
&amp;=\frac{count\ of\ emails\ have\ no\ word\ &quot;free&quot;\ in\
spams}{count\ of\ spam\ emails}
\end{align}\]</span> As long as we have computed all of these values, we
can use the equation to decide which email should be trashed. Suppose we
have computed the probabilites of <span
class="math inline">\(p1\)</span> and <span
class="math inline">\(p2\)</span>: <span
class="math display">\[\begin{align}
&amp;p1\triangleq p(x_1,x_2,...,x_N|y=1)p(y=1)\triangleq p(y=0)\cdot
\Pi_{i=1}^N p(x_i|y=0)=0.00091\\
&amp;p2\triangleq p(x_1,x_2,...,x_N|y=0)p(y=0)\triangleq p(y=1)\cdot
\Pi_{i=1}^N p(x_i|y=1)=0.00000032
\end{align}\]</span> Then we consider that the email is more of a spam
mail. ### Summary Today we have talked about how to run a Naive Bayes
algorithm to decide if an email is a spam. We suppose each word in a
mail is no of business of the other, which is a simple assumption named
<strong>conditional independence assumption</strong> but not the
reality(e.g. "coupon" maybe emerges with "save" and "money" due to their
inner association). However, the algorthm is very effective. ### Future
Thinking Suppose if a word "wooooo" haven't emerged in inbox, then the
probability will reach <span class="math inline">\(p1=p2=0\)</span>, how
to solve it? Suppose you have a task to differiate oranges from apples
and pears using color and shape, how to design the algorithm? suppose x
is continuous rather than discrete, Naive Bayes still works or not?</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/03/11/neural-network-ABC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/03/11/neural-network-ABC/" class="post-title-link" itemprop="url">Neural Network ABC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-03-11 19:46:55" itemprop="dateCreated datePublished" datetime="2017-03-11T19:46:55+01:00">2017-03-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="background">Background</h3>
<p>Deep learning is very popular recently, which is based on Neural
Network, an old algorithm that had degraded for years but is resurging
right now. We talk about some basic concept about Neural network today,
hoping supply a intuitive perspective of it.</p>
<p>Before beginning, I'd like to introduce you an exicting product which
help those who are blind see the world. BrianPort, which is invented by
Wicab, uses you tougue to see the world. Tongue array contains 400
electrodes and is connected to the glasses. The product transfers from
light to electric signal. More than 80% blind persons could pass through
the block during the experiments. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-035015.jpg" /></p>
<p>In fact, Wicab takes advantage the mechanism of neural network of our
brain. There are 86 billion neuron in our brain. We can smell, see, hear
the world just because of these neurons. They are connect to each other
to help us sense the world. Algorithm Neural Network is a way of mimic
the mechanism of our brain.</p>
<h3 id="intuition">Intuition</h3>
<p>Let's start from the easiest model, we get <span
class="math inline">\(a_1\)</span> in two steps: step1: <span
class="math inline">\(z_1=w_1x_1+w_2x_2+w_3x_3\)</span> step2: <span
class="math inline">\(a_1=\frac{1}{1+e^{(-z)}}\)</span> In addition, we
add a bias <span class="math inline">\(w_0\)</span> to the calculate.
After letting <span class="math inline">\(x_0=1\)</span>, then: <span
class="math display">\[z=w_0x_0+w_1x_1+w_2x_2+w_3x_3\]</span> We always
add a bias at each layer but the last to Neural Network. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-043241.jpg" /> If
we contrast this model with logistic regression model, ww find that
right now the to model is just the same: input every <span
class="math inline">\(x\)</span> represents a feature. In logistic
regression, we want to train a model <span
class="math inline">\(h_w(x)=\frac{1}{1+e^{-W^Tx}}\)</span>. The
simpliest Neural Network, the model is a little complex, but if we do
not take hidden layer into account, the model is just logistic
regression.</p>
<h3 id="neural-network">Neural Network</h3>
<p>To approach the authentic Neural Network, we add two more
nerons(<span class="math inline">\(a_2^{(2)}\)</span> and <span
class="math inline">\(a_1^{(3)}\)</span>) to logistic regression model.
Notice that the model inner green triangle box is just like logistic
regression demonstrated above. There are only two layers in Logistic
Regression, in contrast, we can add more layers like L2 layer. In Neural
Network, we call these layers hidden layers which are neither the
input(e.g. layer have <span class="math inline">\(x_1, x_2,
x_2\)</span>), nor the output <span class="math inline">\(h(x)\)</span>.
The figure below has only one hidden layer, though we can add many
hidden layers to the model. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-122045.jpg" />
Look at the figure above, let's look at the definition of Neural
Network, take <span class="math inline">\(w_{12}^{(1)}\)</span> for
example, the subscript <span class="math inline">\(_{12}\)</span>
represents the weight from the former layer <span
class="math inline">\(2nd\)</span> unit to the current layer <span
class="math inline">\(1st\)</span> unit. The superscript <span
class="math inline">\(^1\)</span> represents former layer is layer L1.
These <span class="math inline">\(w\)</span> are named weights of Neural
Network. The sigmoid function <span
class="math inline">\(f=\frac{1}{1+e^{-x}}\)</span> is activation
function. We can choose other activation function such as symmetrical
sigmoid <span
class="math inline">\(S(x)=\frac{1-e^{-x}}{1+e^{-x}}\)</span>. Now let's
think about how to calculate <span class="math inline">\(h(x)\)</span>,
for the L2 layer, we have: <span class="math display">\[\begin{align}
&amp; z_1^{(2)}=w_{10}^{(1)}x_0 + w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2
+  w_{13}^{(1)}x_3\\
&amp; z_2^{(2)}=w_{20}^{(1)}x_0 + w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2
+  w_{23}^{(1)}x_3\\
&amp; a_1^{(2)} = g(w_{10}^{(1)}x_0 + w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2
+  w_{13}^{(1)}x_3)=g(z_1^{2})\\
&amp; a_2^{(2)} = g(w_{20}^{(1)}x_0 + w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2
+  w_{23}^{(1)}x_3)=g(z_2^{2})
\end{align}\]</span> Here, <span class="math inline">\(g()\)</span> is
the activation function. Notice that if we use matrices represent the
equation, result will be simpler: <span class="math display">\[a^{(2)} =
g(z^{(2)}) = g(W^{(1)} a^{(1)})\]</span> Here, we let <span
class="math inline">\(a_i^{(1)}=x_i\)</span>. We can conclude one more
step, for layer k, we have: <span class="math display">\[a^{(k)} =
g(z^{(k)}) = g(W^{(k-1)} a^{(k-1)})\]</span> Then for the L3 Layer, we
have only one neural: <span class="math display">\[\begin{align}
h(x) = a_1^{3}=g(w_{10}^{(1)}a_0^{(2)} + w_{11}^{(1)}a_1^{(2)} +
w_{12}^{(1)}a_2^{(2)})=g(z_1^{3})
\end{align}\]</span> If we substitute <span
class="math inline">\(a_1^{2}\)</span> and <span
class="math inline">\(a_2^{2}\)</span> for elme <span
class="math inline">\(h(x)\)</span>, we have: <span
class="math display">\[\begin{align}
h(x)=a_1^{3}=g(w_{10}^{(1)}\cdot 1 + w_{11}^{(1)}
\cdot g(z_1^{(2)})+ w_{12}^{(1)}\cdot g(z_2^{(2)}))
\end{align}\]</span> The formula show that we use <span
class="math inline">\(g()\)</span> function once and once again to nest
the input, and compute the output eventaully. It is rather a non-linear
classifier than linear classifier such as Linear Regression and Logistic
Regression.</p>
<h3 id="more-complicated-network">More Complicated Network</h3>
<p>A Neural Network can be very complex, as long as we add more hidden
layer into the network, the figure showed below is a neural network
which has 20 layers, which means it has 1 input layer, 1 output layer
and 18 hidden layers. From the connected weight we can imagine how much
many weight we would calculate if we want to train such a big Neural
Network. Notice that we add a bias subscript with zero on each layer
except the output layer. And in each layer, we can add different amount
of nerons. If we want to recognize numer image in zipcode from 0~9, we
can design the Neural Network with 10 outputs in the output layer. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-135243.jpg" /></p>
<h3 id="simple-applications">Simple Applications</h3>
<p>This section, I'd like to construct a Neural Network to simulate a
logic gate. Remember that bias <span class="math inline">\(x_0\)</span>
is always <span class="math inline">\(1\)</span>. Now let set <span
class="math inline">\(w_{10},\)</span><span
class="math inline">\(w_{11}\)</span> and <span
class="math inline">\(w_{12}\)</span>, and find what will h(x) become:
<span
class="math display">\[w_{10}=-30\,,w_{11}=20\,,w_{12}=20\,\]</span></p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(z_1\)</span></th>
<th><span class="math inline">\(a_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>-30</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>-10</td>
<td>0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>-10</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>10</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Here we take advantge the property of sigmoid function, <span
class="math inline">\(g(-10)=4.5\times 10^{-5}\approx 0\)</span> and
<span class="math inline">\(g(10)=0.99995\approx 1\)</span>. From the
table we have constructed an <span class="math inline">\(AND\)</span>
logic gate. It is easy to construct an <span
class="math inline">\(OR\)</span> logic gate. We just set: <span
class="math display">\[w_{10}=-30\,,w_{11}=50\,,w_{12}=50\,\]</span>
Then we get an <span class="math inline">\(OR\)</span> logic gate. We
can construct <span class="math inline">\(NOR\)</span> gate as well,
just set: <span
class="math display">\[w_{10}=10\,,w_{11}=-20\,,w_{12}=-20\,\]</span>
Question: can we construct a <span class="math inline">\(XOR\)</span>
gate? <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-09-052259.jpg" /> In
fact, we can get a more powerful logic gate through adding more hidden
layers. Only 2 layers of Neural Network can not construct a <span
class="math inline">\(XOR\)</span> gate but 3 layers can. Neural Network
shown below can implement function as <span
class="math inline">\(XOR\)</span> logic gate. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-09-085822.jpg" />
The weights matrices is as followed, we can testify through table
listed. <span class="math display">\[\begin{align}
&amp;W^{(1)}=\begin{bmatrix}-30&amp;20&amp;20\\
10&amp;-20&amp;-20
\end{bmatrix}\\
&amp;W^{(2)}=\begin{bmatrix}10&amp;-20&amp;-20
\end{bmatrix}
\end{align}\]</span></p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(a_1^{(2)}\)</span></th>
<th><span class="math inline">\(a_2^{(2)}\)</span></th>
<th><span class="math inline">\(a_1^{(3)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>From examples we have seen, hope you can gain intuition about Neural
Network. We can generate more abstract features through adding hidden
layers. ### Summerize Today we used Logistic Regression adding hidden
layers to generate Neural Network. Then we talked about how to represent
a Neural Network. In the end, we found that Neural Network can simulate
logic gate. We do not talk about how to train a Neural Network here.
Usually we use Backpropagation Algorithm to train a Neural Network.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>https://www.coursera.org/learn/machine-learning</li>
<li>《Neural Networks》by Raul Rojas</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/03/04/linear-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/03/04/linear-regression/" class="post-title-link" itemprop="url">Linear Regression for Trump</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-03-04 20:36:45" itemprop="dateCreated datePublished" datetime="2017-03-04T20:36:45+01:00">2017-03-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr />
<h3 id="introduction">Introduction</h3>
<p>In the end of 2016, Trump occupied the presidency. We are always
thinking about how he will build the wall between US and Mexico, Today,
I'd like to compute how many resources he would cost if he truly began
to build the wall using linear regression.</p>
<h3 id="data">Data</h3>
<p>The table below is some basic information about Chinese GreatWall as
well as one item about Hadrian's Wall：</p>
<table>
<thead>
<tr class="header">
<th>Age</th>
<th>people(1000)</th>
<th>Years</th>
<th>Length(KM)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qin Dynasty</td>
<td>300</td>
<td>15</td>
<td>5000</td>
</tr>
<tr class="even">
<td>Han Dynasty</td>
<td>500</td>
<td>100</td>
<td>5000</td>
</tr>
<tr class="odd">
<td>North Northern Dynasties</td>
<td>1800</td>
<td>12</td>
<td>2800</td>
</tr>
<tr class="even">
<td>Sui Dynasty</td>
<td>1280</td>
<td>30</td>
<td>350</td>
</tr>
<tr class="odd">
<td>Ming Dynasty</td>
<td>3000</td>
<td>40</td>
<td>885</td>
</tr>
<tr class="even">
<td>Hadrian's Wall</td>
<td>18</td>
<td>14</td>
<td>117</td>
</tr>
</tbody>
</table>
<p>We use matrices represent the resources and the length of these
walls. Each row of <span class="math inline">\(x\)</span> represents the
quantity of (1000 people and year) men and years, and each row of <span
class="math inline">\(y\)</span> denotes the length of
walls(KiloMeter)</p>
<p><span class="math display">\[\begin{align}
X=\begin{bmatrix}
5000\\
5000\\
2800\\
350\\
8851\\
117
\end{bmatrix}
y=
\begin{bmatrix}
300\*15\\
500\*100\\
1800\*12\\
1280\*30\\
3000\*40\\
18\*14
\end{bmatrix}
\end{align}\]</span></p>
<p>Let's draw the picture of these data: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">X=[5000;5000;2800;350;8851;117];</span><br><span class="line">y=[300\*15;500\*100; 1800\*12;1280\*30;3000\*40; 18\*14];</span><br><span class="line">xlabel(&quot;x (km)&quot;);</span><br><span class="line">hold on;</span><br><span class="line">ylabel(&quot;y (k people year)&quot;);</span><br><span class="line">plot(X,y,&quot;ro&quot;, &quot;MarkerFaceColor&quot;, &quot;b&quot;);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-01-113516.jpg" /> We
hope find a mapping <span class="math inline">\(x\rightarrow
f(x)\)</span> (<span class="math inline">\(x\)</span> denotes the length
of walls, <span class="math inline">\(y\)</span> denotes the cost of
resources)drawing a line as the figure above which meets all data best.
When we encounter new data(e.g. new length of wall), we hope <span
class="math inline">\(f(x)\)</span> will help us find how many resources
will we cost. This is the goal of linear regression.</p>
<h3 id="definition">Definition</h3>
<p>First of all, we assume that the line have the form as followed:
<span class="math display">\[h\_{\theta}(x)=\theta_0+\theta_1
x_1+\theta_2 x_2 + \cdots + \theta_nx_n\]</span> Specifically, under our
circumstance, there is only one <span
class="math inline">\(x\)</span>(the length of walls), then we will have
the form as below: <span
class="math display">\[h\_{\theta}(x)=\theta_0+\theta_1x\]</span> In
fact, there are many factor infulence the outcome of the cost of wall
besides people and time. Tools we use and the economy as well as terrain
where building the wall. if we add these factors, maybe the assumption
looks like: <span
class="math display">\[h\_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_2\]</span>
<span class="math inline">\(x_1\)</span>denotes the length of wall,
while <span class="math inline">\(x_2\)</span> and <span
class="math inline">\(x_3\)</span> denote economy and terrian condition
respectively. For simplicity, we consider only the length of wall.</p>
<p>In general, we add a <span class="math inline">\(x_0=1\)</span> to
the equation, the we have: <span
class="math display">\[h\_{\theta}(x)=\theta_0x_0+\theta_1 x_1 + \cdots
+ \theta_nx_n
=\sum\_{i=1}^{n}\theta_ix_i\]</span> If we use matrices represent the
equation, then <span class="math display">\[\begin{align}
\Theta=\begin{bmatrix}
\theta_0\\
\theta_1\\
\cdots\\
\theta_n
\end{bmatrix}
\end{align}\]</span> thus, <span class="math display">\[\begin{align}
h\_{\theta}(x)=\sum\_{i=1}^{n}\theta_ix_i=
\begin{bmatrix}
\theta_0\theta_1\cdots\theta_n
\end{bmatrix}
\begin{bmatrix}
x_0\\
x_1\\
\cdots\\
x_n
\end{bmatrix}
=\Theta^T\overrightarrow{X}
\end{align}\]</span> Notice that <span
class="math inline">\(\Theta^T\)</span> is a <span
class="math inline">\(1\times n\)</span> matrices while <span
class="math inline">\(\overrightarrow{X}\)</span> is a <span
class="math inline">\(n\times1\)</span> matrices. We get a real number
after multiply two matrices.</p>
<h3 id="cost-function">Cost Function</h3>
<p>We hope find a way to find the best function <span
class="math inline">\(h\_\theta(x)\)</span> fit these data. Notice that
if <span class="math inline">\(|(h\_\theta(x_i)-y_i)|=0\)</span> for
each <span class="math inline">\(x\)</span>, then the function fit data
absolutely. In practice, if we minimize <span
class="math inline">\(|(h\_\theta(x_i)-y_i)|\)</span>, we can find the
best fit to original data. An optional cost function is square cost
function: <span
class="math display">\[J(\theta)=\frac{1}{2m}\sum\_{i=1}^{m}(h\_\theta(x_i)-y_i)^2\]</span>
As long as we can minimize <span
class="math inline">\(J(\theta)\)</span> with respect to <span
class="math inline">\(\theta\)</span>, can we find the best <span
class="math inline">\(\theta\)</span> fit original data. The cost
function looks like as followed: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">m = size(X,1)</span><br><span class="line">XX = [ones(m,1),X];</span><br><span class="line">theta0 = linspace(-100,6000, 100);    </span><br><span class="line">theta1 = linspace(-20,36, 100);</span><br><span class="line">l0=length(theta0);                               </span><br><span class="line">l1=length(theta1);                               </span><br><span class="line">J_vs = zeros(l0,l1);                             </span><br><span class="line">for i=1:l0                                             </span><br><span class="line">    for j=1:l1                                         </span><br><span class="line">        t = [theta0(i); theta1(j)];               </span><br><span class="line">        J_vs(i,j) = cost(XX, y, t);             </span><br><span class="line">    end                                                 </span><br><span class="line">end                                                     </span><br><span class="line">figure;                                                 </span><br><span class="line">J_vs = J_vs&#x27;;                                       </span><br><span class="line">surfc(theta0, theta1, J_vs);                 </span><br><span class="line">colorbar;                                             </span><br><span class="line">xlabel(&#x27;\theta_0&#x27;);                                </span><br><span class="line">ylabel(&#x27;\theta_1&#x27;);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-02-28-115912.jpg" /></p>
<p>The figure has a bowl shape, and we can find the minimum of it both
using matrices approach and gradient descent.</p>
<h3 id="matrics-solution">Matrics Solution</h3>
<p>Remember that the cost function <span
class="math inline">\(J(\theta)=\frac{1}{2m}\sum\_{i=1}^{m}(h\_\theta(x_i)-y_i)^2\)</span>,
It is more convinient if we apply <span
class="math inline">\(J(\theta)\)</span> matrics form, then: <span
class="math display">\[\begin{align}
J(\theta)=\frac{1}{2m}(h\_\theta(x)-y)^T\cdot(h\_\theta(x)-y)\\   
=\frac{1}{2m}(X\Theta-y)^T\cdot(X\Theta-y)
\end{align}\]</span> We can take the derivative of <span
class="math inline">\(J(\theta)\)</span> and let it be <span
class="math inline">\(\overrightarrow{0}\)</span>. It is a little
complex if we take the derivative of the last equation about <span
class="math inline">\(J(\theta)\)</span>. Let's do the derivation a easy
way, but not a total Matric solution. If we do the derivation on the
sum, we can get: <span
class="math display">\[\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum\_{i=1}^m(h\_\theta(x_i)-y_i)x_j\]</span>
Then, <span class="math display">\[\begin{align}
\frac{\partial}{\partial\Theta}J(\theta)=
\begin{bmatrix}
\frac{\partial}{\partial\theta_1}J(\theta)\\
\frac{\partial}{\partial\theta_2}J(\theta)\\
\frac{\partial}{\partial\theta_3}J(\theta)\\
\cdots\\
\frac{\partial}{\partial\theta_n}J(\theta)
\end{bmatrix}=\overrightarrow{0}
\end{align}\]</span> Now, we transfer the sum into matrices form: <span
class="math display">\[\frac{\partial}{\partial\Theta}J(\theta)=X^T(X\Theta-y)=X^TX\Theta-X^Ty=\overrightarrow{0}\]</span>
thus, we can get the solution of <span
class="math inline">\(\Theta\)</span>: <span
class="math display">\[\Theta=(X^TX)^{(-1)}X^Ty\]</span> Let's look at
the solution of Matrices: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">xlabel(&quot;x (km)&quot;);</span><br><span class="line">hold on;</span><br><span class="line">ylabel(&quot;y (k people year)&quot;);</span><br><span class="line">plot(X,y,&quot;ro&quot;, &quot;MarkerFaceColor&quot;, &quot;b&quot;);</span><br><span class="line">a=linspace(0,10000,100);</span><br><span class="line">t = pinv(XX&#x27;*XX)*XX&#x27;*y;</span><br><span class="line">b=t&#x27;*[ones(1, length(a)); a];</span><br><span class="line">plot(a,b);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-01-114724.jpg" /></p>
<p>And <span class="math inline">\(\theta_0\)</span> and <span
class="math inline">\(\theta_1\)</span> is as followed: <span
class="math display">\[\begin{align}
\Theta=\begin{bmatrix}
2573\\
9.9\\
\end{bmatrix}
\end{align}\]</span> That means <span
class="math inline">\(h\_\theta(x)=2573+9.9x\)</span>, Right Now we
subsitute 3169(km) for <span class="math inline">\(x\)</span>: <span
class="math display">\[h\_\theta(3169)=2573+9.9\*3169=33946(k\  people\
year)\]</span> In other word, Donald Trump need 3,394,600 people
continously build 10 years in order to finish the GreatWall between US
and Mexico. Surely he can stimulate 33,946,000 people, it only take him
one year to finished the wall. ### Gradient Descent Solution Let's look
at the cost function one more time. If we first choose a random <span
class="math inline">\(\Theta\)</span>, say we have <span
class="math inline">\(\theta_0\&amp;\theta_1\)</span> located in point
1. Assume you stand at ponit 1, you want to find a way to go down the
vally. Surely you can not see the land-form completely. But you can look
around at point 1 and find a steepest direction, then you go that way a
baby step. Right now, you are at point 2 after the first step. You do
the same find a steepest direction and make another baby step. After
several steps, you are now standing at point 5. When you are looking
around you find that where you are standing is almost flat. Now we can
stop the iteration, and we have found a reasonable <span
class="math inline">\(\Theta\)</span> which makes the <span
class="math inline">\(J(\theta)\)</span> has a minmum value. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-02-134554.jpg" />
What I described is a method named Gradient Descent which is very easy
way to find minima of <span class="math inline">\(J(\theta)\)</span>.
The step is as followed:</p>
<ol type="1">
<li>Find a reasonable cost function J().</li>
<li>Take the partial derivative of <span
class="math inline">\(J(\theta)\)</span> for each <span
class="math inline">\(\theta_j\)</span></li>
<li>Choose a moderate <span class="math inline">\(\alpha\)</span> to
constrain the step size.</li>
<li>Take a baby step, the direction is from the derivative and the size
is determined by derivative and parameter <span
class="math inline">\(alpha\)</span></li>
<li>update the value of <span class="math inline">\(\Theta\)</span>,
check if we find the minimum. If not, return to the step 4, otherwise,
stop the algo</li>
</ol>
<p>Concretely, the algorithm using octave is listed below: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% octave: The derivative function</span><br><span class="line">function g = gradient(X, y, theta)</span><br><span class="line">    m = size(X,1);</span><br><span class="line">    hx = X*theta;</span><br><span class="line">    g = (1/m)*X&#x27;*(hx-y);</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% octave: I just take 100 steps without checking when to stop.</span><br><span class="line">theta=[0;0];</span><br><span class="line">alpha=0.00000003</span><br><span class="line">for i=1:100</span><br><span class="line">    g = gradient(XX,y,theta);</span><br><span class="line">    theta = theta - alpha*g</span><br><span class="line">end</span><br></pre></td></tr></table></figure> Under the circumstance above, <span
class="math inline">\(\theta_0=0.0048\)</span> and <span
class="math inline">\(\theta_1=10.33\)</span>. If we substitute the
parameters for <span class="math inline">\(h\_\theta(x)\)</span>, the
answer that Trump would take to build the wall is 32745.56$$1000 people
year. ### Summarize Today we have talk about how to use linear
regression to model a problem of building Great Wall. We have also talk
about how to resovle the problem through minimize the cost function both
using Matrices solution and Gradient Descent.</p>
<h3 id="appendix">Appendix</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% octave --cost function:cost.m</span><br><span class="line">function J=cost(X, y, theta)</span><br><span class="line">m = size(X,1);</span><br><span class="line">J=0;</span><br><span class="line">hx=X*theta;</span><br><span class="line">J = 1/(2*m)*(hx-y)&#x27;*(hx-y);</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/02/25/Linear-Algebra-I/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/02/25/Linear-Algebra-I/" class="post-title-link" itemprop="url">Linear Algebra (I)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-02-25 20:11:21" itemprop="dateCreated datePublished" datetime="2017-02-25T20:11:21+01:00">2017-02-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="motivation">Motivation</h3>
<p>When I was learning Linear Algebra in University, I found it was hard
to understand what would applications of this course. I remember that we
learned determinant on Chapter 1, then we learned a lot of other concept
such as Rank and EigenValue. I could only memorize one after another
theorems without knowing why these happened to appear this way. However,
when I was learning matrices from lectures taught by <strong>Gilbert
Strang</strong>, I found every conclusion is so nature that I could
easily understand and associate a bunch of concepts with each other. I'd
like to conclude what I had learned from him as well as hoping that
someone can learn from that.</p>
<h3 id="why-this-subject-exists">Why this subject exists?</h3>
First of all, what is the origin of linear algebra? Why we make the
world increasingly complicated? The truth is that the purpose of
mathmatics is to make the world simpler, not the opposite, Linear
Algebra as well. For example, we want to solve equations with a lot of
variables, the original way is to write all the parameter out：
<span class="math display">\[\begin{cases}
3x+4y+6z = 0\\\\
2x+3y+4z = 24\\\\
x+y+z = 15
\end{cases}\]</span>
<p>If there are only 3 parameters, the problems is not so hard for us to
handle. But what if there are 100 parameters? When we transfer the
equation to matrices, the outcome becomes so grateful: <span
class="math display">\[\begin{align}
\begin{bmatrix}
3&amp;4&amp;6\\\\
2&amp;3&amp;4\\\\
1&amp;1&amp;1
\end{bmatrix}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=\begin{bmatrix}0\\\\24\\\\15\end{bmatrix}
\end{align}\]</span> ### Solve the equation group Now, we try to solve
the problems, our instinction is to draw the three planes, under most
circumstance, there will be a cross point, octave code and the plot is
as followed: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">octave:1&gt; x=linspace(-20, 20, 100);</span><br><span class="line">octave:2&gt; [xx, yy] = meshgrid(x,y);</span><br><span class="line">octave:3&gt; z = 15-xx-yy;</span><br><span class="line">octave:4&gt; mesh(xx, yy, z);</span><br><span class="line">octave:5&gt; l = (-3*xx-4*yy)./6;</span><br><span class="line">octave:6&gt; mesh(xx,yy, l);</span><br><span class="line">octave:7&gt; t = (24-(2*xx+3*yy))./4;</span><br><span class="line">octave:8&gt; mesh(xx,yy,z);</span><br><span class="line">octave:9&gt; hold on;</span><br><span class="line">octave:11&gt; mesh(xx,yy,t);</span><br><span class="line">octave:12&gt; mesh(xx,yy,l);</span><br><span class="line">octave:13&gt; xlabel(&quot;x&quot;)</span><br><span class="line">octave:14&gt; ylabel(&quot;y&quot;)</span><br><span class="line">octave:15&gt; zlabel(&quot;z&quot;)</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-02-24-065543.jpg" /></p>
<p>The answer of this equation group is that <span
class="math display">\[\begin{align}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=\begin{bmatrix}
-18\\\\
72\\\
-39
\end{bmatrix}
\end{align}\]</span>x} \end{align}</p>
<h3 id="another-perspective">Another perspective</h3>
<p>We can find another perspective to solve this equation: see the
equation as a combination of columns, that means: <span
class="math display">\[\begin{align}
x\*\begin{bmatrix}
3\\\\
2\\\\
1
\end{bmatrix}+y\*
\begin{bmatrix}
4\\\\
3\\\\
1
\end{bmatrix}+z\*
\begin{bmatrix}
6\\\\
4\\\\
1
\end{bmatrix}=
\begin{bmatrix}
0\\\\
24\\\\
15
\end{bmatrix}
\end{align}\]</span> Now, we can acquire the new vector through three
vectors' combination, from the plot blow demonstrates that we need to
find a combination of red&amp;blue&amp;black vectors to generate another
vector(the green one). Obviously, <span
class="math inline">\(x=-18,y=72,z=-39\)</span> is the only solution of
the combination problem. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">octave:1&gt; a = quiver3(0, 0, 0, 3,2,1);</span><br><span class="line">octave:2&gt; hold on;</span><br><span class="line">octave:3&gt; b = quiver3(0, 0, 0, 4,3,1);</span><br><span class="line">octave:4&gt; c = quiver3(0, 0, 0, 6,4,1);</span><br><span class="line">octave:5&gt; set(b, &quot;Color&quot;, &quot;r&quot;);</span><br><span class="line">octave:6&gt; set(c, &quot;Color&quot;, &quot;k&quot;);</span><br><span class="line">octave:7&gt; set(a, &quot;maxheadsize&quot;, 0.02);</span><br><span class="line">octave:8&gt; set(b, &quot;maxheadsize&quot;, 0.02);</span><br><span class="line">octave:9&gt; set(c, &quot;maxheadsize&quot;, 0.02);</span><br><span class="line">octave:10&gt; d = quiver3(0, 0, 0, 0,24,15);</span><br><span class="line">octave:11&gt; set(d, &quot;Color&quot;, &quot;g&quot;);</span><br><span class="line">octave:12&gt; set(d, &quot;maxheadsize&quot;, 0.02);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-02-24-103145.jpg" /></p>
How to solve equations using program, and is there always an answer of
equations? An easy way is to do elimination of these there equations.
What now I want to do right now is to change these equation as follows:
<span class="math display">\[\begin{cases}
3x+4y+6z = 0\\\\
(1/3)y+0z = 24\\\\
-z = 39
\end{cases}\]</span>
<p>Step1, calclate z, step2, calclate y, step3, calclate x. We first
leave a variables z alone with an answer in the third equation. Then, we
substitute z to the second equation, and get y. At last we substitute x
and y to the first equation and get x. if we represent above through
matrices language, it is like this: <span
class="math display">\[\begin{align}
\begin{bmatrix}
3&amp;4&amp;6\\\\
0&amp;1/3&amp;0\\\\
0&amp;0&amp;-1
\end{bmatrix}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=
\begin{bmatrix}
0\\\\
24\\\\
39
\end{bmatrix}
\end{align}\]</span> The triangle at bottom-left side is all zeros which
ensure we can solve equations one by one. The process is named
back-substitution. ### Additional thinkings But what if the equation
group looks like: <span class="math display">\[\begin{align}
\begin{bmatrix}
1&amp;2&amp;3\\\\
1&amp;2&amp;3\\\\
1&amp;2&amp;3
\end{bmatrix}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=
\begin{bmatrix}
5\\\\
6\\\\
7
\end{bmatrix}
or
\begin{bmatrix}
3&amp;4\\\\
4&amp;5\\\\
5&amp;6\\\\
6&amp;7\\\\
7&amp;9
\end{bmatrix}
\begin{bmatrix}
x\\\\
y
\end{bmatrix}=
\begin{bmatrix}
5\\\\
5\\\\
5\\\\
5\\\\
5
\end{bmatrix}
\end{align}\]</span><br />
Next time we will talk about LU decomposition and Rank of a matrix.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/02/18/Gredient-Descent-in-Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/02/18/Gredient-Descent-in-Logistic-Regression/" class="post-title-link" itemprop="url">Gradient Descent in Logistic Regression</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-02-18 14:33:10" itemprop="dateCreated datePublished" datetime="2017-02-18T14:33:10+01:00">2017-02-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>As a simple model, Logistic regression is very popular in Machine
Learning, especially in computer industry while gradient descent is more
of popularity as well among dozens of optimization methods. The aim of
this article is to demonstrate how to reach these formulas
conclusion.</p>
<h3 id="model-a-problem">1. Model a problem</h3>
<p>First of all, we know that Logistic regression is a statistical
model. Suppose we have a coin, we'd like to know the probability of the
head and tail. Bernoulli distribution is a good distribution to model
the coin. The hyphothsis is as followed: <span
class="math display">\[P(y=1|\mu)=\mu  \qquad(1)\]</span>.</p>
<p><span class="math inline">\(y=1\)</span> means the probability of the
head, then <span class="math inline">\(y=0\)</span> is that of the tail.
<span class="math inline">\(\mu\)</span> here is the parameter, if <span
class="math inline">\(\mu=0.5\)</span>, then the probability of head
equals to 0.5, which means we have half the probabilty reach the head as
well as the tail. Now we have: <span
class="math display">\[P(y=0|\mu)=1-\mu\qquad(2)\]</span></p>
<p>If we take a look at these two formula, we can conclude a more
general formula: <span
class="math display">\[P(y|\mu)={\mu}^y\cdot{(1-\mu)}^{(1-y)}
\qquad(3)\]</span></p>
<p>Let's test it, <span class="math inline">\(y\)</span> only have two
value <span class="math inline">\(0\)</span> and <span
class="math inline">\(1\)</span>, if <span
class="math inline">\(y=1\)</span>, <span
class="math inline">\(P(y|\mu)=\mu\)</span>, othewise, <span
class="math inline">\(P(y|\mu)=1-\mu\)</span>. Suppose we have a dataset
<span class="math inline">\(D=\{y\_1,y\_2,y\_3...y\_m\}\)</span>
observed values, we want to estimate the parameter <span
class="math inline">\(\mu\)</span>, the problems become following
question: <span class="math display">\[P(\mu|D)=?\qquad (4)\]</span></p>
<h3 id="estimate-parameter">2. Estimate Parameter</h3>
<p>We use Bayes formula to transfer the problem to another: <span
class="math display">\[P(\mu|D)=\frac{P(D|\mu)\cdot
P(\mu)}{P(D)}=\frac{P(y\_1,y\_2,y\_3...,y\_m|\mu)\cdot
P(\mu)}{P(D)}\qquad (5)\]</span></p>
<p>Here denominator <span class="math inline">\(P(D)\)</span> is a
constant as well as <span class="math inline">\(P(\mu)\)</span> if we
see <span class="math inline">\(\mu\)</span> as variable rather than a
distribution. Then we have: <span
class="math display">\[P(\mu|D)\triangleq P(y\_1,y\_2,y\_3...,y\_m|\mu)
\qquad(6)\]</span></p>
<p>We can find a series of <span class="math inline">\(\mu\)</span>(e.g,
<span class="math inline">\(\mu=0.1, \mu=0.72\)</span>), but we should
find the maximum <span class="math inline">\(P(\mu|D)\)</span>, because
when <span class="math inline">\(P(\mu|D)\)</span> reaches its max means
<span class="math inline">\(\mu\)</span> most likely is the right
parameter. We assume that each <span class="math inline">\(y\)</span> is
independent from the others given the parameter <span
class="math inline">\(\mu\)</span>. then we have: <span
class="math display">\[P(\mu|D)\triangleq
P(y\_1|\mu)P(y\_2|\mu)P(y\_3|\mu)...,P(y\_m|\mu)={\prod}\_{i=1}^{m}P(y\_i|\mu)\qquad(7)\]</span></p>
<p>Now, the problem is to maximize the <span
class="math inline">\({\prod}\_{i=1}^{m}P(y\_i|\mu)\)</span>, that is:
<span class="math display">\[L =
\underset{\mu}{argmax}{\prod}\_{i=1}^{m}P(y\_i|\mu)=\underset{\mu}{argmax}{\prod}\_{i=1}^{m}[{\mu}^y\cdot{(1-\mu)}^{(1-y)}]
\qquad(8)\]</span></p>
<p>It is a little hard to find the maximum, we change the problem to
another way: <span class="math display">\[L
=\underset{\mu}{argmax}ln({\prod}\_{i=1}^{m}[{\mu}^y\cdot{(1-\mu)}^{(1-y)}])\qquad(9)\]</span></p>
<p>Then we have: <span class="math display">\[L =
\underset{\mu}{argmax}{\sum}\_{i=1}^{m}[{y ln(\mu}) + (1-y)
ln({1-\mu)}]\qquad(10)\]</span></p>
<h3 id="logistic-regression">3. Logistic Regression</h3>
<p>If we let <span class="math inline">\(\mu=h\_{\theta}(x)\)</span>,
then we have:</p>
<p><span class="math display">\[L =
\underset{\theta}{argmax}{\sum}\_{i=1}^{m}[{y ln(h\_{\theta}(x)}) +
(1-y) ln({1-h\_{\theta}(x))}]\qquad(11)\]</span></p>
<p>Here, <span
class="math display">\[h\_{\theta}(x)=\frac{1}{1+e^{-(\theta\_0
x\_0+\theta\_1 x\_1+\theta\_2 x\_2+\theta\_m
x\_m)}}=\frac{1}{1+e^{-{\Theta}X}}\qquad (12)\]</span></p>
<p><span class="math inline">\(h\_{\theta}(x)\)</span> is named sigmoid
function and now we use parameter <span
class="math inline">\(\theta\)</span> to estimate <span
class="math inline">\(\mu\)</span>. Simgoid function is a pretty good
function, derivative of which is elegant. we let <span
class="math inline">\(\sigma=\frac{1}{1+e^{-x}}\)</span>, then:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{\sigma}}{\partial x}
&amp;= \frac{-1}{(1+e^{-x})^2}\cdot e^{-x}\cdot(-1)\\\\
&amp;=\frac{e^{-x}}{(1+e^{-x})^2}=\frac{1+e^{-x}-1}{(1+e^{-x})^2}\\\\
&amp;=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^2}\\\\
&amp;=\sigma(1-\sigma)\qquad \qquad   (13)
\end{align}\]</span></p>
<h3 id="gradient-descent">4. Gradient Descent</h3>
<p>We transfer the maximizing to a minimizing problem, we define the
cost function:</p>
<p><span class="math display">\[J(\theta) =
-\frac{1}{m}{\sum}\_{i=1}^{m}[{yln(h\_{\theta}(x)}) +
(1-y)ln({1-h\_{\theta}(x))}]\qquad (14) \]</span></p>
<p>if we want to minimize <span
class="math inline">\(J(\theta)\)</span>, we need know the gradient of
<span class="math inline">\(\frac{\partial
J(\theta)}{\partial\theta\_j}\)</span>, that is: <span
class="math display">\[\begin{align}
\frac{\partial J(\theta)}{\partial\theta\_j}
&amp;=\frac{\partial}{\partial\theta\_j}(-\frac{1}{m}{\sum}\_{i=1}^{m}[yln(h\_\theta(x))
+ (1-y)ln(1-h\_\theta(x))])\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y}{h\_\theta(x)} +
\frac{y-1}{1-{h\_\theta(x)}} ]\frac{\partial h\_\theta
(x)}{\partial\theta\_j}\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[{\frac{y}{\sigma}} +
\frac{y-1}{1-\sigma} ]\frac{\partial\sigma}{\partial\theta\_j}\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-y\sigma}{\sigma(1-\sigma)}
+ \frac{\sigma
y-\sigma}{\sigma(1-\sigma)}]\frac{\partial\sigma}{\partial\theta\_j}\\\\
&amp;=
-\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-\sigma}{\sigma\cdot(1-\sigma)}]\frac{\partial\sigma}{\partial\theta\_j}\\\\
&amp;=
-\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-\sigma}{\sigma(1-\sigma)}]\cdot\sigma(1-\sigma)\frac{\partial\Theta
X}{\partial\theta\_j}\\\\
&amp;=
-\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-\sigma}{1}]\frac{\partial\Theta
X}{\partial\theta\_j}\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[y-\sigma]{X\_j}\\\\
&amp;= \frac{1}{m}{\sum}\_{i=1}^{m}[h\_{\theta}(x)-y]{X\_j} \qquad(15)
\end{align}\]</span></p>
<p>Notice that the gradient descent is same as that of square cost
function in Linear Regression, which is an extremely grace equation. ###
Summarize Today We have talked about how to derive cost function of
Logistic Regression, then we get the gradient descent equation of it.
Next step is to use gradient descent to iterate the minimum of the cost
function.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>https://www.coursera.org/learn/machine-learning/home/welcome</li>
<li>Parameter estimation for text analysis, Gregor Heinrich</li>
<li>Pattern Recognition and Machine Learning, Christopher M. Bishop</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2016/12/15/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2016/12/15/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2016-12-15 15:10:50" itemprop="dateCreated datePublished" datetime="2016-12-15T15:10:50+01:00">2016-12-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class=""></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Mingbo Cheng</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"chengmingbo/gitment-comments","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
