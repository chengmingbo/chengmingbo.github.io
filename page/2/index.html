<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chengmingbo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Mingbo">
<meta property="og:type" content="website">
<meta property="og:title" content="Mingbo">
<meta property="og:url" content="http://chengmingbo.github.io/page/2/index.html">
<meta property="og:site_name" content="Mingbo">
<meta property="og:description" content="Mingbo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Mingbo Cheng">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://chengmingbo.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Mingbo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mingbo</h1>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="/%5Bobject%20Object%5D" alt="Mingbo">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-slides"><a href="/slides/" rel="section"><i class="area-chart fa-fw"></i>slides</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mingbo Cheng</p>
  <div class="site-description" itemprop="description">Mingbo</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chengmingbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chengmingbo" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="http://www.flickering.cn/" title="http:&#x2F;&#x2F;www.flickering.cn&#x2F;" rel="noopener" target="_blank">flickering</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.zybuluo.com/codeep/note/163962" title="https:&#x2F;&#x2F;www.zybuluo.com&#x2F;codeep&#x2F;note&#x2F;163962" rel="noopener" target="_blank">mathjax grammar</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vividfree.github.io/" title="http:&#x2F;&#x2F;vividfree.github.io&#x2F;" rel="noopener" target="_blank">vividfree</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://colah.github.io/" title="http:&#x2F;&#x2F;colah.github.io&#x2F;" rel="noopener" target="_blank">colah</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.autonlab.org/tutorials" title="https:&#x2F;&#x2F;www.autonlab.org&#x2F;tutorials" rel="noopener" target="_blank">Andrew Moore</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://plot.ly/matlab/plot/" title="https:&#x2F;&#x2F;plot.ly&#x2F;matlab&#x2F;plot&#x2F;" rel="noopener" target="_blank">matlabplot</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.ryanzhang.info/blog/" title="http:&#x2F;&#x2F;www.ryanzhang.info&#x2F;blog&#x2F;" rel="noopener" target="_blank">Ryan’s Cabinet</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.cnblogs.com/jerrylead/tag/Machine%20Learning/" title="http:&#x2F;&#x2F;www.cnblogs.com&#x2F;jerrylead&#x2F;tag&#x2F;Machine%20Learning&#x2F;" rel="noopener" target="_blank">JerryLead</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://yxzf.github.io/" title="https:&#x2F;&#x2F;yxzf.github.io&#x2F;" rel="noopener" target="_blank">YXZF'S BLOG</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vonng.com/" title="http:&#x2F;&#x2F;vonng.com" rel="noopener" target="_blank">VONNG</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/03/11/neural-network-ABC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/03/11/neural-network-ABC/" class="post-title-link" itemprop="url">Neural Network ABC</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-03-11 19:46:55" itemprop="dateCreated datePublished" datetime="2017-03-11T19:46:55+01:00">2017-03-11</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="background">Background</h3>
<p>Deep learning is very popular recently, which is based on Neural
Network, an old algorithm that had degraded for years but is resurging
right now. We talk about some basic concept about Neural network today,
hoping supply a intuitive perspective of it.</p>
<p>Before beginning, I'd like to introduce you an exicting product which
help those who are blind see the world. BrianPort, which is invented by
Wicab, uses you tougue to see the world. Tongue array contains 400
electrodes and is connected to the glasses. The product transfers from
light to electric signal. More than 80% blind persons could pass through
the block during the experiments. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-035015.jpg" /></p>
<p>In fact, Wicab takes advantage the mechanism of neural network of our
brain. There are 86 billion neuron in our brain. We can smell, see, hear
the world just because of these neurons. They are connect to each other
to help us sense the world. Algorithm Neural Network is a way of mimic
the mechanism of our brain.</p>
<h3 id="intuition">Intuition</h3>
<p>Let's start from the easiest model, we get <span
class="math inline">\(a_1\)</span> in two steps: step1: <span
class="math inline">\(z_1=w_1x_1+w_2x_2+w_3x_3\)</span> step2: <span
class="math inline">\(a_1=\frac{1}{1+e^{(-z)}}\)</span> In addition, we
add a bias <span class="math inline">\(w_0\)</span> to the calculate.
After letting <span class="math inline">\(x_0=1\)</span>, then: <span
class="math display">\[z=w_0x_0+w_1x_1+w_2x_2+w_3x_3\]</span> We always
add a bias at each layer but the last to Neural Network. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-043241.jpg" /> If
we contrast this model with logistic regression model, ww find that
right now the to model is just the same: input every <span
class="math inline">\(x\)</span> represents a feature. In logistic
regression, we want to train a model <span
class="math inline">\(h_w(x)=\frac{1}{1+e^{-W^Tx}}\)</span>. The
simpliest Neural Network, the model is a little complex, but if we do
not take hidden layer into account, the model is just logistic
regression.</p>
<h3 id="neural-network">Neural Network</h3>
<p>To approach the authentic Neural Network, we add two more
nerons(<span class="math inline">\(a_2^{(2)}\)</span> and <span
class="math inline">\(a_1^{(3)}\)</span>) to logistic regression model.
Notice that the model inner green triangle box is just like logistic
regression demonstrated above. There are only two layers in Logistic
Regression, in contrast, we can add more layers like L2 layer. In Neural
Network, we call these layers hidden layers which are neither the
input(e.g. layer have <span class="math inline">\(x_1, x_2,
x_2\)</span>), nor the output <span class="math inline">\(h(x)\)</span>.
The figure below has only one hidden layer, though we can add many
hidden layers to the model. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-122045.jpg" />
Look at the figure above, let's look at the definition of Neural
Network, take <span class="math inline">\(w_{12}^{(1)}\)</span> for
example, the subscript <span class="math inline">\(_{12}\)</span>
represents the weight from the former layer <span
class="math inline">\(2nd\)</span> unit to the current layer <span
class="math inline">\(1st\)</span> unit. The superscript <span
class="math inline">\(^1\)</span> represents former layer is layer L1.
These <span class="math inline">\(w\)</span> are named weights of Neural
Network. The sigmoid function <span
class="math inline">\(f=\frac{1}{1+e^{-x}}\)</span> is activation
function. We can choose other activation function such as symmetrical
sigmoid <span
class="math inline">\(S(x)=\frac{1-e^{-x}}{1+e^{-x}}\)</span>. Now let's
think about how to calculate <span class="math inline">\(h(x)\)</span>,
for the L2 layer, we have: <span class="math display">\[\begin{align}
&amp; z_1^{(2)}=w_{10}^{(1)}x_0 + w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2
+  w_{13}^{(1)}x_3\\
&amp; z_2^{(2)}=w_{20}^{(1)}x_0 + w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2
+  w_{23}^{(1)}x_3\\
&amp; a_1^{(2)} = g(w_{10}^{(1)}x_0 + w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2
+  w_{13}^{(1)}x_3)=g(z_1^{2})\\
&amp; a_2^{(2)} = g(w_{20}^{(1)}x_0 + w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2
+  w_{23}^{(1)}x_3)=g(z_2^{2})
\end{align}\]</span> Here, <span class="math inline">\(g()\)</span> is
the activation function. Notice that if we use matrices represent the
equation, result will be simpler: <span class="math display">\[a^{(2)} =
g(z^{(2)}) = g(W^{(1)} a^{(1)})\]</span> Here, we let <span
class="math inline">\(a_i^{(1)}=x_i\)</span>. We can conclude one more
step, for layer k, we have: <span class="math display">\[a^{(k)} =
g(z^{(k)}) = g(W^{(k-1)} a^{(k-1)})\]</span> Then for the L3 Layer, we
have only one neural: <span class="math display">\[\begin{align}
h(x) = a_1^{3}=g(w_{10}^{(1)}a_0^{(2)} + w_{11}^{(1)}a_1^{(2)} +
w_{12}^{(1)}a_2^{(2)})=g(z_1^{3})
\end{align}\]</span> If we substitute <span
class="math inline">\(a_1^{2}\)</span> and <span
class="math inline">\(a_2^{2}\)</span> for elme <span
class="math inline">\(h(x)\)</span>, we have: <span
class="math display">\[\begin{align}
h(x)=a_1^{3}=g(w_{10}^{(1)}\cdot 1 + w_{11}^{(1)}
\cdot g(z_1^{(2)})+ w_{12}^{(1)}\cdot g(z_2^{(2)}))
\end{align}\]</span> The formula show that we use <span
class="math inline">\(g()\)</span> function once and once again to nest
the input, and compute the output eventaully. It is rather a non-linear
classifier than linear classifier such as Linear Regression and Logistic
Regression.</p>
<h3 id="more-complicated-network">More Complicated Network</h3>
<p>A Neural Network can be very complex, as long as we add more hidden
layer into the network, the figure showed below is a neural network
which has 20 layers, which means it has 1 input layer, 1 output layer
and 18 hidden layers. From the connected weight we can imagine how much
many weight we would calculate if we want to train such a big Neural
Network. Notice that we add a bias subscript with zero on each layer
except the output layer. And in each layer, we can add different amount
of nerons. If we want to recognize numer image in zipcode from 0~9, we
can design the Neural Network with 10 outputs in the output layer. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-135243.jpg" /></p>
<h3 id="simple-applications">Simple Applications</h3>
<p>This section, I'd like to construct a Neural Network to simulate a
logic gate. Remember that bias <span class="math inline">\(x_0\)</span>
is always <span class="math inline">\(1\)</span>. Now let set <span
class="math inline">\(w_{10},\)</span><span
class="math inline">\(w_{11}\)</span> and <span
class="math inline">\(w_{12}\)</span>, and find what will h(x) become:
<span
class="math display">\[w_{10}=-30\,,w_{11}=20\,,w_{12}=20\,\]</span></p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(z_1\)</span></th>
<th><span class="math inline">\(a_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>-30</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>-10</td>
<td>0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>-10</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>10</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Here we take advantge the property of sigmoid function, <span
class="math inline">\(g(-10)=4.5\times 10^{-5}\approx 0\)</span> and
<span class="math inline">\(g(10)=0.99995\approx 1\)</span>. From the
table we have constructed an <span class="math inline">\(AND\)</span>
logic gate. It is easy to construct an <span
class="math inline">\(OR\)</span> logic gate. We just set: <span
class="math display">\[w_{10}=-30\,,w_{11}=50\,,w_{12}=50\,\]</span>
Then we get an <span class="math inline">\(OR\)</span> logic gate. We
can construct <span class="math inline">\(NOR\)</span> gate as well,
just set: <span
class="math display">\[w_{10}=10\,,w_{11}=-20\,,w_{12}=-20\,\]</span>
Question: can we construct a <span class="math inline">\(XOR\)</span>
gate? <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-09-052259.jpg" /> In
fact, we can get a more powerful logic gate through adding more hidden
layers. Only 2 layers of Neural Network can not construct a <span
class="math inline">\(XOR\)</span> gate but 3 layers can. Neural Network
shown below can implement function as <span
class="math inline">\(XOR\)</span> logic gate. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-09-085822.jpg" />
The weights matrices is as followed, we can testify through table
listed. <span class="math display">\[\begin{align}
&amp;W^{(1)}=\begin{bmatrix}-30&amp;20&amp;20\\
10&amp;-20&amp;-20
\end{bmatrix}\\
&amp;W^{(2)}=\begin{bmatrix}10&amp;-20&amp;-20
\end{bmatrix}
\end{align}\]</span></p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(a_1^{(2)}\)</span></th>
<th><span class="math inline">\(a_2^{(2)}\)</span></th>
<th><span class="math inline">\(a_1^{(3)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>From examples we have seen, hope you can gain intuition about Neural
Network. We can generate more abstract features through adding hidden
layers. ### Summerize Today we used Logistic Regression adding hidden
layers to generate Neural Network. Then we talked about how to represent
a Neural Network. In the end, we found that Neural Network can simulate
logic gate. We do not talk about how to train a Neural Network here.
Usually we use Backpropagation Algorithm to train a Neural Network.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>https://www.coursera.org/learn/machine-learning</li>
<li>《Neural Networks》by Raul Rojas</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/03/04/linear-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/03/04/linear-regression/" class="post-title-link" itemprop="url">Linear Regression for Trump</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-03-04 20:36:45" itemprop="dateCreated datePublished" datetime="2017-03-04T20:36:45+01:00">2017-03-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr />
<h3 id="introduction">Introduction</h3>
<p>In the end of 2016, Trump occupied the presidency. We are always
thinking about how he will build the wall between US and Mexico, Today,
I'd like to compute how many resources he would cost if he truly began
to build the wall using linear regression.</p>
<h3 id="data">Data</h3>
<p>The table below is some basic information about Chinese GreatWall as
well as one item about Hadrian's Wall：</p>
<table>
<thead>
<tr class="header">
<th>Age</th>
<th>people(1000)</th>
<th>Years</th>
<th>Length(KM)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qin Dynasty</td>
<td>300</td>
<td>15</td>
<td>5000</td>
</tr>
<tr class="even">
<td>Han Dynasty</td>
<td>500</td>
<td>100</td>
<td>5000</td>
</tr>
<tr class="odd">
<td>North Northern Dynasties</td>
<td>1800</td>
<td>12</td>
<td>2800</td>
</tr>
<tr class="even">
<td>Sui Dynasty</td>
<td>1280</td>
<td>30</td>
<td>350</td>
</tr>
<tr class="odd">
<td>Ming Dynasty</td>
<td>3000</td>
<td>40</td>
<td>885</td>
</tr>
<tr class="even">
<td>Hadrian's Wall</td>
<td>18</td>
<td>14</td>
<td>117</td>
</tr>
</tbody>
</table>
<p>We use matrices represent the resources and the length of these
walls. Each row of <span class="math inline">\(x\)</span> represents the
quantity of (1000 people and year) men and years, and each row of <span
class="math inline">\(y\)</span> denotes the length of
walls(KiloMeter)</p>
<p><span class="math display">\[\begin{align}
X=\begin{bmatrix}
5000\\
5000\\
2800\\
350\\
8851\\
117
\end{bmatrix}
y=
\begin{bmatrix}
300\*15\\
500\*100\\
1800\*12\\
1280\*30\\
3000\*40\\
18\*14
\end{bmatrix}
\end{align}\]</span></p>
<p>Let's draw the picture of these data: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">X=[5000;5000;2800;350;8851;117];</span><br><span class="line">y=[300\*15;500\*100; 1800\*12;1280\*30;3000\*40; 18\*14];</span><br><span class="line">xlabel(&quot;x (km)&quot;);</span><br><span class="line">hold on;</span><br><span class="line">ylabel(&quot;y (k people year)&quot;);</span><br><span class="line">plot(X,y,&quot;ro&quot;, &quot;MarkerFaceColor&quot;, &quot;b&quot;);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-01-113516.jpg" /> We
hope find a mapping <span class="math inline">\(x\rightarrow
f(x)\)</span> (<span class="math inline">\(x\)</span> denotes the length
of walls, <span class="math inline">\(y\)</span> denotes the cost of
resources)drawing a line as the figure above which meets all data best.
When we encounter new data(e.g. new length of wall), we hope <span
class="math inline">\(f(x)\)</span> will help us find how many resources
will we cost. This is the goal of linear regression.</p>
<h3 id="definition">Definition</h3>
<p>First of all, we assume that the line have the form as followed:
<span class="math display">\[h\_{\theta}(x)=\theta_0+\theta_1
x_1+\theta_2 x_2 + \cdots + \theta_nx_n\]</span> Specifically, under our
circumstance, there is only one <span
class="math inline">\(x\)</span>(the length of walls), then we will have
the form as below: <span
class="math display">\[h\_{\theta}(x)=\theta_0+\theta_1x\]</span> In
fact, there are many factor infulence the outcome of the cost of wall
besides people and time. Tools we use and the economy as well as terrain
where building the wall. if we add these factors, maybe the assumption
looks like: <span
class="math display">\[h\_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_2\]</span>
<span class="math inline">\(x_1\)</span>denotes the length of wall,
while <span class="math inline">\(x_2\)</span> and <span
class="math inline">\(x_3\)</span> denote economy and terrian condition
respectively. For simplicity, we consider only the length of wall.</p>
<p>In general, we add a <span class="math inline">\(x_0=1\)</span> to
the equation, the we have: <span
class="math display">\[h\_{\theta}(x)=\theta_0x_0+\theta_1 x_1 + \cdots
+ \theta_nx_n
=\sum\_{i=1}^{n}\theta_ix_i\]</span> If we use matrices represent the
equation, then <span class="math display">\[\begin{align}
\Theta=\begin{bmatrix}
\theta_0\\
\theta_1\\
\cdots\\
\theta_n
\end{bmatrix}
\end{align}\]</span> thus, <span class="math display">\[\begin{align}
h\_{\theta}(x)=\sum\_{i=1}^{n}\theta_ix_i=
\begin{bmatrix}
\theta_0\theta_1\cdots\theta_n
\end{bmatrix}
\begin{bmatrix}
x_0\\
x_1\\
\cdots\\
x_n
\end{bmatrix}
=\Theta^T\overrightarrow{X}
\end{align}\]</span> Notice that <span
class="math inline">\(\Theta^T\)</span> is a <span
class="math inline">\(1\times n\)</span> matrices while <span
class="math inline">\(\overrightarrow{X}\)</span> is a <span
class="math inline">\(n\times1\)</span> matrices. We get a real number
after multiply two matrices.</p>
<h3 id="cost-function">Cost Function</h3>
<p>We hope find a way to find the best function <span
class="math inline">\(h\_\theta(x)\)</span> fit these data. Notice that
if <span class="math inline">\(|(h\_\theta(x_i)-y_i)|=0\)</span> for
each <span class="math inline">\(x\)</span>, then the function fit data
absolutely. In practice, if we minimize <span
class="math inline">\(|(h\_\theta(x_i)-y_i)|\)</span>, we can find the
best fit to original data. An optional cost function is square cost
function: <span
class="math display">\[J(\theta)=\frac{1}{2m}\sum\_{i=1}^{m}(h\_\theta(x_i)-y_i)^2\]</span>
As long as we can minimize <span
class="math inline">\(J(\theta)\)</span> with respect to <span
class="math inline">\(\theta\)</span>, can we find the best <span
class="math inline">\(\theta\)</span> fit original data. The cost
function looks like as followed: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">m = size(X,1)</span><br><span class="line">XX = [ones(m,1),X];</span><br><span class="line">theta0 = linspace(-100,6000, 100);    </span><br><span class="line">theta1 = linspace(-20,36, 100);</span><br><span class="line">l0=length(theta0);                               </span><br><span class="line">l1=length(theta1);                               </span><br><span class="line">J_vs = zeros(l0,l1);                             </span><br><span class="line">for i=1:l0                                             </span><br><span class="line">    for j=1:l1                                         </span><br><span class="line">        t = [theta0(i); theta1(j)];               </span><br><span class="line">        J_vs(i,j) = cost(XX, y, t);             </span><br><span class="line">    end                                                 </span><br><span class="line">end                                                     </span><br><span class="line">figure;                                                 </span><br><span class="line">J_vs = J_vs&#x27;;                                       </span><br><span class="line">surfc(theta0, theta1, J_vs);                 </span><br><span class="line">colorbar;                                             </span><br><span class="line">xlabel(&#x27;\theta_0&#x27;);                                </span><br><span class="line">ylabel(&#x27;\theta_1&#x27;);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-02-28-115912.jpg" /></p>
<p>The figure has a bowl shape, and we can find the minimum of it both
using matrices approach and gradient descent.</p>
<h3 id="matrics-solution">Matrics Solution</h3>
<p>Remember that the cost function <span
class="math inline">\(J(\theta)=\frac{1}{2m}\sum\_{i=1}^{m}(h\_\theta(x_i)-y_i)^2\)</span>,
It is more convinient if we apply <span
class="math inline">\(J(\theta)\)</span> matrics form, then: <span
class="math display">\[\begin{align}
J(\theta)=\frac{1}{2m}(h\_\theta(x)-y)^T\cdot(h\_\theta(x)-y)\\   
=\frac{1}{2m}(X\Theta-y)^T\cdot(X\Theta-y)
\end{align}\]</span> We can take the derivative of <span
class="math inline">\(J(\theta)\)</span> and let it be <span
class="math inline">\(\overrightarrow{0}\)</span>. It is a little
complex if we take the derivative of the last equation about <span
class="math inline">\(J(\theta)\)</span>. Let's do the derivation a easy
way, but not a total Matric solution. If we do the derivation on the
sum, we can get: <span
class="math display">\[\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum\_{i=1}^m(h\_\theta(x_i)-y_i)x_j\]</span>
Then, <span class="math display">\[\begin{align}
\frac{\partial}{\partial\Theta}J(\theta)=
\begin{bmatrix}
\frac{\partial}{\partial\theta_1}J(\theta)\\
\frac{\partial}{\partial\theta_2}J(\theta)\\
\frac{\partial}{\partial\theta_3}J(\theta)\\
\cdots\\
\frac{\partial}{\partial\theta_n}J(\theta)
\end{bmatrix}=\overrightarrow{0}
\end{align}\]</span> Now, we transfer the sum into matrices form: <span
class="math display">\[\frac{\partial}{\partial\Theta}J(\theta)=X^T(X\Theta-y)=X^TX\Theta-X^Ty=\overrightarrow{0}\]</span>
thus, we can get the solution of <span
class="math inline">\(\Theta\)</span>: <span
class="math display">\[\Theta=(X^TX)^{(-1)}X^Ty\]</span> Let's look at
the solution of Matrices: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">xlabel(&quot;x (km)&quot;);</span><br><span class="line">hold on;</span><br><span class="line">ylabel(&quot;y (k people year)&quot;);</span><br><span class="line">plot(X,y,&quot;ro&quot;, &quot;MarkerFaceColor&quot;, &quot;b&quot;);</span><br><span class="line">a=linspace(0,10000,100);</span><br><span class="line">t = pinv(XX&#x27;*XX)*XX&#x27;*y;</span><br><span class="line">b=t&#x27;*[ones(1, length(a)); a];</span><br><span class="line">plot(a,b);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-01-114724.jpg" /></p>
<p>And <span class="math inline">\(\theta_0\)</span> and <span
class="math inline">\(\theta_1\)</span> is as followed: <span
class="math display">\[\begin{align}
\Theta=\begin{bmatrix}
2573\\
9.9\\
\end{bmatrix}
\end{align}\]</span> That means <span
class="math inline">\(h\_\theta(x)=2573+9.9x\)</span>, Right Now we
subsitute 3169(km) for <span class="math inline">\(x\)</span>: <span
class="math display">\[h\_\theta(3169)=2573+9.9\*3169=33946(k\  people\
year)\]</span> In other word, Donald Trump need 3,394,600 people
continously build 10 years in order to finish the GreatWall between US
and Mexico. Surely he can stimulate 33,946,000 people, it only take him
one year to finished the wall. ### Gradient Descent Solution Let's look
at the cost function one more time. If we first choose a random <span
class="math inline">\(\Theta\)</span>, say we have <span
class="math inline">\(\theta_0\&amp;\theta_1\)</span> located in point
1. Assume you stand at ponit 1, you want to find a way to go down the
vally. Surely you can not see the land-form completely. But you can look
around at point 1 and find a steepest direction, then you go that way a
baby step. Right now, you are at point 2 after the first step. You do
the same find a steepest direction and make another baby step. After
several steps, you are now standing at point 5. When you are looking
around you find that where you are standing is almost flat. Now we can
stop the iteration, and we have found a reasonable <span
class="math inline">\(\Theta\)</span> which makes the <span
class="math inline">\(J(\theta)\)</span> has a minmum value. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-02-134554.jpg" />
What I described is a method named Gradient Descent which is very easy
way to find minima of <span class="math inline">\(J(\theta)\)</span>.
The step is as followed:</p>
<ol type="1">
<li>Find a reasonable cost function J().</li>
<li>Take the partial derivative of <span
class="math inline">\(J(\theta)\)</span> for each <span
class="math inline">\(\theta_j\)</span></li>
<li>Choose a moderate <span class="math inline">\(\alpha\)</span> to
constrain the step size.</li>
<li>Take a baby step, the direction is from the derivative and the size
is determined by derivative and parameter <span
class="math inline">\(alpha\)</span></li>
<li>update the value of <span class="math inline">\(\Theta\)</span>,
check if we find the minimum. If not, return to the step 4, otherwise,
stop the algo</li>
</ol>
<p>Concretely, the algorithm using octave is listed below: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% octave: The derivative function</span><br><span class="line">function g = gradient(X, y, theta)</span><br><span class="line">    m = size(X,1);</span><br><span class="line">    hx = X*theta;</span><br><span class="line">    g = (1/m)*X&#x27;*(hx-y);</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% octave: I just take 100 steps without checking when to stop.</span><br><span class="line">theta=[0;0];</span><br><span class="line">alpha=0.00000003</span><br><span class="line">for i=1:100</span><br><span class="line">    g = gradient(XX,y,theta);</span><br><span class="line">    theta = theta - alpha*g</span><br><span class="line">end</span><br></pre></td></tr></table></figure> Under the circumstance above, <span
class="math inline">\(\theta_0=0.0048\)</span> and <span
class="math inline">\(\theta_1=10.33\)</span>. If we substitute the
parameters for <span class="math inline">\(h\_\theta(x)\)</span>, the
answer that Trump would take to build the wall is 32745.56$$1000 people
year. ### Summarize Today we have talk about how to use linear
regression to model a problem of building Great Wall. We have also talk
about how to resovle the problem through minimize the cost function both
using Matrices solution and Gradient Descent.</p>
<h3 id="appendix">Appendix</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% octave --cost function:cost.m</span><br><span class="line">function J=cost(X, y, theta)</span><br><span class="line">m = size(X,1);</span><br><span class="line">J=0;</span><br><span class="line">hx=X*theta;</span><br><span class="line">J = 1/(2*m)*(hx-y)&#x27;*(hx-y);</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/02/25/Linear-Algebra-I/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/02/25/Linear-Algebra-I/" class="post-title-link" itemprop="url">Linear Algebra (I)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-02-25 20:11:21" itemprop="dateCreated datePublished" datetime="2017-02-25T20:11:21+01:00">2017-02-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="motivation">Motivation</h3>
<p>When I was learning Linear Algebra in University, I found it was hard
to understand what would applications of this course. I remember that we
learned determinant on Chapter 1, then we learned a lot of other concept
such as Rank and EigenValue. I could only memorize one after another
theorems without knowing why these happened to appear this way. However,
when I was learning matrices from lectures taught by <strong>Gilbert
Strang</strong>, I found every conclusion is so nature that I could
easily understand and associate a bunch of concepts with each other. I'd
like to conclude what I had learned from him as well as hoping that
someone can learn from that.</p>
<h3 id="why-this-subject-exists">Why this subject exists?</h3>
First of all, what is the origin of linear algebra? Why we make the
world increasingly complicated? The truth is that the purpose of
mathmatics is to make the world simpler, not the opposite, Linear
Algebra as well. For example, we want to solve equations with a lot of
variables, the original way is to write all the parameter out：
<span class="math display">\[\begin{cases}
3x+4y+6z = 0\\\\
2x+3y+4z = 24\\\\
x+y+z = 15
\end{cases}\]</span>
<p>If there are only 3 parameters, the problems is not so hard for us to
handle. But what if there are 100 parameters? When we transfer the
equation to matrices, the outcome becomes so grateful: <span
class="math display">\[\begin{align}
\begin{bmatrix}
3&amp;4&amp;6\\\\
2&amp;3&amp;4\\\\
1&amp;1&amp;1
\end{bmatrix}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=\begin{bmatrix}0\\\\24\\\\15\end{bmatrix}
\end{align}\]</span> ### Solve the equation group Now, we try to solve
the problems, our instinction is to draw the three planes, under most
circumstance, there will be a cross point, octave code and the plot is
as followed: <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">octave:1&gt; x=linspace(-20, 20, 100);</span><br><span class="line">octave:2&gt; [xx, yy] = meshgrid(x,y);</span><br><span class="line">octave:3&gt; z = 15-xx-yy;</span><br><span class="line">octave:4&gt; mesh(xx, yy, z);</span><br><span class="line">octave:5&gt; l = (-3*xx-4*yy)./6;</span><br><span class="line">octave:6&gt; mesh(xx,yy, l);</span><br><span class="line">octave:7&gt; t = (24-(2*xx+3*yy))./4;</span><br><span class="line">octave:8&gt; mesh(xx,yy,z);</span><br><span class="line">octave:9&gt; hold on;</span><br><span class="line">octave:11&gt; mesh(xx,yy,t);</span><br><span class="line">octave:12&gt; mesh(xx,yy,l);</span><br><span class="line">octave:13&gt; xlabel(&quot;x&quot;)</span><br><span class="line">octave:14&gt; ylabel(&quot;y&quot;)</span><br><span class="line">octave:15&gt; zlabel(&quot;z&quot;)</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-02-24-065543.jpg" /></p>
<p>The answer of this equation group is that <span
class="math display">\[\begin{align}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=\begin{bmatrix}
-18\\\\
72\\\
-39
\end{bmatrix}
\end{align}\]</span>x} \end{align}</p>
<h3 id="another-perspective">Another perspective</h3>
<p>We can find another perspective to solve this equation: see the
equation as a combination of columns, that means: <span
class="math display">\[\begin{align}
x\*\begin{bmatrix}
3\\\\
2\\\\
1
\end{bmatrix}+y\*
\begin{bmatrix}
4\\\\
3\\\\
1
\end{bmatrix}+z\*
\begin{bmatrix}
6\\\\
4\\\\
1
\end{bmatrix}=
\begin{bmatrix}
0\\\\
24\\\\
15
\end{bmatrix}
\end{align}\]</span> Now, we can acquire the new vector through three
vectors' combination, from the plot blow demonstrates that we need to
find a combination of red&amp;blue&amp;black vectors to generate another
vector(the green one). Obviously, <span
class="math inline">\(x=-18,y=72,z=-39\)</span> is the only solution of
the combination problem. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">octave:1&gt; a = quiver3(0, 0, 0, 3,2,1);</span><br><span class="line">octave:2&gt; hold on;</span><br><span class="line">octave:3&gt; b = quiver3(0, 0, 0, 4,3,1);</span><br><span class="line">octave:4&gt; c = quiver3(0, 0, 0, 6,4,1);</span><br><span class="line">octave:5&gt; set(b, &quot;Color&quot;, &quot;r&quot;);</span><br><span class="line">octave:6&gt; set(c, &quot;Color&quot;, &quot;k&quot;);</span><br><span class="line">octave:7&gt; set(a, &quot;maxheadsize&quot;, 0.02);</span><br><span class="line">octave:8&gt; set(b, &quot;maxheadsize&quot;, 0.02);</span><br><span class="line">octave:9&gt; set(c, &quot;maxheadsize&quot;, 0.02);</span><br><span class="line">octave:10&gt; d = quiver3(0, 0, 0, 0,24,15);</span><br><span class="line">octave:11&gt; set(d, &quot;Color&quot;, &quot;g&quot;);</span><br><span class="line">octave:12&gt; set(d, &quot;maxheadsize&quot;, 0.02);</span><br></pre></td></tr></table></figure> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-02-24-103145.jpg" /></p>
How to solve equations using program, and is there always an answer of
equations? An easy way is to do elimination of these there equations.
What now I want to do right now is to change these equation as follows:
<span class="math display">\[\begin{cases}
3x+4y+6z = 0\\\\
(1/3)y+0z = 24\\\\
-z = 39
\end{cases}\]</span>
<p>Step1, calclate z, step2, calclate y, step3, calclate x. We first
leave a variables z alone with an answer in the third equation. Then, we
substitute z to the second equation, and get y. At last we substitute x
and y to the first equation and get x. if we represent above through
matrices language, it is like this: <span
class="math display">\[\begin{align}
\begin{bmatrix}
3&amp;4&amp;6\\\\
0&amp;1/3&amp;0\\\\
0&amp;0&amp;-1
\end{bmatrix}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=
\begin{bmatrix}
0\\\\
24\\\\
39
\end{bmatrix}
\end{align}\]</span> The triangle at bottom-left side is all zeros which
ensure we can solve equations one by one. The process is named
back-substitution. ### Additional thinkings But what if the equation
group looks like: <span class="math display">\[\begin{align}
\begin{bmatrix}
1&amp;2&amp;3\\\\
1&amp;2&amp;3\\\\
1&amp;2&amp;3
\end{bmatrix}
\begin{bmatrix}
x\\\\
y\\\\
z
\end{bmatrix}=
\begin{bmatrix}
5\\\\
6\\\\
7
\end{bmatrix}
or
\begin{bmatrix}
3&amp;4\\\\
4&amp;5\\\\
5&amp;6\\\\
6&amp;7\\\\
7&amp;9
\end{bmatrix}
\begin{bmatrix}
x\\\\
y
\end{bmatrix}=
\begin{bmatrix}
5\\\\
5\\\\
5\\\\
5\\\\
5
\end{bmatrix}
\end{align}\]</span><br />
Next time we will talk about LU decomposition and Rank of a matrix.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/02/18/Gredient-Descent-in-Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/02/18/Gredient-Descent-in-Logistic-Regression/" class="post-title-link" itemprop="url">Gradient Descent in Logistic Regression</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-02-18 14:33:10" itemprop="dateCreated datePublished" datetime="2017-02-18T14:33:10+01:00">2017-02-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>As a simple model, Logistic regression is very popular in Machine
Learning, especially in computer industry while gradient descent is more
of popularity as well among dozens of optimization methods. The aim of
this article is to demonstrate how to reach these formulas
conclusion.</p>
<h3 id="model-a-problem">1. Model a problem</h3>
<p>First of all, we know that Logistic regression is a statistical
model. Suppose we have a coin, we'd like to know the probability of the
head and tail. Bernoulli distribution is a good distribution to model
the coin. The hyphothsis is as followed: <span
class="math display">\[P(y=1|\mu)=\mu  \qquad(1)\]</span>.</p>
<p><span class="math inline">\(y=1\)</span> means the probability of the
head, then <span class="math inline">\(y=0\)</span> is that of the tail.
<span class="math inline">\(\mu\)</span> here is the parameter, if <span
class="math inline">\(\mu=0.5\)</span>, then the probability of head
equals to 0.5, which means we have half the probabilty reach the head as
well as the tail. Now we have: <span
class="math display">\[P(y=0|\mu)=1-\mu\qquad(2)\]</span></p>
<p>If we take a look at these two formula, we can conclude a more
general formula: <span
class="math display">\[P(y|\mu)={\mu}^y\cdot{(1-\mu)}^{(1-y)}
\qquad(3)\]</span></p>
<p>Let's test it, <span class="math inline">\(y\)</span> only have two
value <span class="math inline">\(0\)</span> and <span
class="math inline">\(1\)</span>, if <span
class="math inline">\(y=1\)</span>, <span
class="math inline">\(P(y|\mu)=\mu\)</span>, othewise, <span
class="math inline">\(P(y|\mu)=1-\mu\)</span>. Suppose we have a dataset
<span class="math inline">\(D=\{y\_1,y\_2,y\_3...y\_m\}\)</span>
observed values, we want to estimate the parameter <span
class="math inline">\(\mu\)</span>, the problems become following
question: <span class="math display">\[P(\mu|D)=?\qquad (4)\]</span></p>
<h3 id="estimate-parameter">2. Estimate Parameter</h3>
<p>We use Bayes formula to transfer the problem to another: <span
class="math display">\[P(\mu|D)=\frac{P(D|\mu)\cdot
P(\mu)}{P(D)}=\frac{P(y\_1,y\_2,y\_3...,y\_m|\mu)\cdot
P(\mu)}{P(D)}\qquad (5)\]</span></p>
<p>Here denominator <span class="math inline">\(P(D)\)</span> is a
constant as well as <span class="math inline">\(P(\mu)\)</span> if we
see <span class="math inline">\(\mu\)</span> as variable rather than a
distribution. Then we have: <span
class="math display">\[P(\mu|D)\triangleq P(y\_1,y\_2,y\_3...,y\_m|\mu)
\qquad(6)\]</span></p>
<p>We can find a series of <span class="math inline">\(\mu\)</span>(e.g,
<span class="math inline">\(\mu=0.1, \mu=0.72\)</span>), but we should
find the maximum <span class="math inline">\(P(\mu|D)\)</span>, because
when <span class="math inline">\(P(\mu|D)\)</span> reaches its max means
<span class="math inline">\(\mu\)</span> most likely is the right
parameter. We assume that each <span class="math inline">\(y\)</span> is
independent from the others given the parameter <span
class="math inline">\(\mu\)</span>. then we have: <span
class="math display">\[P(\mu|D)\triangleq
P(y\_1|\mu)P(y\_2|\mu)P(y\_3|\mu)...,P(y\_m|\mu)={\prod}\_{i=1}^{m}P(y\_i|\mu)\qquad(7)\]</span></p>
<p>Now, the problem is to maximize the <span
class="math inline">\({\prod}\_{i=1}^{m}P(y\_i|\mu)\)</span>, that is:
<span class="math display">\[L =
\underset{\mu}{argmax}{\prod}\_{i=1}^{m}P(y\_i|\mu)=\underset{\mu}{argmax}{\prod}\_{i=1}^{m}[{\mu}^y\cdot{(1-\mu)}^{(1-y)}]
\qquad(8)\]</span></p>
<p>It is a little hard to find the maximum, we change the problem to
another way: <span class="math display">\[L
=\underset{\mu}{argmax}ln({\prod}\_{i=1}^{m}[{\mu}^y\cdot{(1-\mu)}^{(1-y)}])\qquad(9)\]</span></p>
<p>Then we have: <span class="math display">\[L =
\underset{\mu}{argmax}{\sum}\_{i=1}^{m}[{y ln(\mu}) + (1-y)
ln({1-\mu)}]\qquad(10)\]</span></p>
<h3 id="logistic-regression">3. Logistic Regression</h3>
<p>If we let <span class="math inline">\(\mu=h\_{\theta}(x)\)</span>,
then we have:</p>
<p><span class="math display">\[L =
\underset{\theta}{argmax}{\sum}\_{i=1}^{m}[{y ln(h\_{\theta}(x)}) +
(1-y) ln({1-h\_{\theta}(x))}]\qquad(11)\]</span></p>
<p>Here, <span
class="math display">\[h\_{\theta}(x)=\frac{1}{1+e^{-(\theta\_0
x\_0+\theta\_1 x\_1+\theta\_2 x\_2+\theta\_m
x\_m)}}=\frac{1}{1+e^{-{\Theta}X}}\qquad (12)\]</span></p>
<p><span class="math inline">\(h\_{\theta}(x)\)</span> is named sigmoid
function and now we use parameter <span
class="math inline">\(\theta\)</span> to estimate <span
class="math inline">\(\mu\)</span>. Simgoid function is a pretty good
function, derivative of which is elegant. we let <span
class="math inline">\(\sigma=\frac{1}{1+e^{-x}}\)</span>, then:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial{\sigma}}{\partial x}
&amp;= \frac{-1}{(1+e^{-x})^2}\cdot e^{-x}\cdot(-1)\\\\
&amp;=\frac{e^{-x}}{(1+e^{-x})^2}=\frac{1+e^{-x}-1}{(1+e^{-x})^2}\\\\
&amp;=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^2}\\\\
&amp;=\sigma(1-\sigma)\qquad \qquad   (13)
\end{align}\]</span></p>
<h3 id="gradient-descent">4. Gradient Descent</h3>
<p>We transfer the maximizing to a minimizing problem, we define the
cost function:</p>
<p><span class="math display">\[J(\theta) =
-\frac{1}{m}{\sum}\_{i=1}^{m}[{yln(h\_{\theta}(x)}) +
(1-y)ln({1-h\_{\theta}(x))}]\qquad (14) \]</span></p>
<p>if we want to minimize <span
class="math inline">\(J(\theta)\)</span>, we need know the gradient of
<span class="math inline">\(\frac{\partial
J(\theta)}{\partial\theta\_j}\)</span>, that is: <span
class="math display">\[\begin{align}
\frac{\partial J(\theta)}{\partial\theta\_j}
&amp;=\frac{\partial}{\partial\theta\_j}(-\frac{1}{m}{\sum}\_{i=1}^{m}[yln(h\_\theta(x))
+ (1-y)ln(1-h\_\theta(x))])\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y}{h\_\theta(x)} +
\frac{y-1}{1-{h\_\theta(x)}} ]\frac{\partial h\_\theta
(x)}{\partial\theta\_j}\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[{\frac{y}{\sigma}} +
\frac{y-1}{1-\sigma} ]\frac{\partial\sigma}{\partial\theta\_j}\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-y\sigma}{\sigma(1-\sigma)}
+ \frac{\sigma
y-\sigma}{\sigma(1-\sigma)}]\frac{\partial\sigma}{\partial\theta\_j}\\\\
&amp;=
-\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-\sigma}{\sigma\cdot(1-\sigma)}]\frac{\partial\sigma}{\partial\theta\_j}\\\\
&amp;=
-\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-\sigma}{\sigma(1-\sigma)}]\cdot\sigma(1-\sigma)\frac{\partial\Theta
X}{\partial\theta\_j}\\\\
&amp;=
-\frac{1}{m}{\sum}\_{i=1}^{m}[\frac{y-\sigma}{1}]\frac{\partial\Theta
X}{\partial\theta\_j}\\\\
&amp;= -\frac{1}{m}{\sum}\_{i=1}^{m}[y-\sigma]{X\_j}\\\\
&amp;= \frac{1}{m}{\sum}\_{i=1}^{m}[h\_{\theta}(x)-y]{X\_j} \qquad(15)
\end{align}\]</span></p>
<p>Notice that the gradient descent is same as that of square cost
function in Linear Regression, which is an extremely grace equation. ###
Summarize Today We have talked about how to derive cost function of
Logistic Regression, then we get the gradient descent equation of it.
Next step is to use gradient descent to iterate the minimum of the cost
function.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>https://www.coursera.org/learn/machine-learning/home/welcome</li>
<li>Parameter estimation for text analysis, Gregor Heinrich</li>
<li>Pattern Recognition and Machine Learning, Christopher M. Bishop</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2016/12/15/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2016/12/15/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2016-12-15 15:10:50" itemprop="dateCreated datePublished" datetime="2016-12-15T15:10:50+01:00">2016-12-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class=""></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Mingbo Cheng</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"chengmingbo/gitment-comments","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
