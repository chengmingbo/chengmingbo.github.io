<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chengmingbo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Fundamentals Torch.tensor 1234t &#x3D; tensor([[1,2,4],[4,5,6]])t.shape: (2,3)t.ndim: 2type: scalar, vector, matrix, tensor Tensor data produce 12345random_tensor &#x3D; torch.rand(size&#x3D;(3, 4, 5))zeros &#x3D; torch.">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch ABC">
<meta property="og:url" content="http://chengmingbo.github.io/2020/03/07/pytorch_abc/index.html">
<meta property="og:site_name" content="Mingbo">
<meta property="og:description" content="Fundamentals Torch.tensor 1234t &#x3D; tensor([[1,2,4],[4,5,6]])t.shape: (2,3)t.ndim: 2type: scalar, vector, matrix, tensor Tensor data produce 12345random_tensor &#x3D; torch.rand(size&#x3D;(3, 4, 5))zeros &#x3D; torch.">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-03-07T21:50:11.000Z">
<meta property="article:modified_time" content="2024-11-08T07:45:30.831Z">
<meta property="article:author" content="Mingbo Cheng">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://chengmingbo.github.io/2020/03/07/pytorch_abc/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://chengmingbo.github.io/2020/03/07/pytorch_abc/","path":"2020/03/07/pytorch_abc/","title":"Pytorch ABC"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Pytorch ABC | Mingbo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Mingbo</p>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="/%5Bobject%20Object%5D" alt="Mingbo">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-slides"><a href="/slides/" rel="section"><i class="area-chart fa-fw"></i>slides</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#fundamentals"><span class="nav-number">1.</span> <span class="nav-text">Fundamentals</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#torch.tensor"><span class="nav-number">1.0.1.</span> <span class="nav-text">Torch.tensor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-data-produce"><span class="nav-number">1.0.2.</span> <span class="nav-text">Tensor data produce</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#float"><span class="nav-number">1.0.3.</span> <span class="nav-text">Float</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#types-specific-for-gpu-or-cpu"><span class="nav-number">1.0.4.</span> <span class="nav-text">types specific for GPU or
CPU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor-operations"><span class="nav-number">1.0.5.</span> <span class="nav-text">tensor operations</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#multiply"><span class="nav-number">1.0.5.1.</span> <span class="nav-text">multiply</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#layer"><span class="nav-number">1.0.5.2.</span> <span class="nav-text">layer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#other-operations"><span class="nav-number">1.0.5.3.</span> <span class="nav-text">other operations</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#slice"><span class="nav-number">1.0.5.4.</span> <span class="nav-text">slice</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#numpy"><span class="nav-number">1.0.5.5.</span> <span class="nav-text">numpy</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#random-seed"><span class="nav-number">1.0.5.6.</span> <span class="nav-text">random seed</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#variable"><span class="nav-number">1.0.5.7.</span> <span class="nav-text">Variable</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#gpu"><span class="nav-number">1.0.5.8.</span> <span class="nav-text">GPU</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-network"><span class="nav-number">2.</span> <span class="nav-text">Neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#define-a-net"><span class="nav-number">2.0.1.</span> <span class="nav-text">Define a net</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#check-module"><span class="nav-number">2.0.1.1.</span> <span class="nav-text">Check module</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#training"><span class="nav-number">2.0.2.</span> <span class="nav-text">Training</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#training-example"><span class="nav-number">2.0.2.1.</span> <span class="nav-text">Training example</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#test"><span class="nav-number">2.0.3.</span> <span class="nav-text">test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#inference-and-save-model"><span class="nav-number">2.0.4.</span> <span class="nav-text">Inference and save model</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#examples"><span class="nav-number">3.</span> <span class="nav-text">Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#example-1"><span class="nav-number">3.0.1.</span> <span class="nav-text">Example 1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#example-2"><span class="nav-number">3.0.2.</span> <span class="nav-text">Example 2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#vae"><span class="nav-number">3.0.3.</span> <span class="nav-text">VAE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cnn"><span class="nav-number">3.0.4.</span> <span class="nav-text">CNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lstm"><span class="nav-number">3.0.5.</span> <span class="nav-text">LSTM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#references"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mingbo Cheng</p>
  <div class="site-description" itemprop="description">Mingbo</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chengmingbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chengmingbo" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://deeplearningmath.org/supervised-machine-learning" title="https:&#x2F;&#x2F;deeplearningmath.org&#x2F;supervised-machine-learning" rel="noopener" target="_blank">deeplearningmath</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://lilianweng.github.io/archives/" title="https:&#x2F;&#x2F;lilianweng.github.io&#x2F;archives&#x2F;" rel="noopener" target="_blank">Lil'Log</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://arjun-sarkar786.medium.com/" title="https:&#x2F;&#x2F;arjun-sarkar786.medium.com&#x2F;" rel="noopener" target="_blank">Arjun Sarkar</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.zybuluo.com/codeep/note/163962" title="https:&#x2F;&#x2F;www.zybuluo.com&#x2F;codeep&#x2F;note&#x2F;163962" rel="noopener" target="_blank">mathjax grammar</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vividfree.github.io/" title="http:&#x2F;&#x2F;vividfree.github.io&#x2F;" rel="noopener" target="_blank">vividfree</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://colah.github.io/" title="http:&#x2F;&#x2F;colah.github.io&#x2F;" rel="noopener" target="_blank">colah</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.autonlab.org/tutorials" title="https:&#x2F;&#x2F;www.autonlab.org&#x2F;tutorials" rel="noopener" target="_blank">Andrew Moore</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://plot.ly/matlab/plot/" title="https:&#x2F;&#x2F;plot.ly&#x2F;matlab&#x2F;plot&#x2F;" rel="noopener" target="_blank">matlabplot</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.ryanzhang.info/blog/" title="http:&#x2F;&#x2F;www.ryanzhang.info&#x2F;blog&#x2F;" rel="noopener" target="_blank">Ryan’s Cabinet</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.cnblogs.com/jerrylead/tag/Machine%20Learning/" title="http:&#x2F;&#x2F;www.cnblogs.com&#x2F;jerrylead&#x2F;tag&#x2F;Machine%20Learning&#x2F;" rel="noopener" target="_blank">JerryLead</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://yxzf.github.io/" title="https:&#x2F;&#x2F;yxzf.github.io&#x2F;" rel="noopener" target="_blank">YXZF'S BLOG</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vonng.com/" title="http:&#x2F;&#x2F;vonng.com" rel="noopener" target="_blank">VONNG</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2020/03/07/pytorch_abc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Pytorch ABC | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Pytorch ABC
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-03-07 22:50:11" itemprop="dateCreated datePublished" datetime="2020-03-07T22:50:11+01:00">2020-03-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="fundamentals">Fundamentals</h2>
<h4 id="torch.tensor">Torch.tensor</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">t.shape: (<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t.ndim: <span class="number">2</span></span><br><span class="line"><span class="built_in">type</span>: scalar, vector, matrix, tensor</span><br></pre></td></tr></table></figure></code></pre>
<h4 id="tensor-data-produce">Tensor data produce</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">random_tensor = torch.rand(size=(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">zeros = torch.zeros(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">ones = torch.ones(size=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">zero_to_ten = torch.arange(start=<span class="number">0</span>, end=<span class="number">10</span>, step=<span class="number">1</span>)</span><br><span class="line">ten_zeros = torch.zeros_like(<span class="built_in">input</span>=zero_to_ten) <span class="comment"># same shape but all zeros</span></span><br></pre></td></tr></table></figure></code></pre>
<h4 id="float">Float</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.float32/torch.<span class="built_in">float</span></span><br><span class="line">torch.float16</span><br><span class="line">torch.half</span><br><span class="line">torch.float64/torch.double</span><br></pre></td></tr></table></figure></code></pre>
<h4 id="types-specific-for-gpu-or-cpu">types specific for GPU or
CPU</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device=’cuda’ <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> ‘cpu’</span><br><span class="line">t = tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], device=device)</span><br></pre></td></tr></table></figure></code></pre>
<h4 id="tensor-operations">tensor operations</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor_A = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line">                        dtype = torch.float32)</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="multiply">multiply</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor + <span class="number">10</span></span><br><span class="line">torch.multiply(tensor, <span class="number">10</span>)</span><br><span class="line">tensor * tensor <span class="comment"># tensor([1,4,9])</span></span><br><span class="line">tenorA @ tensorB <span class="comment"># matrix multiplication -&gt; tensor(14)</span></span><br><span class="line">torch.matmul(tensor, tensor)/torch.mm <span class="comment"># 1*1 + 2*2 + 3*3 = tensor(14)</span></span><br><span class="line">tensor.T</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="layer">layer</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Torch.nn.Linear</span></span><br><span class="line">y = x A^T + b</span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">linear = torch.nn.Linear(in_features=<span class="number">2</span>, <span class="comment"># in_features = matches inner dimension of input</span></span><br><span class="line">             out_features=<span class="number">6</span>) <span class="comment"># out_features = describes outer value</span></span><br><span class="line">x = tensor_A</span><br><span class="line">output = linear(x)</span><br><span class="line">x.shape, output, output.shape</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="other-operations">other operations</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.arange(<span class="number">10</span>, <span class="number">100</span>, <span class="number">10</span>) <span class="comment"># tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])</span></span><br><span class="line">tensor.argmax() <span class="comment"># 8</span></span><br><span class="line">tensor.argmin() <span class="comment"># 0</span></span><br><span class="line">tensor.<span class="built_in">type</span>(torch.float16) <span class="comment"># tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.]</span></span><br><span class="line">torch.reshape(new_shape) <span class="comment"># -1 is to ask calculating automatically</span></span><br><span class="line">tensor.view(new_shape) <span class="comment"># return a new shape view</span></span><br><span class="line">torch.stack(t, dim=<span class="number">0</span>) <span class="comment"># concate a sequence of tensors along a new dimension(dim)</span></span><br><span class="line">torch.squeeze() <span class="comment"># all into the first dimensions</span></span><br><span class="line">torch.clamp() <span class="comment"># min=min, max=max, limit the range</span></span><br><span class="line">torch.unsqueeze()</span><br><span class="line">torch.permute() <span class="comment"># torch.Size([224, 224, 3]) -&gt; torch.Size([3, 224, 224])</span></span><br><span class="line">torch.permute_(), x.unsqueeze_() -&gt; inplace operation</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="slice">slice</h5>
<pre><code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x[:, <span class="number">0</span>] <span class="comment">#  tensor([[1, 2, 3]])</span></span><br><span class="line">x[:, :, <span class="number">1</span>] <span class="comment"># tensor([[2,5,8]])</span></span><br><span class="line">x[:, <span class="number">1</span>, <span class="number">1</span>] <span class="comment"># tensor([5])</span></span><br><span class="line">x[<span class="number">0</span>, <span class="number">0</span>, :]/x[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># tensor([1,2,3])</span></span><br></pre></td></tr></table></figure></code></pre>
<h5 id="numpy">numpy</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.from_numpy(array)</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="random-seed">random seed</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">random seed</span><br><span class="line">torch.manual_seed(seed=RANDOM_SEED)</span><br><span class="line">torch.random.manual_seed(seed=RANDOM_SEED)</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="variable">Variable</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">.data, .grad, .grad_fn</span><br><span class="line">x_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">y_tensor = torch.randn(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">x = Variable(x_tensor, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(y_tensor, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.<span class="built_in">sum</span>(x + y)</span><br><span class="line"><span class="built_in">print</span>(z.data) <span class="comment">#-2.1379</span></span><br><span class="line"><span class="built_in">print</span>(z.grad_fn) <span class="comment">#&lt;SumBackward0 object at 0x10da636a0&gt;</span></span><br><span class="line">z.backward()</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="gpu">GPU</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = <span class="string">&quot;cuda&quot;</span> <span class="comment"># Use NVIDIA GPU (if available)</span></span><br><span class="line"><span class="keyword">elif</span> torch.backends.mps.is_available():</span><br><span class="line">    device = <span class="string">&quot;mps&quot;</span> <span class="comment"># Use Apple Silicon GPU (if available)</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    device = <span class="string">&quot;cpu&quot;</span> <span class="comment"># Default to CPU if no GPU is available</span></span><br><span class="line"></span><br><span class="line">tensor.to(device)</span><br><span class="line">tensor_on_gpu.cpu().numpy()</span><br></pre></td></tr></table></figure></code></pre>
<h2 id="neural-network">Neural network</h2>
<p><code>torch.nn</code></p>
<blockquote>
<p>Contains all of the building blocks for computational graphs
(essentially a series of computations executed in a particular way).</p>
</blockquote>
<p><code>torch.nn.Parameter</code></p>
<blockquote>
<p>Stores tensors that can be used with nn.Module. If requires_grad=True
gradients (used for updating model parameters via gradient descent) are
calculated automatically, this is often referred to as "autograd".</p>
</blockquote>
<p><code>torch.nn.Module</code></p>
<blockquote>
<p>The base class for all neural network modules, all the building
blocks for neural networks are subclasses. If you're building a neural
network in PyTorch, your models should subclass nn.Module. Requires a
forward() method be implemented.</p>
</blockquote>
<p><code>torch.optim</code></p>
<blockquote>
<p>Contains various optimization algorithms (these tell the model
parameters stored in nn.Parameter how to best change to improve gradient
descent and in turn reduce the loss).</p>
</blockquote>
<p><code>def forward()</code></p>
<blockquote>
<p>All nn.Module subclasses require a forward() method, this defines the
computation that will take place on the data passed to the particular
nn.Module (e.g. the linear regression formula above).</p>
</blockquote>
<h4 id="define-a-net">Define a net</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">net</span>(nn.Module):</span><br><span class="line">    __init__(<span class="variable language_">self</span>): <span class="built_in">super</span>().__init__() ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>.weights * x + <span class="variable language_">self</span>.bias</span><br><span class="line"><span class="comment">## expample</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressModle</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="variable language_">self</span>.weights = nn.Parameter(torch.randn(<span class="number">1</span>, required_grad=<span class="literal">True</span>, dtype=torch.<span class="built_in">float</span>))</span><br><span class="line">    <span class="variable language_">self</span>.bias = nn.Parameter(torch.randn(<span class="number">1</span>, required_grad=<span class="literal">True</span>, dtype=torch.<span class="built_in">float</span>))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x:torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.weight * x + <span class="variable language_">self</span>.bias</span><br></pre></td></tr></table></figure></code></pre>
<h5 id="check-module">Check module</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">model_0 = LinearRegressionModel()</span><br><span class="line"><span class="built_in">list</span>(model_0.parameters()) <span class="comment">#  tensor([0.3367], requires_grad=True)</span></span><br><span class="line">model_0.state_dict() <span class="comment"># OrderedDict([(&#x27;weights&#x27;, tensor([0.3367])), (&#x27;bias&#x27;, tensor([0.1288]))])</span></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode(): y_preds = model_0(X_test) <span class="comment"># run inference</span></span><br></pre></td></tr></table></figure></code></pre>
<h4 id="training">Training</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.L1Loss() <span class="comment"># MAE loss is same as L1Loss</span></span><br><span class="line">optimizer = torch.optim.SGD(params=model_0.parameters(), lr=<span class="number">0.01</span>) <span class="comment">## lr(learning rate)</span></span><br></pre></td></tr></table></figure></code></pre>
<table style="width:100%;">
<colgroup>
<col style="width: 1%" />
<col style="width: 18%" />
<col style="width: 63%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Step name</th>
<th>What does it do?</th>
<th>Code example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Forward pass</td>
<td>The model goes through all of the training data once, performing its
forward() function calculations.</td>
<td>model(x_train)</td>
</tr>
<tr class="even">
<td>2</td>
<td>Calculate the loss</td>
<td>The model's outputs (predictions) are compared to the ground truth
and evaluated to see how wrong they are.</td>
<td>loss = loss_fn(y_pred, y_train)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Zero gradients</td>
<td>The optimizers gradients are set to zero (they are accumulated by
default) so they can be recalculated for the specific training
step.</td>
<td>optimizer.zero_grad()</td>
</tr>
<tr class="even">
<td>4</td>
<td>Perform backpropagation on the loss</td>
<td>Computes the gradient of the loss with respect for every model
parameter to be updated (each parameter with requires_grad=True)</td>
<td>loss.backward()</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Update the optimizer (gradient descent)</td>
<td>Update the parameters with requires_grad=True with respect to the
loss gradients in order to improve them.</td>
<td>optimizer.step()</td>
</tr>
</tbody>
</table>
<h5 id="training-example">Training example</h5>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    model.train()</span><br><span class="line">y_pred = model(X_train)</span><br><span class="line">loss = loss_fn(y_pred, y_true)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure></code></pre>
<h4 id="test">test</h4>
<table>
<colgroup>
<col style="width: 1%" />
<col style="width: 20%" />
<col style="width: 59%" />
<col style="width: 18%" />
</colgroup>
<tbody>
<tr class="odd">
<td></td>
<td>Forward pass</td>
<td>The model goes through all of the training data once, performing its
forward() function calculations.</td>
<td>model(x_test)</td>
</tr>
<tr class="even">
<td></td>
<td>Calculate the loss</td>
<td>The model's outputs (predictions) are compared to the ground truth
and evaluated to see how wrong they are.</td>
<td>loss = loss_fn(y_pred, y_test)</td>
</tr>
<tr class="odd">
<td></td>
<td>Calulate evaluation metrics (optional)</td>
<td>Alongisde the loss value you may want to calculate other evaluation
metrics such as accuracy on the test set.</td>
<td>Custom functions</td>
</tr>
</tbody>
</table>
<h4 id="inference-and-save-model">Inference and save model</h4>
<p><code>Inferennce</code></p>
<blockquote>
<p>model_0.eval() # Set the model in evaluation mode with
torch.inference_mode(): y_preds = model_0(X_test)</p>
</blockquote>
<p><code>torch.save</code></p>
<blockquote>
<p>Saves a serialized object to disk using Python's pickle utility.
Models, tensors and various other Python objects like dictionaries can
be saved using torch.save.</p>
</blockquote>
<p><code>torch.load</code></p>
<blockquote>
<p>Uses pickle's unpickling features to deserialize and load pickled
Python object files (like models, tensors or dictionaries) into memory.
You can also set which device to load the object to (CPU, GPU etc).</p>
</blockquote>
<p><code>torch.nn.Module.load_state_dict</code> ## recommended</p>
<blockquote>
<p>Loads a model's parameter dictionary (model.state_dict()) using a
saved state_dict() object.</p>
</blockquote>
<h2 id="examples">Examples</h2>
<h4 id="example-1">Example 1</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">epochs = <span class="number">100</span> <span class="comment"># Set the number of epochs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create empty loss lists to track values</span></span><br><span class="line">train_loss_values = []</span><br><span class="line">test_loss_values = []</span><br><span class="line">epoch_count = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_0.train() <span class="comment"># Put model in training mode (this is the default state of a model)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass on train data using the forward() method inside</span></span><br><span class="line">    y_pred = model_0(X_train)</span><br><span class="line">    <span class="comment"># 2. Calculate the loss (how different are our models predictions to the ground truth)</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 3. Zero grad of the optimizer</span></span><br><span class="line">    loss.backward() <span class="comment"># 4. Loss backwards</span></span><br><span class="line">    optimizer.step() <span class="comment"># 5. Progress the optimizer</span></span><br><span class="line">     <span class="comment">### Testing</span></span><br><span class="line">    <span class="comment"># Put the model in evaluation mode</span></span><br><span class="line">    model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">      <span class="comment"># 1. Forward pass on test data</span></span><br><span class="line">      test_pred = model_0(X_test)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 2. Caculate loss on test data</span></span><br><span class="line">       <span class="comment"># predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type</span></span><br><span class="line">      test_loss = loss_fn(test_pred, y_test.<span class="built_in">type</span>(torch.<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Print out what&#x27;s happening</span></span><br><span class="line">      <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            epoch_count.append(epoch)</span><br><span class="line">            train_loss_values.append(loss.detach().numpy())</span><br><span class="line">            test_loss_values.append(test_loss.detach().numpy())</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | MAE Train Loss: <span class="subst">&#123;loss&#125;</span> | MAE Test Loss: <span class="subst">&#123;test_loss&#125;</span> &quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></code></pre>
<h4 id="example-2">Example 2</h4>
<pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs</span></span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Put data on the available device</span></span><br><span class="line"><span class="comment"># Without this, error will happen (not all model/data on device)</span></span><br><span class="line">X_train = X_train.to(device)</span><br><span class="line">X_test = X_test.to(device)</span><br><span class="line">y_train = y_train.to(device)</span><br><span class="line">y_test = y_test.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment">### Training</span></span><br><span class="line">    model_1.train() <span class="comment"># train mode is on by default after construction</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    y_pred = model_1(X_train)</span><br><span class="line">    <span class="comment"># 2. Calculate loss</span></span><br><span class="line">    loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Zero grad optimizer</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Loss backward</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Step the optimizer</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Testing</span></span><br><span class="line">    model_1.<span class="built_in">eval</span>() <span class="comment"># put the model in evaluation mode for testing (inference)</span></span><br><span class="line">    <span class="comment"># 1. Forward pass</span></span><br><span class="line">    <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">        test_pred = model_1(X_test)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. Calculate the loss</span></span><br><span class="line">        test_loss = loss_fn(test_pred, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch&#125;</span> | Train loss: <span class="subst">&#123;loss&#125;</span> | Test loss: <span class="subst">&#123;test_loss&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></code></pre>
<h4 id="vae">VAE</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">784</span>, hidden_dim=<span class="number">400</span>, latent_dim=<span class="number">200</span>, device=device</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, hidden_dim),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(hidden_dim, latent_dim),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># latent mean and variance</span></span><br><span class="line">        <span class="variable language_">self</span>.mean_layer = nn.Linear(latent_dim, <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.logvar_layer = nn.Linear(latent_dim, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder</span></span><br><span class="line">        <span class="variable language_">self</span>.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">2</span>, latent_dim),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(latent_dim, hidden_dim),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(hidden_dim, input_dim),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">            )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.encoder(x)</span><br><span class="line">        mean, logvar = <span class="variable language_">self</span>.mean_layer(x), <span class="variable language_">self</span>.logvar_layer(x)</span><br><span class="line">        <span class="keyword">return</span> mean, logvar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterization</span>(<span class="params">self, mean, var</span>):</span><br><span class="line">        epsilon = torch.randn_like(var).to(device)</span><br><span class="line">        z = mean + var*epsilon</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean, logvar = <span class="variable language_">self</span>.encode(x)</span><br><span class="line">        z = <span class="variable language_">self</span>.reparameterization(mean, logvar)</span><br><span class="line">        x_hat = <span class="variable language_">self</span>.decode(z)</span><br><span class="line">        <span class="keyword">return</span> x_hat, mean, log_var</span><br></pre></td></tr></table></figure>
<h4 id="cnn">CNN</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a neural net class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="comment"># Constructor</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Our images are RGB, so input channels = 3. We&#x27;ll apply 12 filters in the first convolutional layer</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We&#x27;ll apply max pooling with a kernel size of 2</span></span><br><span class="line">        <span class="variable language_">self</span>.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># A second convolutional layer takes 12 input channels, and generates 12 outputs</span></span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=<span class="number">12</span>, out_channels=<span class="number">12</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># A third convolutional layer takes 12 inputs and generates 24 outputs</span></span><br><span class="line">        <span class="variable language_">self</span>.conv3 = nn.Conv2d(in_channels=<span class="number">12</span>, out_channels=<span class="number">24</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># A drop layer deletes 20% of the features to help prevent overfitting</span></span><br><span class="line">        <span class="variable language_">self</span>.drop = nn.Dropout2d(p=<span class="number">0.2</span>)</span><br><span class="line">        <span class="comment"># Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.</span></span><br><span class="line">        <span class="comment"># So our feature tensors are now 32 x 32, and we&#x27;ve generated 24 of them</span></span><br><span class="line">        <span class="comment"># We need to flatten these and feed them to a fully-connected layer</span></span><br><span class="line">        <span class="comment"># to map them to  the probability for each class</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(in_features=<span class="number">32</span> * <span class="number">32</span> * <span class="number">24</span>, out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Use a relu activation function after layer 1 (convolution 1 and pool)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.pool(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Use a relu activation function after layer 2 (convolution 2 and pool)</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.pool(<span class="variable language_">self</span>.conv2(x)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select some features to drop after the 3rd convolution to prevent overfitting</span></span><br><span class="line">        x = F.relu(<span class="variable language_">self</span>.drop(<span class="variable language_">self</span>.conv3(x)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Only drop the features if this is a training pass</span></span><br><span class="line">        x = F.dropout(x, training=<span class="variable language_">self</span>.training)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Flatten</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">32</span> * <span class="number">32</span> * <span class="number">24</span>)</span><br><span class="line">        <span class="comment"># Feed to fully-connected layer to predict class</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="comment"># Return log_softmax tensor</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="lstm">LSTM</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMClassifier</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim, output_size</span>):</span><br><span class="line">    <span class="built_in">super</span>(LSTMClassifier, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="variable language_">self</span>.embedding_dim = embedding_dim</span><br><span class="line">    <span class="variable language_">self</span>.hidden_dim = hidden_dim</span><br><span class="line">    <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">    <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">    <span class="variable language_">self</span>.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=<span class="number">1</span>)</span><br><span class="line">    <span class="variable language_">self</span>.hidden2out = nn.Linear(hidden_dim, output_size)</span><br><span class="line">    <span class="variable language_">self</span>.softmax = nn.LogSoftmax()</span><br><span class="line">    <span class="variable language_">self</span>.dropout_layer = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">init_hidden</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span>(autograd.Variable(torch.randn(<span class="number">1</span>, batch_size, <span class="variable language_">self</span>.hidden_dim)),</span><br><span class="line">            autograd.Variable(torch.randn(<span class="number">1</span>, batch_size, <span class="variable language_">self</span>.hidden_dim)))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch, lengths</span>):</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>.hidden = <span class="variable language_">self</span>.init_hidden(batch.size(-<span class="number">1</span>))</span><br><span class="line">    embeds = <span class="variable language_">self</span>.embedding(batch)</span><br><span class="line">    packed_input = pack_padded_sequence(embeds, lengths)</span><br><span class="line">    outputs, (ht, ct) = <span class="variable language_">self</span>.lstm(packed_input, <span class="variable language_">self</span>.hidden)</span><br><span class="line">    <span class="comment"># ht is the last hidden state of the sequences</span></span><br><span class="line">    <span class="comment"># ht = (1 x batch_size x hidden_dim)</span></span><br><span class="line">    <span class="comment"># ht[-1] = (batch_size x hidden_dim)</span></span><br><span class="line">    output = <span class="variable language_">self</span>.dropout_layer(ht[-<span class="number">1</span>])</span><br><span class="line">    output = <span class="variable language_">self</span>.hidden2out(output)</span><br><span class="line">    output = <span class="variable language_">self</span>.softmax(output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h2 id="references">References</h2>
<pre><code>1. https://github.com/mrdbourke/pytorch-deep-learning
2. https://readmedium.com/@rekalantar/variational-auto-encoder-vae-pytorch-tutorial-dce2d2fe0f5f
3. https://github.com/MicrosoftDocs/ml-basics/blob/master/05b%20-%20Convolutional%20Neural%20Networks%20(PyTorch).ipynb
4. https://github.com/ritchieng/the-incredible-pytorch?tab=readme-ov-file
5. https://github.com/claravania/lstm-pytorch
6. https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/</code></pre>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/05/10/CCA/" rel="prev" title="A tutorial on Canonical Correlation Analysis(CCA)">
                  <i class="fa fa-angle-left"></i> A tutorial on Canonical Correlation Analysis(CCA)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/06/07/spectral/" rel="next" title="Spectral analysis (1)">
                  Spectral analysis (1) <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class=""></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Mingbo Cheng</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"chengmingbo/gitment-comments","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
