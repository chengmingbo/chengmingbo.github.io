<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chengmingbo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Mingbo">
<meta property="og:type" content="website">
<meta property="og:title" content="Mingbo">
<meta property="og:url" content="http://chengmingbo.github.io/index.html">
<meta property="og:site_name" content="Mingbo">
<meta property="og:description" content="Mingbo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Mingbo Cheng">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://chengmingbo.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Mingbo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mingbo</h1>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="/%5Bobject%20Object%5D" alt="Mingbo">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-slides"><a href="/slides/" rel="section"><i class="area-chart fa-fw"></i>slides</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mingbo Cheng</p>
  <div class="site-description" itemprop="description">Mingbo</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chengmingbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chengmingbo" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="http://www.flickering.cn/" title="http:&#x2F;&#x2F;www.flickering.cn&#x2F;" rel="noopener" target="_blank">flickering</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.zybuluo.com/codeep/note/163962" title="https:&#x2F;&#x2F;www.zybuluo.com&#x2F;codeep&#x2F;note&#x2F;163962" rel="noopener" target="_blank">mathjax grammar</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vividfree.github.io/" title="http:&#x2F;&#x2F;vividfree.github.io&#x2F;" rel="noopener" target="_blank">vividfree</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://colah.github.io/" title="http:&#x2F;&#x2F;colah.github.io&#x2F;" rel="noopener" target="_blank">colah</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.autonlab.org/tutorials" title="https:&#x2F;&#x2F;www.autonlab.org&#x2F;tutorials" rel="noopener" target="_blank">Andrew Moore</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://plot.ly/matlab/plot/" title="https:&#x2F;&#x2F;plot.ly&#x2F;matlab&#x2F;plot&#x2F;" rel="noopener" target="_blank">matlabplot</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.ryanzhang.info/blog/" title="http:&#x2F;&#x2F;www.ryanzhang.info&#x2F;blog&#x2F;" rel="noopener" target="_blank">Ryan’s Cabinet</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.cnblogs.com/jerrylead/tag/Machine%20Learning/" title="http:&#x2F;&#x2F;www.cnblogs.com&#x2F;jerrylead&#x2F;tag&#x2F;Machine%20Learning&#x2F;" rel="noopener" target="_blank">JerryLead</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://yxzf.github.io/" title="https:&#x2F;&#x2F;yxzf.github.io&#x2F;" rel="noopener" target="_blank">YXZF'S BLOG</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vonng.com/" title="http:&#x2F;&#x2F;vonng.com" rel="noopener" target="_blank">VONNG</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2019/05/10/CCA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/05/10/CCA/" class="post-title-link" itemprop="url">A tutorial on Canonical Correlation Analysis(CCA)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-05-10 19:54:43" itemprop="dateCreated datePublished" datetime="2019-05-10T19:54:43+02:00">2019-05-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="introduction">Introduction</h2>
<p>Suppose we have two sets of variable corresponding to two aspects
such as height and weight, we want to analysis the relationship between
this two sets. There are several ways to measure the relationship
between them. However, sometime the it is hard to handle datasets with
different dimensions, meaning, if <span class="math inline">\(X\in
\mathbb{R}^m\)</span> and <span class="math inline">\(Y\in
\mathbb{R}^n\)</span>, how to resolve the relationship?</p>
<h2 id="yxsic-of-cca">yxsic of CCA</h2>
<p>Assume there are two sets of data <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>, the size of <span
class="math inline">\(X\)</span> is <span class="math inline">\(n \times
p\)</span>, whereas size of <span class="math inline">\(Y\)</span> is
<span class="math inline">\(n\times q\)</span>. That is, <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> share the same row numbers but are
differnt in columns number. The idea of CCA is simple: find the best
match of <span class="math inline">\(X w_x\)</span> and <span
class="math inline">\(Y w_y\)</span>. Let's just set: <span
class="math display">\[X w_x = z_x\qquad\text{and}\qquad Y w_y =
z_y\]</span></p>
<p>Where <span class="math inline">\(X\in \mathbb{R}^{n\times
p}\)</span>, <span class="math inline">\(w_x \in
\mathbb{R}^{p}\)</span>, <span class="math inline">\(z_x\in
\mathbb{R}^n\)</span>, <span class="math inline">\(Y\in
\mathbb{R}^{n\times q}\)</span>, <span class="math inline">\(w_y \in
\mathbb{R}^{q}\)</span>, <span class="math inline">\(z_y\in
\mathbb{R}^n\)</span>. <span class="math inline">\(w_x\)</span> and
<span class="math inline">\(w_y\)</span> are often refered as canonical
weight vectors, <span class="math inline">\(z_x\)</span> and <span
class="math inline">\(z_y\)</span> are named images as well as canonical
variates or canonical scores. To simplify the problem, we assume <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are standardized to zero mean and unit
variance. Our task is to maximize the angle of <span
class="math inline">\(z_x\)</span> and <span
class="math inline">\(z_y\)</span>, meaning:</p>
<p><span class="math display">\[\max_{z_x, z_y \in \mathbf{R^n}}
&lt;z_x, z_y&gt;=\max \cos(z_x, z_y)=\max\frac{&lt;z_x,
z_y&gt;}{\|z_x\|\|z_y\|}\]</span></p>
<p>with respect to: <span class="math inline">\(\|z_x\|_{2}=1\quad
\|z_y\|_{2}=1\)</span>.</p>
<p>In fact, our task is just project <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> to a new coordinate system after the
linear transformation to <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span>.</p>
<h2 id="resolve-cca">Resolve CCA</h2>
<p>There are many solutions to this problems. Before start, We need make
some assumptions: 1. the each column vector of <span
class="math inline">\(X\)</span> is perpendicular to the others. Which
means <span class="math inline">\(X^T X= I\)</span>. The assumption is
the same with <span class="math inline">\(Y\)</span> and <span
class="math inline">\(w_x, w_y\)</span>. We can find <span
class="math inline">\(\min(p,q)\)</span> canonical components, and the
<span class="math inline">\(r\)</span>th component is orthogonal to all
the <span class="math inline">\(r-1\)</span> components.</p>
<h4 id="resolve-cca-through-svd">Resolve CCA through SVD</h4>
<p>To solve the CCA problem using SVD, we first introduce the joint
covariance matrix <span class="math inline">\(C\)</span> such such that:
<span class="math display">\[\begin{equation}
    C = \begin{pmatrix}
        C_{xx} &amp; C_{xy}\\
        C_{yx} &amp; C_{yy}\\
    \end{pmatrix}
\end{equation}\]</span> Where <span
class="math inline">\(C_{xx}=\frac{1}{n-1}X^\top X\)</span> and <span
class="math inline">\(C_{yy}=\frac{1}{n-1}Y^\top Y\)</span> are the
empirical variance matrices between <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> respectively. The <span
class="math inline">\(C_{xy}=\frac{1}{n-1} X^\top Y\)</span> is the
covariance matrix between <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span>.</p>
<p>We next can reform CCA problem with two linear transformations <span
class="math inline">\(w_x\)</span> and <span
class="math inline">\(w_y\)</span> such that:</p>
<p><span class="math display">\[\begin{equation}
w_x^\top C_{xx} w_x = I_p, \quad w_y^\top C_{yy} w_y = I_q, \quad
w_x^\top C_{xy} w_y = D
\end{equation}\]</span> Where I_p and I_q are th p-dimensional and
q-dimensional identity meatrics respectively. The diagonal matrix <span
class="math inline">\(D = \text{diag}(\gamma_i)\)</span> so that:</p>
<p><span class="math display">\[\begin{equation}
    \begin{pmatrix}
        {w}_x^\top &amp; { 0}\\
        { 0} &amp;  {w}_y^\top
        \end{pmatrix}
        \begin{pmatrix}
        C_{xx} &amp; C_{xy}\\
        C_{yx} &amp; C_{yy}
        \end{pmatrix}
        \begin{pmatrix}
         {w}_x &amp; { 0}\\
        { 0} &amp;  {w}_y
        \end{pmatrix}
        =
        \begin{pmatrix}
        I_p &amp; D\\
        D^\top &amp; I_q
    \end{pmatrix},
\end{equation}\]</span></p>
<p>The canoical variable: <span class="math display">\[\begin{equation}
Z_x = Xw_x, \quad Z_y = Y w_y
\end{equation}\]</span> The diagonal elements <span
class="math inline">\(\gamma_i\)</span> of D denote the canonical
correlations. Thus we find the linear compounds <span
class="math inline">\({Z}_x\)</span> and <span
class="math inline">\({Z}_y\)</span> to maximize the cross-correlations.
Since both <span class="math inline">\(C_{xx}\)</span> and <span
class="math inline">\(C_{yy}\)</span> are symmetric positive definite,
we can perform Cholesky Decomposition on them to get: <span
class="math display">\[\begin{equation}
    C_{xx} = C_{xx}^{\top/2} C_{xx}^{1/2}, \quad C_{yy} =
C_{yy}^{\top/2} C_{yy}^{1/2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(C_{xx}^{\top/2}\)</span> is the
transpose of <span class="math inline">\(C_{xx}^{1/2}\)</span>. Applying
the inverses of the square root factors symmetrically on the joint
covariance matrix <span class="math inline">\(C\)</span>, the matrix is
transformed into: <span class="math display">\[\begin{equation}
\begin{pmatrix}
    C_{xx}^{-\top/2} &amp; {\mathbf 0}\\
    {\mathbf 0} &amp; C_{yy}^{-\top/2}
    \end{pmatrix}
    \begin{pmatrix}
    C_{xx} &amp; C_{ab}\\
    C_{yx} &amp; C_{yy}
    \end{pmatrix}
    \begin{pmatrix}
    C_{xx}^{-1/2} &amp; {\mathbf 0}\\
    {\mathbf 0} &amp; C_{yy}^{-1/2}
    \end{pmatrix}
    =
    \begin{pmatrix}
    I_p &amp; C_{xx}^{-1/2}C_{ab}C_{yy}^{-1/2}\\
    C_{yy}^{-1/2}C_{yx}C_{xx}^{-1/2} &amp; I_q
\end{pmatrix}.
\end{equation}\]</span></p>
<p>The canonical correlation problem is reduced to that of finding an
SVD of a triple product: <span class="math display">\[\begin{equation}
    U^{\top} (C_{xx}^{-1/2}C_{ab}C_{yy}^{-1/2}) V = D.
\end{equation}\]</span> The matrix <span
class="math inline">\(C\)</span> is thus reduced to the joint covariance
matrix by applying a two-sided Jacobi method such that: <span
class="math display">\[\begin{equation}
    \begin{pmatrix}
        U^\top &amp; {\mathbf 0}\\
        {\mathbf 0} &amp; V^\top
    \end{pmatrix}
    \begin{pmatrix}
        I_p &amp; C_{xx}^{-1/2}C_{ab}C_{yy}^{-1/2}\\
        C_{yy}^{-1/2}C_{_y}C_{xx}^{-1/2} &amp; I_q
    \end{pmatrix}
    \begin{pmatrix}
        U &amp; {\mathbf 0}\\
        {\mathbf 0} &amp; V
    \end{pmatrix} =
    \begin{pmatrix}
    I_p &amp; D\\
    D^\top &amp; I_q
    \end{pmatrix}
\end{equation}\]</span></p>
<p>with the desired transformation <span
class="math inline">\({w}_x\)</span> and <span
class="math inline">\({w}_y\)</span>: <span
class="math display">\[\begin{equation}
    {w}_x = C_{xx}^{-1/2} U, \quad {w}_y = C_{yy}^{-1/2}V
\end{equation}\]</span> where the singular values <span
class="math inline">\(\gamma_i\)</span> are in descending order such
that: <span class="math display">\[\begin{equation}
    \gamma_1 \geq \gamma_2 \geq \cdots \geq 0.
\end{equation}\]</span></p>
<h4 id="resolve-cca-through-standard-eigenvalue-problem">Resolve CCA
through Standard EigenValue Problem</h4>
<p>The Problem can be reformed to solve the problem: <span
class="math display">\[\begin{equation}
\underset{w_x \in \mathbb{R}^p, w_y\in \mathbb{R}^q}{\arg \max} w_x^\top
C_{xy} w_y
\end{equation}\]</span> With respect to <span
class="math inline">\(\|\|w_x^\top C_{xx} w_x\|\|_2 = \sqrt{w_x^\top
C_{xx} w_x}=1\)</span> and <span class="math inline">\(\|\|w_y^\top
C_{yy} w_y\|\|_2 = \sqrt{w_y^\top C_{yy} w_y}=1\)</span>. The problem
can apparently sovled by Lagrange multiplier technique. Let construct
the Lagrange multiplier <span class="math inline">\(L\)</span> such
that: <span class="math display">\[\begin{equation}
    L = w_x^\top C_{xy} w_y - \frac{\rho_1}{2} w_x^\top C_{xx} w_x -
\frac{\rho_2}{2} w_y^\top C_{yy} w_y
    \end{equation}\]</span></p>
<p>The differentiation of L to <span class="math inline">\(w_x\)</span>
and <span class="math inline">\(w_y\)</span> is: <span
class="math display">\[\begin{equation}
\begin{aligned}
\frac{\partial L}{\partial w_x} = C_{xy} w_y - \rho_1 C_{xx}w_x =
\mathbf{0}\\
\frac{\partial L}{\partial w_y} = C_{yx} w_x - \rho_2 C_{yy}w_y =
\mathbf{0}
\end{aligned}
\end{equation}\]</span></p>
<p>By left multipling <span class="math inline">\(w_x\)</span> and <span
class="math inline">\(w_y\)</span> the above equation, we have:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
w_x^\top C_xy w_y -\rho_1 w_x^\top C_xx w_x = \mathbf{0}\\
w_y^\top C_yx w_x -\rho_2 w_y^\top C_yy w_y = \mathbf{0}
\end{aligned}
\end{equation}\]</span> Since w_x^C_xx w_x = 1 and w_y^C_yy w_y = 1, we
can obtain that <span class="math inline">\(\rho_1 = \rho_2 =
\rho\)</span>. By substituting <span class="math inline">\(\rho\)</span>
to the formula. We can get: <span
class="math display">\[\begin{equation}
w_x = \frac{C_{xx}^{-1}C_{xy}w_y}{rho}
\end{equation}\]</span> Evantually we have the equation: <span
class="math display">\[\begin{equation}
C_{yx} C_{xx}^{-1} C_{xy} w_y = \rho^2 C_yy w_y
\end{equation}\]</span> Obviously, this is the form of eigenvalue
decompostion problem where all eigen values are greater or equal to
zero. By solving the eigenvalue decomposition we can find <span
class="math inline">\(w_x\)</span> and <span
class="math inline">\(w_y\)</span>.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2019/03/10/add-comments/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/03/10/add-comments/" class="post-title-link" itemprop="url">简单增加博客评论</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-03-10 20:55:48" itemprop="dateCreated datePublished" datetime="2019-03-10T20:55:48+01:00">2019-03-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="博客的评论系统">博客的评论系统</h2>
<p>希望把博客的评论系统建立起来，之前使用的是disqus，重新部署的时候，页面大部分都无法显示。不想再用disqus。看到有人创造性的利用github作为载体建立评论系统，也就是Gitment了。
按照教程在github上设置了Gitment，惊闻Gitment需要请求服务，是作者搭的，作者已经不维护了。按照以下操作：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i --save gitment</span><br></pre></td></tr></table></figure></p>
<p>修改自己js，连接自己搭建的服务器，WTF？ <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/node_modules/gitment/dist/gitment.browser.js</span><br></pre></td></tr></table></figure>
详细修改过程可参照：
https://sherry0429.github.io/2019/02/12/gitment%E4%BF%AE%E5%A4%8D/</p>
<p>后继续寻觅其他可以评论系统，找到这篇文章：
https://wangjiezhe.com/posts/2018-10-29-Hexo-NexT-3/
根据此文章的教程安装了utterances。目前发现还是比较不错。知识现在看到的效果是全局评论。
issue-term不太了解具体，目前不想深入探究，仅仅设置pathname。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2019/03/10/Change-theme-to-Next/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/03/10/Change-theme-to-Next/" class="post-title-link" itemprop="url">Change theme to Next</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-03-10 14:07:25" itemprop="dateCreated datePublished" datetime="2019-03-10T14:07:25+01:00">2019-03-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="不再折腾主题乖乖的切到next">不再折腾主题，乖乖的切到Next</h2>
<p>又一次，今年年初的又一次，博客系统hexo下的maupassant主题又罢工了。由于年初的各种事情繁琐而多，我就放弃治疗博客系统了，也就是说，有新的博文也无法发出来，先不管那些报错了。</p>
<p>现在稍微腾出一点儿时间，准备把博客系统好好弄一下。其实最简单的办法，也是屡试不爽的方法就是把所有的环境重新安装一遍，显示hexo，再是maupassant。这次不灵了，hexo
generate之后一堆报错。我甚至觉得maupassant已经无法搞定了，搜索错误的关键词，发现没有人遇到与我相同的问题。最后是怀疑我文章里有公式的特殊字符，影响markdown
parse。修改了hexo-renderer-marked的js，仍然有问题。最后决定换其他主题了，然而只要把所有的文章迁移过来一定会报错。报错如下：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INFO  Start processing</span><br><span class="line">FATAL Something&#x27;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html</span><br><span class="line">Template render error: (unknown path) [Line 65, Column 565]</span><br><span class="line">  expected variable end</span><br><span class="line">    at Object._prettifyError (/Users/chengmingbo/blog_deploy/blog/node_modules/nunjucks/src/lib.js:36:11)</span><br><span class="line">    at Template.render (/Users/chengmingbo/blog_deploy/blog/node_modules/nunjucks/src/environment.js:542:21)</span><br><span class="line">    at Environment.renderString (/Users/chengmingbo/blog_deploy/blog/node_modules/nunjucks/src/environment.js:380:17</span><br><span class="line">    ... ...</span><br></pre></td></tr></table></figure></p>
<p>好吧，一切从头来，一个文件一个文件的添加，每次hexo
generate一下。终于找到了一个有问题的文件。先注释掉再说，后面慢慢查是什么特殊字符引起的问题。好在可以更新博客了。</p>
<h3 id="反复">反复</h3>
<p>选定了Next主题，又出现了反复，加上评论系统disqus发现博客白屏了，只有一个文件头显示。可是加功能一时爽，调试火葬场。当时实在记不起来到底是加了什么使得博客又不工作了。只能重头再来。在找主题的过程中发现star排名第四的hexo-theme-apollo已经停止开发，作者一句话让我决定不再折腾什么主题了：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">专注文章内容的创作胜过博客样式的美观，祝各位玩的开心:</span><br></pre></td></tr></table></figure></p>
<h3 id="后续工作">后续工作</h3>
<ol type="1">
<li>追查出什么特殊字符引起了hexo generate出现问题</li>
<li>看是否能复原评论系统，如果不能先这样吧，只要不耽误写博文。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/08/07/Expectation-and-variance-of-poisson-distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/08/07/Expectation-and-variance-of-poisson-distribution/" class="post-title-link" itemprop="url">Expectation and Variance of Poisson Distribution</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-08-07 21:49:00" itemprop="dateCreated datePublished" datetime="2017-08-07T21:49:00+02:00">2017-08-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Pmf of Poisson Distribution is as follows:</p>
<p><span class="math display">\[f(X=k;\lambda)=\frac{\lambda^k
e^{-\lambda}}{k!}\]</span></p>
<p>Our aim is to derive the the expectation of <span
class="math inline">\(E(X)\)</span> and the variance <span
class="math inline">\(Var(X)\)</span>. Given that the formula of
expectation: <span class="math display">\[
E(X)=\sum_{k=0}^{\infty} k \frac{\lambda^k e^{-\lambda }}{k!}
\]</span></p>
<p>Notice that when <span class="math inline">\(k=0\)</span>, the
formula is equal to 0, that is:</p>
<p><span class="math display">\[\sum_{k=0}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}\Large|_{k=0}=0\]</span></p>
<p>Then, the formula become as followed:</p>
<p><span class="math display">\[E(X)=\sum_{k=1}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}\]</span></p>
<p><span
class="math display">\[\begin{aligned}E(X)&amp;=\sum_{k=0}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}=\sum_{k=0}^{\infty}
\frac{\lambda^ke^{-\lambda}}{(k-1)!}\\&amp;=\sum_{k=0}^{\infty}  \frac{\lambda^{k-1}\lambda
e^{-\lambda}}{(k-1)!}\\&amp;=\lambda
e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}\end{aligned}\]</span></p>
<p>Now we need take advantage of Taylor Expansion, recall that:</p>
<p><span
class="math display">\[e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\cdots+\frac{x^{k-1}}{(k-1)!}=\sum_{k=1}^{\infty}\frac{x^{k-1}}{(k-1)!}\]</span></p>
<p>Compare <span class="math inline">\(E(X)\)</span>, we can get:</p>
<p><span class="math display">\[E(X)=\lambda
e^{-\lambda}e^\lambda=\lambda\]</span></p>
<p>As known that <span
class="math inline">\(Var(X)=E(X^2)-(E(x))^2\)</span>, we just get <span
class="math inline">\(E(X^2)\)</span>. Given that:</p>
<p><span class="math display">\[E(X)=\sum_{k=1}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}=\lambda\]</span></p>
<p>we can use this formula to derive the <span
class="math inline">\(E(X^2)\)</span>,</p>
<p><span
class="math display">\[\begin{aligned}E(X)=&amp;\sum_{k=1}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}=\lambda\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}
k \frac{\lambda^k}{k!}=\lambda
e^{\lambda}\\\Leftrightarrow&amp;\frac{\partial\sum_{k=1}^{\infty} k
\frac{\lambda^k}{k!}}{\partial \lambda}=\frac{\partial \lambda
e^{\lambda}}{\partial
\lambda}\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k-1}}{k!}=e^\lambda+\lambda
e^\lambda\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k-1}e^{-\lambda}}{k!}=1+\lambda
\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k}e^{-\lambda}}{k!}=\lambda+\lambda^2=E(X^2)\end{aligned}\]</span></p>
<p>then,</p>
<p><span
class="math display">\[Var(X)=E(X^2)-(E(X))^2=\lambda+\lambda^2-(\lambda)^2=\lambda\]</span></p>
<p>Thus, we have proved that the Expectation and the Variance of Poisson
Distribution are both <span class="math inline">\(\lambda\)</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/06/17/sample-variance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/06/17/sample-variance/" class="post-title-link" itemprop="url">样本方差为什么除以N-1?（翻译）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-06-17 17:52:38" itemprop="dateCreated datePublished" datetime="2017-06-17T17:52:38+02:00">2017-06-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文作者：<a target="_blank" rel="noopener" href="http://www.visiondummy.com/">Vincent Spruy</a></p>
<p>译者：程明波</p>
<p><a
target="_blank" rel="noopener" href="http://www.visiondummy.com/2014/03/divide-variance-n-1/">英文文章地址</a></p>
<p><a
href="http://chengmingbo.github.io/2017/06/17/sample-variance/">译文地址</a></p>
<p>译者注：由于历史原因，高斯分布(Gaussian
Distribution)，正态分布(Normal Distribution)皆指概率密度函数形如<span
class="math inline">\(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span>的分布。文中我会采用正态分布的提法。</p>
<h3 id="简介">简介</h3>
<p>本文，呼应标题，我将推导著名正态分布数据均值和方差的计算公式。如果一些读者对于这个问题的“为什么”并不感兴趣，仅仅是对“什么时候使用”感兴趣，那答案就非常简单了：</p>
<p>如果你想预估一份数据的均值和方差(典型情况)，那么方差公式除的是<span
class="math inline">\(N-1\)</span>，即：</p>
<p><span class="math display">\[\sigma^2 = \frac{1}{N-1}\sum_{i=1}^N
(x_i - \mu)^2\]</span></p>
<p>另一种情况，如果整体的真实均值已知，那么方差公式除的就是<span
class="math inline">\(N\)</span>，即：</p>
<p><span class="math display">\[\sigma^2 = \frac{1}{N}\sum_{i=1}^N (x_i
- \mu)^2\]</span></p>
<p>然而，前一种情况，会是你遇到更典型的情形。一会儿，我会举一个预估高斯白噪音的离散程度例子。例子中高斯白噪音的均值是已知的0，这种情况下，我们只需要估计方差。</p>
<p>如果数据是正态分布，我们可以完全用均值<span
class="math inline">\(\mu\)</span>和方差<span
class="math inline">\(\sigma^2\)</span>刻画这个分布。其中，方差是标准差<span
class="math inline">\(\sigma\)</span>的平方，标准差代表了每个数据点偏离均值点的平均距离，也就是说，方差表示了数据离散程度。对于正态分布，68.3%的数据的值会介于<span
class="math inline">\(\mu-\sigma\)</span>和<span
class="math inline">\(\mu+\sigma\)</span>之间。下面图片展示是一个正态分布的概率密度函数，他的均值是<span
class="math inline">\(\mu=10\)</span>,方差是<span
class="math inline">\(\sigma^2=3^2=9\)</span>：</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-06-20-110159.jpg" /></p>
<p><strong>图1.</strong> 正态分布概率密度函数.
对于正态分布数据，68%的样本落在均值<span
class="math inline">\(\pm\)</span>方差。</p>
<p>通常，我们拿不到全部的全体数据。上面的例子中，典型的情况是我们有一些观察数据，但是，我们没有上图中x轴上所有可能的观察数据。例如我们可能有下面一些观察数据：</p>
<p>表1</p>
<table>
<thead>
<tr class="header">
<th>观察数据ID</th>
<th>观察值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>观察数据 1</td>
<td>10</td>
</tr>
<tr class="even">
<td>观察数据 2</td>
<td>12</td>
</tr>
<tr class="odd">
<td>观察数据 3</td>
<td>7</td>
</tr>
<tr class="even">
<td>观察数据 4</td>
<td>5</td>
</tr>
<tr class="odd">
<td>观察数据 5</td>
<td>11</td>
</tr>
</tbody>
</table>
<p>现在如果我们通过把所有值相加并除以观察的次数，得到经验均值：</p>
<p><span
class="math display">\[\mu=\frac{10+12+7+5+11}{5}=9\tag{1}\]</span>.</p>
<p>通常，我们会假设经验均值接近分布的未知的真实均值，因此，我们可以假设观测数据来自于均值为<span
class="math inline">\(\mu=9\)</span>的正态分布。在这个例子中，分布真实均值是10，
也就是说，经验均值实际上接近于真实均值。</p>
<p>数据的方差计算如下：</p>
<p><span class="math display">\[\begin{aligned}\sigma^2&amp;=
\frac{1}{N-1}\sum_{i=1}^N (x_i - \mu)^2\\&amp;=
\frac{(10-9)^2+(12-9)^2+(7-9)^2+(5-9)^2+(11-9)^2}{4})\\&amp;=
8.5.\end{aligned}\tag{2}\]</span></p>
<p>同样，我们一般假设经验方差接近于基于分布真实未知方差。在此例中，真实方差是9，所以，经验方差也是接近于真实方差。</p>
<p>那么我们手上的问题现在就是为什么我们用于计算经验均值和经验方差的公式是正确的。事实上，另一个我们经常用于计算方差的公式是这样定义的：</p>
<p><span class="math display">\[\begin{aligned}\sigma^2 &amp;=
\frac{1}{N}\sum_{i=1}^N (x_i - \mu)^2 \\&amp;=
\frac{(10-9)^2+(12-9)^2+(7-9)^2+(5-9)^2+(11-9)^2}{4}) \\&amp;=
6.8.\end{aligned}\tag{3}\]</span></p>
<p>公式(2)和公式(3)的唯一不同是前一个公式除的是<span
class="math inline">\(N-1\)</span>，而后一个除的是<span
class="math inline">\(N\)</span>。两个公式都是对的，只是根据不同的场景使用不同的公式。</p>
<p>接下来的部分，我们针对给定一个正态分布的样本集，完成对其未知方差和均值最好估计的完整推导。我们将会看到，一些情况下，方差除的是<span
class="math inline">\(N\)</span>，另一些情况除的是<span
class="math inline">\(N-1\)</span>。</p>
<p>用一个公式近似一个参数(均值或方差)叫做估计量。下面，我们定义一个分布的真实但未知的参数为<span
class="math inline">\(\hat{\mu}\)</span>和<span
class="math inline">\(\hat{\sigma}^2\)</span>。而估计量，例如，经验的平均和经验方差，定义为<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma^2\)</span>。</p>
<p>为了找到最优的估计量，首先，一个整体均值为<span
class="math inline">\(\mu\)</span>标准差为<span
class="math inline">\(\sigma\)</span>的正态分布，对于特定的观察点<span
class="math inline">\(x_i\)</span>，我们需要一个分析相似的表达式。对于一个已知参数的正态分布一般定义为<span
class="math inline">\(N(\mu,\sigma^2)\)</span>。似然函数为：</p>
<p><span class="math display">\[x_i \sim N(\mu,\sigma^2) \Rightarrow
P(x_i;
\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}.\tag{4}\]</span></p>
<p>为了计算均值和方差，显然，我们需要这个分布一个以上的样本。接下来，设<span
class="math inline">\(\vec{x}=(x_1,x_2,\cdots,x_N)\)</span>为包含所有的可用样本的向量（例如：表一中所有的值）。如果所有这些样本统计独立，我们可以写出联合似然函数为所有似然函数的乘积：</p>
<p><span
class="math display">\[\begin{aligned}P(\vec{x};\mu,\sigma^2)&amp;=P(x_1,x_2,\cdots,x_n;\mu,\sigma^2)\\&amp;=P(x_1;\mu,\sigma^2)P(x_2;\mu,\sigma^2)\cdots
P(x_N;\mu,\sigma^2)\\&amp;=\prod_{i=1}^{N}P(x_i;\mu,\sigma^2)\end{aligned}.\tag{5}\]</span></p>
<p>把公式(4)代入公式(5)，可得出联合概率密度函数的分析表达式：</p>
<p><span
class="math display">\[\begin{aligned}P({\vec{x};\mu,\sigma})&amp;=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x_i-\mu)^2}\\&amp;=\frac{1}{(2\pi\sigma^2)^{\frac{N}{2}}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i-\mu)^2}\end{aligned}.\tag{6}\]</span></p>
<p>公式(6)在接下来的部分将非常重要。我们会用它推导关于正态分布著名的估计量均值和方差。</p>
<h3 id="最小方差无偏估计量">最小方差，无偏估计量</h3>
<p>决定一个估计量是不是“好”估计量，首先我们需要定义什么是真正的“好”
估计量。说一个估计量好，依赖于两个度量，叫做其偏差(bias)和方差(variance)(是的，我们要讨论均值估计量的方差，以及方差估计量的方差)。本节将简单的讨论这两个度量。</p>
<h4 id="参数偏差">参数偏差</h4>
<p>想象一下，如果我们能拿到全体不同的(互斥)数据子集。类比之前的的例子，假设，除了【表1】中的数据，我们还有完全不同观察结果表2及表3。那么，一个关于均值好的估计量，应该使得这个估计量平均下来等于真实的均值。我们可以接受其中一个自己的经验均值不等于真实均值，但是，一个好的估计量应该保证：对于所有子集均值估计的平均值等于真实均值。这个限制条件用数学化的表示，就是估计量的期望值(Expected
Value)应该等于参数值：</p>
<p><span class="math display">\[E(\mu)=\hat{\mu}\qquad
E(\sigma^2)=\hat{\sigma}^2.\tag{7}\]</span></p>
<p>如果满足上面的条件，那么这些估计量就被称之为“无偏估计”。反之，如果上面的条件不满足，这些估计量叫做“有偏的”，也就是说平均来看，他们或者低估或者高估了参数的真实值。</p>
<h4 id="参数方差">参数方差</h4>
<p>无偏估计量保证平均来看，它们估计的值等于真是参数。但是，这并不意味着每次估计是一个好的估计。比如，如果真实均值为10，一个无偏估计量可以估计全体的其中一个子集的均值为50，而另一个均值为-30。期望的估计的值确实是10，也等于真是的参数值，但是，估计量的质量明显依赖每次估计的离散程度。对于全体5个不同子集，一个估计量产生的估计值(10,15,5,12,8)是无偏的和另一个估计量产生的估计值（50，-30，100，-90，20）（译者注：原文作者最后一个是10，我计算换成20，这样均值才是10）。但是第一个估计量的所有估计值明显比第二个估计量的估计值更接近真实值。</p>
<p>因此，一个好的估计量不仅需要有低偏差，同时也需要低方差。这个方差表示为平均平方误差的估计量：</p>
<p><span
class="math display">\[Var(\mu)=E[(\hat{\mu}-\mu)^2]\]</span></p>
<p><span
class="math display">\[Var(\sigma^2)=E[(\hat{\sigma}-\sigma)^2]\]</span></p>
<p>因此一个好的估计量是低偏差，低方差的。如果存在最优的估计量，那么这个估计应该是无偏的，而且方差比所有的其他可能估计量都要低。这样的一个估计量被称之为最小方差，无偏（MVU）估计量。下一节，我们将会针对一个正态分布推导均值和方差估计量的数学表达式。我们将会看到，一个正态分布的方差MVU估计量在一些假设下需要除以<span
class="math inline">\(N\)</span>，而在另一些假设下需要除以<span
class="math inline">\(N-1\)</span>。</p>
<h3 id="最大似然估计">最大似然估计</h3>
<p>基于整体的一个子集，尽管有大量的获取一个参数估计量的技术，所有这些技术中最简单的可能就数最大似然估计了。</p>
<p>观察值<span
class="math inline">\(\vec{x}\)</span>的概率在公式(6)定义为<span
class="math inline">\(P(\vec{x};\mu,\sigma^2)\)</span>.
如果我们在此函数中固定<span class="math inline">\(x\)</span>和<span
class="math inline">\(\sigma^2\)</span>，当使<span
class="math inline">\(\vec{x}\)</span>变化时，我们就可以获得图(1)的正态分布。但是，我们也可以固定<span
class="math inline">\(\vec{x}\)</span>，使<span
class="math inline">\(\mu\)</span>和（或）<span
class="math inline">\(\sigma^2\)</span>变化。比如，我们可以选择类似前面例子中的<span
class="math inline">\(\vec{x}=(10,12,7,5,11)\)</span>。我们选择固定<span
class="math inline">\(\mu=10\)</span>，同时使<span
class="math inline">\(\sigma^2\)</span>变化。图(2)展示了当<span
class="math inline">\(x\)</span>和<span
class="math inline">\(\mu\)</span>固定时，<span
class="math inline">\(\sigma^2\)</span>对于这个分布取不同值的变化曲线：</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-06-20-110329.jpg" /></p>
<p>图 2. 此图表示了似然函数在特定观察数据<span
class="math inline">\(\vec{x}\)</span>，下固定<span
class="math inline">\(\mu=10\)</span>，<span
class="math inline">\(\sigma^2\)</span>变化曲线。</p>
<p>上图，我们通过固定<span
class="math inline">\(\mu=10\)</span>，令<span
class="math inline">\(\sigma^2\)</span>变化计算了<span
class="math inline">\(P(\vec{x};\sigma^2)\)</span>的似然函数。在结果曲线的每一个数据点代表了似然度，观察值<span
class="math inline">\(\vec{x}\)</span>是一个正态分布在参数<span
class="math inline">\(\sigma^2\)</span>下的样本。那么对应最大似然度的参数值最有可能是从我们定义的分布中产生数据的参数。因此，我们能通过找到似然度曲线的最大值决定最优的<span
class="math inline">\(\sigma^2\)</span>。在此例中，最大值在<span
class="math inline">\(\sigma^2=7.8\)</span>，这样标准差就是<span
class="math inline">\(\sqrt{(\sigma^2)=2.8}\)</span>。事实上，如果给定<span
class="math inline">\(\mu=10\)</span>，通过传统的方法计算，我们会发明方差就是7.8：</p>
<p><span
class="math display">\[\frac{(10-10)^2+(12-10)^2+(7-10)^2+(5-10)^2+(11-10)^2}{5}=7.8\]</span></p>
<p>因此，基于样本数据的方差计算公式只需要简单的通过找到最大的似然函数的最高点。此外，除了固定<span
class="math inline">\(\mu\)</span>，我们可以使<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma^2\)</span>同时变化。然后找到两个估计量对应在两个维度的似然函数的最大值。</p>
<p>要找一个函数的最大值，也很简单，只需要求导使其等于0。如果想找一个有两个变量函数的最大值，我们需要计算每个变量的偏导，再把两个偏导全部设置为0。接下来，设<span
class="math inline">\(\hat{\mu}_{ML}\)</span>为通过极大似然方法得到的总体均值的最优估计量，设<span
class="math inline">\(\hat{\sigma}^2_ML\)</span>为方差的最优估计量。要最大化似然函数，我们可以简单的计算它的(偏)导数，然后赋值为0，如下：</p>
<p><span class="math display">\[\begin{aligned} &amp;\hat{\mu}_{ML} =
\arg\max_\mu P(\vec{x}; \mu, \sigma^2)\\ &amp;\Rightarrow \frac{\partial
P(\vec{x}; \mu, \sigma^2)}{\partial \mu} = 0 \end{aligned}\]</span></p>
<p>及</p>
<p><span class="math display">\[\begin{aligned} &amp;\hat{\sigma}^2_{ML}
= \arg\max_{\sigma^2} P(\vec{x}; \mu, \sigma^2)\\ &amp;\Rightarrow
\frac{\partial P(\vec{x}; \mu, \sigma^2)}{\partial \sigma^2} = 0
\end{aligned}\]</span></p>
<p>下一节，我们将利用这个技术得到<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma^2\)</span>的MVU估计量。我们考虑两种情形：</p>
<p>第一种情形，我们假设分布的真正的均值<span
class="math inline">\(\hat{\mu}\)</span>是已知的，因此，我们只需要估计方差，那么问题就变成在参数为<span
class="math inline">\(\sigma^2\)</span>的一维的极大似然函数中对应找其最大值。这种情况不经常出现，但是，在实际应用中确实存在。例如，如果我们知道一个信号(比如：一幅图中一个像素的颜色值)本来应该有特定的值，但是，信号被白噪音污染了（均值为0的高斯噪音），这时分布的均值是已知的，我们只需要估计方差。</p>
<p>第二种情形就是处理均值和方差的真实值都不知道的情况。这种情况最常见，这时，我们需要基于样本数据估计均值和方差。</p>
<p>后面我们将看到，每种情形产生不同的MVU估计量。具体来说，第一种情形方差估计量需要除以<span
class="math inline">\(N\)</span>来标准化MVU。而第二种除的是<span
class="math inline">\(N-1\)</span>。</p>
<h3 id="均值已知的方差估计">均值已知的方差估计</h3>
<h4 id="参数估计">参数估计</h4>
<p>如果分布的均值真实值已知，那么似然函数只有一个参数<span
class="math inline">\(\sigma^2\)</span>。求最大似然估计量也就是解决：</p>
<p><span class="math display">\[\hat{\sigma^2}_{ML}=\arg\max_{\sigma^2}
P(\vec{x};\sigma^2).\tag{8}\]</span></p>
<p>但是，根据公式(6)的定义，如果计算<span
class="math inline">\(P(\vec{x};\sigma^2)\)</span>涉及到计算函数中指数的偏导。事实上，计算对数似然函数比计算似然函数本身的导数要简单的多。因为对数函数是单调递增函数，其最大值取值位置与原似然函数是一样的。因此我们用下面的式子替换：</p>
<p><span
class="math display">\[\hat{\sigma}^2_{ML}=\arg\max_{\sigma^2}\log(P(\vec{x};\sigma^2)).\tag{9}\]</span></p>
<p>下面，我令<span
class="math inline">\(s=\sigma^2\)</span>简化式子。我们通过计算公式(6)的对数的导数赋值为0来最大化对数似然函数：</p>
<p><span class="math display">\[\begin{aligned}&amp;\frac{\partial
\log(P(\vec{x};\sigma^2))}{\partial
\sigma^2}=0\\&amp;\Leftrightarrow\frac{\partial\log(P(\vec{x};s))}{\partial
s}=0\\&amp;\Leftrightarrow\frac{\partial}{\partial
s}\log\left(\frac{1}{(2\pi
s)^{\frac{N}{2}}}e^{-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2}
\right)=0\\&amp;\Leftrightarrow\frac{\partial}{\partial
s}\log\left(\frac{1}{(2\pi)^{\frac{N}{2}}}\right)+\frac{\partial}{\partial
s}\log\left(\frac{1}{\sqrt{s}^\frac{N}{2}}\right)+\frac{\partial}{\partial
s} \log\left(e^{-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2}\right
)=0\\&amp;\Leftrightarrow0+\frac{\partial}{\partial
s}\log\left((s)^{-\frac{N}{2}}\right)+\frac{\partial}{\partial
s}\left(-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2\right)=0\\&amp;\Leftrightarrow
-\frac{N}{2}\log (s)+\frac{1}{2
s^2}\sum_{i=1}^{N}(x_i-\mu)^2=0\\&amp;\Leftrightarrow
-\frac{N}{2s}+\frac{1}{2s^2}\sum_{i=1}^{N}(x_i-\mu)^2=0\\&amp;\Leftrightarrow
\frac{N}{2s^2}\left(-s+\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2\right)=0\\&amp;\Leftrightarrow\frac{N}{2s^2}\left(\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2-s\right)=0\end{aligned}\]</span></p>
<p>很明显，如果<span
class="math inline">\(N&gt;0\)</span>，那么上面等式唯一的解就是：</p>
<p><span
class="math display">\[s=\sigma^2=\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2.\tag{10}\]</span></p>
<p>注意到，实际上<span
class="math inline">\(\hat{\sigma}^2\)</span>的极大似然估计估计量就是传统上一般计算方差的公式。这里标准化因子是<span
class="math inline">\(\frac{1}{N}\)</span>.</p>
<p>但是，极大似然估计并不保证得出的是一个无偏估计量。另外，就算得到的估计量是无偏的，极大似然估计也不能保证估计是最小方差，即MVU。因此，我们需要检查公式(10)的的估计量是否是无偏的。</p>
<h4 id="表现评价">表现评价</h4>
<p>我们需要检查公式(7)的等式是否成立，来确定是否公式(10)中的估计量是无偏的。即判断：</p>
<p><span class="math display">\[E(s)=\hat{s}.\]</span></p>
<p>我们把公式(10)代入到<span
class="math inline">\(E(s)\)</span>，计算：</p>
<p><span class="math display">\[\begin{aligned}E[s] &amp;= E
\left[\frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2 \right] = \frac{1}{N}
\sum_{i=1}^N E \left[(x_i - \mu)^2 \right] = \frac{1}{N} \sum_{i=1}^N E
\left[x_i^2 - 2x_i \mu + \mu^2 \right]\\&amp;= \frac{1}{N} \left( N
E[x_i^2] -2N \mu E[x_i] + N \mu^2 \right)\\&amp;= \frac{1}{N} \left( N
E[x_i^2] -2N \mu^2 + N \mu^2 \right)\\&amp;= \frac{1}{N} \left( N
E[x_i^2] -N \mu^2 \right)\end{aligned}\]</span></p>
<p>另外，真实方差<span class="math inline">\(\hat{s}\)</span>有一个<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variance#Definition">非常重要的性质</a>为<span
class="math inline">\(\hat{s}=E[x_i^2]-E[x_i]^2\)</span>，可变换公式为<span
class="math inline">\(E[x_i^2]=\hat{s}+E[x_i]^2=\hat{s}+\mu^2\)</span>。使用此性质我们可能从上面的公式推出：</p>
<p><span class="math display">\[\begin{aligned}E[s]&amp;=\frac{1}{N}(N
E[x_i^2]-N\mu^2)\\&amp;=\frac{1}{N}(N\hat{s}+N\mu^2-N\mu^2)\\&amp;=\frac{1}{N}(N\hat{s})\\&amp;=\hat{s}\end{aligned}\]</span></p>
<p>满足了公式(7)的条件<span class="math inline">\(E[s]=\hat
s\)</span>，因此，我们得到的数据方差<span class="math inline">\(\hat
s\)</span>的统计量是无偏的。此外，因为极大似然估计的如果是一个无偏的估计量，那么也是最小方差(MVU)，也就是说，我们得到的估计量比任何一个其他的估计量都大。</p>
<p>因此，在分布真实均值已知的情况下，我们不用除以<span
class="math inline">\(N-1\)</span>，而是用除<span
class="math inline">\(N\)</span>计算正态分布的方差。</p>
<h3 id="均值未知的方差估计">均值未知的方差估计</h3>
<h4 id="参数估计-1">参数估计</h4>
<p>上一节，分布的真实均值已知，因此，我们只需要估计数据的方差。但是，如果真实的均值未知，我们均值的估计量就也需要计算了。</p>
<p>此外，方差的估计量需要使用均值的估计量。我们会看到，这时，之前我们得到的方差的估计量就不再无偏了。我们一会儿会通过除以N-1，而不是N来稍微的增加方差估计量的值，从而使方差估计无偏。</p>
<p>与之前一样，基于log似然函数，我们用极大似然估计计算两个估计量。首先我们先计算<span
class="math inline">\(\hat\mu\)</span>的极大似然估计量：</p>
<p><span class="math display">\[\begin{aligned}&amp;\frac{\partial
\log(P(\vec{x}; s, \mu))}{\partial \mu} = 0\\&amp;\Leftrightarrow
\frac{\partial}{\partial \mu} \log \left( \frac{1}{(2 \pi
s)^{\frac{N}{2}}} e^{-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2} \right) =
0\\&amp;\Leftrightarrow \frac{\partial}{\partial \mu} \log \left(
\frac{1}{(2 \pi)^{\frac{N}{2}}} \right) + \frac{\partial}{\partial \mu}
\log \left(e^{-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2} \right) =
0\\&amp;\Leftrightarrow \frac{\partial}{\partial \mu}
\left(-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2 \right) =
0\\&amp;\Leftrightarrow -\frac{1}{2s}\frac{\partial}{\partial \mu}
\left(\sum_{i=1}^N(x_i - \mu)^2 \right) = 0\\&amp;\Leftrightarrow
-\frac{1}{2s} \left(\sum_{i=1}^N -2(x_i - \mu) \right) =
0\\&amp;\Leftrightarrow \frac{1}{s} \left(\sum_{i=1}^N (x_i - \mu)
\right) = 0 \\&amp;\Leftrightarrow \frac{N}{s} \left( \frac{1}{N}
\sum_{i=1}^N (x_i) - \mu \right) = 0 \end{aligned}\]</span></p>
<p>显然，如果<span
class="math inline">\(N&gt;0\)</span>，那么上面的等式只有一种解：</p>
<p><span
class="math display">\[\mu=\frac{1}{N}\sum_{i=1}^{N}x_i.\tag{11}\]</span></p>
<p>注意到，实际的这是计算一个分布均值的著名公式。虽然我们知道这个公式，但我们现在证明了极大似然估计量估计了一个正态分布未知均值的真实值。现在我们先假定我们之前公式(10)计算的方差<span
class="math inline">\(\hat
s\)</span>的估计量仍然是MVU方差估计量。但下一节我们会证明这个估计量已经是有偏的了。</p>
<h4 id="表现评价-1">表现评价</h4>
<p>我们需要通过检查估计量<span
class="math inline">\(\mu\)</span>对真实<span class="math inline">\(\hat
\mu\)</span>的估计是否无偏来确定公式(7)的条件能否成立：</p>
<p><span
class="math display">\[E[\mu]=E\left[\frac{1}{N}\sum_{i=1}^{N}x_i\right]=\frac{1}{N}\sum_{i=1}^N
E[x_i]=\frac{1}{N}N E[x_i]=\frac{1}{N} N \hat\mu=\hat\mu.\]</span></p>
<p>既然<span
class="math inline">\(E[\mu]=\hat\mu\)</span>，那么也就是说我们对分布均值的估计量是无偏的。因为极大似然估计可以保证在估计是无偏的情况下得到的是最小方差估计量，所以我们就已经是证明了<span
class="math inline">\(\mu\)</span>是均值的MVU估计量。</p>
<p>现在我们检查基于经验均值<span
class="math inline">\(\mu\)</span>，而不是真实均值<span
class="math inline">\(\hat\mu\)</span>的方差估计量<span
class="math inline">\(s\)</span>对真实方差<span
class="math inline">\(\hat
s\)</span>的估计身上仍然是无偏的。我们只需要把得到的估计量<span
class="math inline">\(\mu\)</span>带入到之前在公式(10)推导出的公式：</p>
<p><span class="math display">\[\begin{aligned} s &amp;= \sigma^2 =
\frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\\&amp;=\frac{1}{N}\sum_{i=1}^N
\left(x_i - \frac{1}{N} \sum_{i=1}^N (x_i)
\right)^2\\&amp;=\frac{1}{N}\sum_{i=1}^N \left[x_i^2 - 2 x_i \frac{1}{N}
\sum_{i=1}^N (x_i) + \left(\frac{1}{N} \sum_{i=1}^N (x_i) \right)^2
\right]\\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \frac{2\sum_{i=1}^N x_i
\sum_{i=1}^N x_i}{N^2} + \left(\frac{\sum_{i=1}^N x_i}{N}
\right)^2\\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \frac{2\sum_{i=1}^N x_i
\sum_{i=1}^N x_i}{N^2} + \left(\frac{\sum_{i=1}^N x_i}{N}
\right)^2\\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \left(\frac{\sum_{i=1}^N
x_i}{N} \right)^2\end{aligned}\]</span></p>
<p>现在我们需要再次检查公式(7)的条件是否成立，来决定估计量是否无偏：</p>
<p><span class="math display">\[\begin{aligned} E[s]&amp;= E \left[
\frac{\sum_{i=1}^N x_i^2}{N} - \left(\frac{\sum_{i=1}^N x_i}{N}
\right)^2 \right ]\\&amp;= \frac{\sum_{i=1}^N E[x_i^2]}{N} -
\frac{E[(\sum_{i=1}^N x_i)^2]}{N^2} \end{aligned}\]</span></p>
<p>记得我们在之前用过方差一个非常重要的性质，真实方差<span
class="math inline">\(\hat s\)</span>可以写成<span
class="math inline">\(\hat s = E[x_i^2]-E[x_i]^2\)</span>，即，<span
class="math inline">\(E[x_i^2]=\hat s + E[x_i]^2=\hat s
+\mu^2\)</span>。利用这个性质我们可以推出：</p>
<p><span class="math display">\[\begin{aligned} E[s] &amp;=
\frac{\sum_{i=1}^N E[x_i^2]}{N} - \frac{E[(\sum_{i=1}^N
x_i)^2]}{N^2}\\&amp;= s + \mu^2 - \frac{E[(\sum_{i=1}^N
x_i)^2]}{N^2}\\&amp;= s + \mu^2 - \frac{E[\sum_{i=1}^N x_i^2 + \sum_i^N
\sum_{j\neq i}^N x_i x_j]}{N^2}\\&amp;= s + \mu^2 - \frac{E[N(s+\mu^2) +
\sum_i^N \sum_{j\neq i}^N x_i x_j]}{N^2}\\&amp;= s + \mu^2 -
\frac{N(s+\mu^2) + \sum_i^N \sum_{j\neq i}^N E[x_i] E[x_j]}{N^2}\\&amp;=
s + \mu^2 - \frac{N(s+\mu^2) + N(N-1)\mu^2}{N^2}\\&amp;= s + \mu^2 -
\frac{N(s+\mu^2) + N^2\mu^2 -N\mu^2}{N^2}\\&amp;= s + \mu^2 -
\frac{s+\mu^2 + N\mu^2 -\mu^2}{N}\\&amp;= s + \mu^2 - \frac{s}{N} -
\frac{\mu^2}{N} - \mu^2 + \frac{\mu^2}{N}\\&amp;= s -
\frac{s}{N}\\&amp;= s \left( 1 - \frac{1}{N} \right)\\&amp;= s
\left(\frac{N-1}{N} \right) \end{aligned}\]</span></p>
<p>显然<span class="math inline">\(E[s]\neq\hat
s\)</span>，上面公式可知分布的方差估计量不再是无偏的了。事实上，平均来看，这个估计量低估了真实方差，比例为<span
class="math inline">\(\frac{N-1}{N}\)</span>。当样本的数量趋于无穷时(<span
class="math inline">\(N\rightarrow\infty\)</span>)，这个偏差趋近于0。但是对于小的样本集，这个偏差就意义了，需要被消除。</p>
<h4 id="修正偏差">修正偏差</h4>
<p>因为偏差不过是一个因子，我们只需通过对公式(10)的估计量乘以偏差的倒数。这样我们就可以定义一个如下的无偏的估计量<span
class="math inline">\(s\prime\)</span>：</p>
<p><span class="math display">\[\begin{aligned} s\prime &amp;= \left (
\frac{N-1}{N} \right )^{-1} s\\s\prime &amp;= \left ( \frac{N-1}{N}
\right )^{-1} \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\\s\prime &amp;=\left
( \frac{N}{N-1} \right ) \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\\s\prime
&amp;= \frac{1}{N-1}\sum_{i=1}^N(x_i - \mu)^2\end{aligned}\]</span></p>
<p>这个估计量现在就是无偏的了，事实上，这个公式与传统计算方差的公式非常像，不同的是除的是<span
class="math inline">\(N-1\)</span>而不是<span
class="math inline">\(N\)</span>。然而，你可能注意到这个估计量不再是最小方差估计量，但是这个估计量是所有无偏估计量中最小方差的一个。如果我们除以<span
class="math inline">\(N\)</span>，那么估计量就是有偏的了，如果我们除以<span
class="math inline">\(N-1\)</span>，估计量就不是最小方差估计量。但大体来说，一个有偏的估计量要比一个稍高一点方差的估计量要糟糕的多。因此，如果当总体的均值是未知的情况下，方差除的是<span
class="math inline">\(N-1\)</span>，而不是<span
class="math inline">\(N\)</span>。</p>
<h3 id="总结">总结</h3>
<p>本文，我们推导了如果从分布数据中计算常见的方差和均值公式。此外，我们还证明了在方差估计中，标准化因子在总体均值已知时是<span
class="math inline">\(\frac{1}{N}\)</span>，在均值也需要估计时是<span
class="math inline">\(\frac{1}{N-1}\)</span>。</p>
<p><a
target="_blank" rel="noopener" href="http://cmb.oss-cn-qingdao.aliyuncs.com/%E6%A0%B7%E6%9C%AC%E6%96%B9%E5%B7%AE%E4%B8%BA%E4%BB%80%E4%B9%88%E9%99%A4%E4%BB%A5N-1%3F%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89.pdf">本文PDF</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/05/21/decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/21/decision-tree/" class="post-title-link" itemprop="url">Decision Tree (ID3)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-21 15:10:50" itemprop="dateCreated datePublished" datetime="2017-05-21T15:10:50+02:00">2017-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Alogorithm/" itemprop="url" rel="index"><span itemprop="name">Alogorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="preface">Preface</h2>
<p>In April 9th, 2017, incident occurred in United Airlines where crew
of UA beat up a passenger and dragged him out of the plane before which
was about to take off attracted attention all around the world. Many
would gave out doubt: why a company being so rude to passengers can
exist in this world? Actually, UA is going well is just because they
have an extremely precise emergency situation procedure which is
calculate by compute depending on big-data analysis. Computer can help
us make decisions though, it has no emotions, which is effective in most
cases, but can not be approved by our human beings. Let's take a look at
how algorithm make a decision: <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-United%20Airlines.png" />
It is a decision tree, which simply represents the procedure of how UA
algorithm make the decision. First of all, before taking off, four
employees of UA need fly from Chicago to Kentucky. Then the algorithm
check if there is any seats left, if so, passengers were safe for the
moment. But UA3411 was full, the algorithm began assessing the
importance of employees or passengers. Obviously, the algorithm think
crew is more important due to business consideration. Then how to choose
who should be evicted from the plane. The algorithm was more complicated
than the tree I drew, however, Asian or not was one of the criterion.
But why? Because Asian are pushovers. The passenger agreed at first,
however, when he heard that he had to wait for one day, he realized that
he could not treat his patient, then he refused. Then he was beat up and
dragged off the plane.</p>
<p>As you have seen, it is a decision tree, which is similar to human
decision-making process. Decision tree is a simple but powerful
algorithm in machine learning. In fact, you are often using decision
tree theory when making decision, for example <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-homework.png" /></p>
<h2 id="introduction">Introduction</h2>
<p>Decision tree is a classification and regression algorithm, we build
a tree through statistics. Today we only talk about how to classify
dataset using Decision Tree. First we will introduce some information
theory background knowledge, then we use iris data build a decision tree
using IDC3 algorithm.</p>
<h2 id="iris-data">Iris data</h2>
<p><a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Iris">Iris
dataset</a> is a very famous dataset deposited on UCI machine learning
repository, which described three kinds of iris. there are four columns
corresponding for features as followed： * sepal length in cm * sepal
width in cm * petal length in cm * petal width in cm</p>
<p>The last column represents iris categories:</p>
<ul>
<li>Iris-setosa (n=50)</li>
<li>Iris-versicolor (n=50)</li>
<li>Iris-virginica (n=50)</li>
</ul>
<p>Here, our task is to use the dataset to train a model and generate a
decision tree. During the process we need calculate some statistics
values to decide how to generate a better one.</p>
<p>The dataset is very small so that you can easily download it and take
a look.</p>
<h2 id="entropy-and-information-gain">Entropy and Information Gain</h2>
<h4 id="entropy">Entropy</h4>
<p>Before Decision Tree, I'd like to talk about some concept in
Information Theory. Entropy is a concept from thermodynamics at first,
C.E.Shannon introduced which into information theory which represent
redundancy in 1948. It sounds a very strange concept. In fact, it is
very easy to understand. For example, during the knockout stages in
world Cup Games, there are 16 teams. Now I let you guess which team will
win the champion which assume I know the answer, how many times do you
need to get the outcome? First of all, you cut 16 teams to 8-8 parts,
you asked me if the team in first 8 teams or the other. I told you that
the team was in the other 8 teams. Then you cut the the 8 teams again,
you ask me if the team is in the first 4 teams or the other, I told you
that the champion would be in the first 4 teams, and so forth and so on.
And how many times is the entropy of who wining the champion.</p>
<p><span class="math display">\[ Entropy(champion) = {\rm log}_2^{16}=4
\]</span></p>
<p>That is, we can use 4 bits to represents which team will win the
game. Clever you may ask why we divide team to two parts other than
three or four parts. That is because we use binary represents the world
in computer world. $ 2^4=16 $ means we can use 4 bit represents 16
conditions. We can use entropy represent all information in this world.
And if you have known that which team will win the campion, the entropy
is 0, because, you do not need any more information to deduce the
outcome.</p>
<p>Entropy represents uncertainty indeed. Ancient China, we have to
record history on bamboo slips, which demanded us decrease words. That
means entropy of every single ancient Chinese character is higher than
words we are saying today. That is, if we lost just some of these words,
we would lose lots of stories. There are many songs starts with:"Yoo,
yoo, check now", which barely offer us information, which means we can
drop those words and interpret the these songs precisely as well. The
entropy of these sentence is low.</p>
<p>Assume <span class="math inline">\(X\)</span> is discrete random
variable, the distribution is: <span
class="math display">\[P(X=x_i)=p_i\]</span> then the entropy of X is:
<span class="math display">\[H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2
p_i\]</span> where if p_0=0, we define 0log0 = 0.</p>
<p>It seems that the equation has nothing to do with the entropy we have
calculated in the champion example. Now let's calculate the example.
First of all <span class="math inline">\(X\)</span> represents the
probability of each team which would win the game. we assume all teams
were at the same level, so we have <span
class="math display">\[p(X=x_1)=p(X=x_2)=p(X=x_3)=\cdots =
p(X=x_{16})=\frac{1}{16}\]</span> the entropy is <span
class="math display">\[H(X)=-\sum_{i=1}^{16}\frac{1}{16}{\rm log}_2
\frac{1}{16}=-16\times\frac{1}{16}\times {\rm log}_2
{2^{-4}}=4\]</span></p>
<p>Bingo, the the answer is same. In fact, if we know some more
information, the entropy is lower than 4. for example, the probability
of Germany is higher than some Asian teams. #### Entropy and Iris Data
Now we calculate entropy of Iris Data which will be used to fit a
decision tree in following sections. We concern about the
categories(setosa, versicolor and virginica). Remember the equation of
how to calculate entropy: <span
class="math display">\[H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2
p_i\]</span></p>
<p>Three kinds of flowers are all 50s, so the probability of each
category is the same: <span
class="math display">\[p_1=p_2=p_3=\frac{50}{50+50+50}=\frac{1}{3}\]</span>
Then, the entropy is pretty easy to calculate <span
class="math display">\[H(X)=-1\times (\frac{1}{3}{\rm
log}_2\frac{1}{3}+\frac{1}{3}{\rm log}_2\frac{1}{3}+\frac{1}{3}{\rm
log}_2\frac{1}{3})=1.5850\]</span> #### Conditional Entropy The meaning
of Conditional Entropy is as its name. With respect with random
variable<span class="math inline">\((X, Y)\)</span>, the joint
distribution is <span class="math display">\[P(X=x_i, Y=y_j)=p_{ij},
i=1,2,3\cdots m; j=1,2,3,\cdots n\]</span> Conditional Entropy H(Y|X)
represents that given we have known random variable <span
class="math inline">\(X\)</span> , the disorder or uncertainty of <span
class="math inline">\(Y\)</span>. The definition is as followed: <span
class="math display">\[H(Y|X)=\sum_{i=1}^m p_i H(Y|X=x_i)\]</span> Here,
<span class="math inline">\(p_i=P(X=x_i)\)</span>.</p>
<h4 id="conditional-entropy-and-iris-data">Conditional Entropy and Iris
Data</h4>
<p>We calculate some Conditional Entropy as examples. First of all, I
random choose 15 columns of sepal length with respect to their
categories. the result is as followed：</p>
<table>
<thead>
<tr class="header">
<th>No.</th>
<th>sepal length in cm</th>
<th>categories</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>2</td>
<td>7.20</td>
<td>Iris-virginica</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5.00</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>4</td>
<td>5.00</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>5</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.70</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>7</td>
<td>5.20</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.50</td>
<td>Iris-versicolor</td>
</tr>
<tr class="odd">
<td>9</td>
<td>4.80</td>
<td>Iris-setosa</td>
</tr>
<tr class="even">
<td>10</td>
<td>4.60</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>11</td>
<td>6.50</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>12</td>
<td>5.20</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>13</td>
<td>7.70</td>
<td>Iris-virginica</td>
</tr>
<tr class="even">
<td>14</td>
<td>6.40</td>
<td>Iris-virginica</td>
</tr>
<tr class="odd">
<td>15</td>
<td>6.00</td>
<td>Iris-versicolor</td>
</tr>
</tbody>
</table>
<p>The octave code is <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%% octave</span><br><span class="line">[a,b,c,d, cate] = textread(&quot;iris.data&quot;, &quot;%f%f%f%f%s&quot;,&quot;delimiter&quot;, &quot;,&quot;);</span><br><span class="line"></span><br><span class="line">for i=1:15</span><br><span class="line">  x = floor(rand()*150);</span><br><span class="line">  fprintf(&#x27;%f %s\n&#x27;, a(x), cate&#123;x&#125; );</span><br><span class="line">end</span><br></pre></td></tr></table></figure> We just take this 15 items for
examples, I assume that we divide sepal length into two parts: greater
than mean and less than mean. The mean is <span
class="math display">\[mean = (5.90+7.2+\cdots+6.00)/15 =
5.7733\]</span> There are 8 elements less then 5.7733 and 7 bigger ones.
That is</p>
<table>
<thead>
<tr class="header">
<th>mean</th>
<th>idx of greater than mean</th>
<th>idx of less than mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5.7733</td>
<td>1,2,5,11,13,14,15</td>
<td>3,4,6,7,8,9,10,12</td>
</tr>
</tbody>
</table>
<p>We let <span
class="math inline">\(x_1=greater\)</span>(1,2,5,11,13,14,15), <span
class="math inline">\(x_2=less\)</span>(3,4,6,7,8,9,10,12) then <span
class="math display">\[H(Y|X=x_1)=-(p_1 {\rm log}_2 p_1 + p_2 {\rm
log}_2 p_2 + p_3 {\rm log}_2 p_3)=\frac{4}{7}{\rm
log}_2\frac{4}{7}+\frac{3}{7}{\rm log}_2\frac{3}{7}+0{\rm log}_2
0=0.98523\]</span> <span class="math display">\[H(Y|X=x_2)=-(p_1 {\rm
log}_2 p_1 + p_2 {\rm log}_2 p_2+p_3 {\rm log}_2 p_3)=\frac{3}{8}{\rm
log}_2\frac{3}{8}+0{\rm log}_2 0+\frac{5}{8}{\rm
log}_2\frac{5}{8}=0.95443\]</span></p>
<p>The Conditional Entropy then is <span
class="math display">\[H(Y|X)=\sum_{i=1}^{2}p_i
H(Y|x_i)=\frac{7}{15}\times 0.98523+\frac{8}{15}\times
0.95443=0.96880\]</span> #### Information Gain Just as its name implies,
Information Gain means the information we have gained after adding some
features. That is, we can vanish some uncertainty when we add some
information. For example, I want you to guess an NBA player, the
uncertainty is very high, however, there are only several persons in the
list if I tell you that he is a Chinese. You gained information after
knowing the Chinese feature to decrease the uncertainty. The calculation
of Information Gain is <span class="math display">\[IG(Y, X)=
H(Y)-H(Y|X)\]</span> Here, we want to decide <span
class="math inline">\(Y\)</span> with feature <span
class="math inline">\(X\)</span>. It is easy, just Entropy of <span
class="math inline">\(Y\)</span> minus Conditional Entropy <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(X\)</span>. The meaning is obvious too: <span
class="math inline">\(H(Y)\)</span> represents uncertainty, <span
class="math inline">\(H(Y|X)\)</span> represents uncertainty of <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(X\)</span>, the difference is the Information
Gain. #### Information Gain and Iris Data In this section, I will apply
Information Gain equations to the whole Iris data. First of all, let
<span class="math inline">\(Y\)</span> represent categories of iris, and
<span class="math inline">\(X_1,X_2,X_3, X_4\)</span> represent sepal
length, sepal width, petal length petal width respectively.</p>
<p>We have computed that <span
class="math inline">\(H(Y)=1.0986\)</span>, next, we will calculate 4
Conditional Entropy <span
class="math inline">\(H(Y|X_1),H(Y|X_2),H(Y|X_3),H(Y|X_4)\)</span>. In
light of continuousness of <span class="math inline">\(X\)</span>, we
divide them by mean of each feature. Then <span
class="math display">\[\overline{X_1}=5.8433,\,\overline{X_2}=3.0540,\,\overline{X_3}=3.7587,\,\overline{X_4}=1.1987\]</span></p>
<p><span class="math display">\[H(Y|X_1)=-\sum_{i=1}^3 p_i
H(Y|X_{1i})=-(\frac{70}{150}(\frac{0}{70}{\rm
log}_2\frac{0}{70}+\frac{26}{70}{\rm log}_2\frac{26}{70}
+\frac{44}{70}{\rm log}_2\frac{44}{70})+\frac{80}{150}(\frac{50}{80}{\rm
log}_2\frac{50}{80}+\frac{24}{80}{\rm
log}_2\frac{24}{80}+\frac{6}{80}{\rm
log}_2\frac{6}{80}))=1.09757\]</span></p>
<p><span class="math display">\[H(Y|X_2)=-\sum_{i=1}^3 p_i
H(Y|X_{2i})=-(\frac{67}{150}(\frac{42}{67}{\rm
log}_2\frac{42}{67}+\frac{8}{67}{\rm
log}_2\frac{8}{67}+\frac{17}{67}{\rm
log}_2\frac{17}{67}+\frac{83}{150}(\frac{8}{83}{\rm
log}_2\frac{8}{83}+\frac{42}{83}{\rm
log}_2\frac{42}{83}+\frac{33}{83}{\rm
log}_2\frac{33}{83}))=1.32433\]</span></p>
<p><span class="math display">\[H(Y|X_3)=-\sum_{i=1}^3 p_i
H(Y|X_{3i})=-(\frac{93}{150}(\frac{0}{93}{\rm
log}_2\frac{0}{93}+\frac{43}{93}{\rm
log}_2\frac{43}{93}+\frac{50}{93}{\rm
log}_2\frac{50}{93}+\frac{57}{150}(\frac{50}{57}{\rm
log}_2\frac{50}{57}+\frac{7}{57}{\rm log}_2\frac{7}{57}+\frac{0}{57}{\rm
log}_2\frac{0}{57}))=0.821667\]</span></p>
<p><span class="math display">\[H(Y|X_4)=-\sum_{i=1}^3 p_i
H(Y|X_{4i})=-(\frac{90}{150}(\frac{0}{90}{\rm
log}_2\frac{0}{90}+\frac{40}{90}{\rm
log}_2\frac{40}{90}+\frac{50}{90}{\rm
log}_2\frac{50}{90}+\frac{60}{150}(\frac{50}{60}{\rm
log}_2\frac{50}{60}+\frac{10}{60}{\rm
log}_2\frac{10}{60}+\frac{0}{60}{\rm log}_2\frac{0}{60}))=0.854655
\]</span> Information Gains is easy to get <span
class="math display">\[IG(Y,
X_1)=H(Y)-H(Y|X_1)=1.5850-1.09757=0.487427\]</span></p>
<p><span class="math display">\[IG(Y,
X_2)=H(Y)-H(Y|X_2)=1.5850-1.32433=0.260669\]</span></p>
<p><span class="math display">\[IG(Y,
X_3)=H(Y)-H(Y|X_3)=1.5850-0.821667=0.763333\]</span></p>
<p><span class="math display">\[IG(Y,
X_4)=H(Y)-H(Y|X_4)=1.5850-0.854655=0.730345\]</span> By now, we find
that <span class="math inline">\(IG(Y, X_3)\)</span> is bigger than
others, which means feature <span class="math inline">\(X_3\)</span>
supplies more information.</p>
<h2 id="id3iterative-dichotomiser-3">ID3(Iterative Dichotomiser 3)</h2>
<p>ID3 algorithm was developed by Ross Quinlan in 1986, which is a very
classic algorithm as well as C4.5 and CART. We First apply Information
Gain of each feature with respect to iris data. Then to choose the
maximum to divide data into 2 parts. For each part we apply Information
Gain recursively until we put all parents data to one node. Now that we
have know Information Gain from the last section, obviously we choose X3
as the feature dividing data into 2 parts in the first place.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-17-2.png" /></p>
<p>Let's take a look at the first cut using feature <span
class="math inline">\(X_3\)</span>. We have 150 items at first, after
comparing if <span class="math inline">\(X_3&gt;3.7587\)</span>, we
divide data into two parts, one has 93 items, the other got 57. From the
data, we know that there is no setosa in node B, meanwhile, no virginica
in node C, which means that this feature is very good for split data due
to exclude setosa and virginica.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Node B</th>
<th>Node C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>setosa</td>
<td>0</td>
<td>50</td>
</tr>
<tr class="even">
<td>versicolor</td>
<td>43</td>
<td>7</td>
</tr>
<tr class="odd">
<td>virginica</td>
<td>50</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The end condition of the algorithm is decided by IG. When IG is less
then some threshold or if there is only one category left, we can end
the algorithm. If IG less than some value(e.g. 0.01) and more than one
category left simultaneously, we have to choose a final category to be
the leaf, the rule is to set the category having samples more than the
others.</p>
<p>Take Node H for example, we set IG threshold to 0.01 in the first
place. Then we calculate the Information Gain for each feature, the
biggest IG from feature 2(sepal width in cm), which is 0.003204 and less
than 0.01. So we have to set H as a leaf. There are 0 Iris-setosa, 25
Iris-versicolor and 44 Iris-virginica in the leaf, so we set the bigger
one(i.e. Iris-virginica) to the leaf.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-19-2.png" /></p>
<h2 id="summary">Summary</h2>
<p>Today we have talked about what is decision tree algorithm. Firstly,
I introduce three background concept Entropy, Conditional Entropy and
Information Gain. Next we apply ID3 algorithm to Iris data to build a
decision.</p>
<p>One of the most significant advantages of decision tree is that we
can explain the result. If the algorithm decided UA should beat the
their passengers, they could trace the tree to find the path of reason
chain. It is very useful to tell consumers why we recommend them
something, under such circumstance, we can use decision tree to train a
model.</p>
<p>There is a shortcoming that Information Gain tends to use feature
with more values. In order to resolve the problem, Ross Quinlan improved
the algorithm through Information Gain Rate Rather than IG. <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Leo_Breiman">Breiman</a> introduced
CART algorithm subsequently, which can be applied to classification as
well as regression. Recently, Scientists have developed more powerful
algorithm such as Random Forest and Gradient Boosting Decision Tree
etc.</p>
<h2 id="reference">Reference</h2>
<ol type="1">
<li>《统计学习方法》，李航</li>
<li>《数学之美》，吴军</li>
<li>http://www.shogun-toolbox.org/static/notebook/current/DecisionTrees.html</li>
<li>https://en.wikipedia.org/</li>
</ol>
<h2 id="appendix-code">Appendix code</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%% octave main function file</span><br><span class="line">%% iris data dowload link: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</span><br><span class="line">[a,b,c,d, cate] = textread(&quot;iris.data&quot;, &quot;%f%f%f%f%s&quot;,&quot;delimiter&quot;, &quot;,&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%for i=1:15</span><br><span class="line">%	x = floor(rand()*150);</span><br><span class="line">%	fprintf(&#x27;%f %s\n&#x27;, a(x), cate&#123;x&#125; );</span><br><span class="line">%end;</span><br><span class="line"></span><br><span class="line">features = [a, b, c, d];</span><br><span class="line">for i=1:length(features(1, :))</span><br><span class="line">	col = features(:, i);</span><br><span class="line">	me = mean(col);</span><br><span class="line">	disp(me);</span><br><span class="line">	feat(i).greater = find(col &gt; me);</span><br><span class="line">	feat(i).less = find(col &lt;= me);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">total = (1:150)&#x27;;</span><br><span class="line">decision(feat, length(features(1, :)), cate, total);</span><br><span class="line">fprintf(&#x27;\n&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">%% octave: decsion tree file</span><br><span class="line">function decision(feat, feat_size, cate, total)</span><br><span class="line">	if length(total) == 0</span><br><span class="line">		return</span><br><span class="line">	end</span><br><span class="line">	fprintf(&#x27;(-%d-)&#x27;, length(total));</span><br><span class="line">	%plogp = @(x)[x*log2(x)];</span><br><span class="line">	function e = plogp(pi)</span><br><span class="line">		if pi == 0</span><br><span class="line">			e = 0;</span><br><span class="line">		else</span><br><span class="line">			e = pi*log2(pi);</span><br><span class="line">		end</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	function d = div(a, b)</span><br><span class="line">		if b == 0</span><br><span class="line">			d = 0;</span><br><span class="line">		else</span><br><span class="line">			d = a/b;</span><br><span class="line">		end</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	debug = 0;</span><br><span class="line"></span><br><span class="line">	function m = maxc(cate, cates, total)</span><br><span class="line">		maxidx = 1;</span><br><span class="line">		max_c = 0;</span><br><span class="line">		for i=1:length(cates)</span><br><span class="line">			c =find(strcmp(cate, cates&#123;i&#125;));</span><br><span class="line">			cl = length(intersect(c, total));</span><br><span class="line">			if debug == 1 fprintf(&#x27;\n%d##%d  %s###&#x27;,i, cl, char(cates&#123;i&#125;)) end</span><br><span class="line">			%if (debug == 1 &amp;&amp; cl &lt;10 &amp;&amp; cl &gt;0) disp(intersect(c, total)&#x27;) end</span><br><span class="line">			if cl &gt; max_c</span><br><span class="line">				max_c = cl;</span><br><span class="line">				maxidx = i;</span><br><span class="line">			end</span><br><span class="line">		end</span><br><span class="line">		if debug == 1 fprintf(&#x27;\n****%d    %d******\n&#x27;, maxidx, max_c) end</span><br><span class="line">		%m = cates(maxidx);</span><br><span class="line">		m = maxidx;</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	% compute h(y)</span><br><span class="line">	cates = unique(cate);</span><br><span class="line">	hx = 0;</span><br><span class="line">	for i = 1:length(cates)</span><br><span class="line">		c = find(strcmp(cate, cates&#123;i&#125;));</span><br><span class="line">		rc = intersect(c, total);</span><br><span class="line">		hx -= plogp(length(rc)/length(total));</span><br><span class="line">	end</span><br><span class="line">	%fprintf(&#x27;hx = %f\n&#x27;, hx)			</span><br><span class="line">	% compute h(y|x)</span><br><span class="line">	max_feature = 1;</span><br><span class="line">	max_ig = 0;</span><br><span class="line"></span><br><span class="line">	max_left = intersect(feat(1).greater, total);</span><br><span class="line">	max_right = intersect(feat(1).less, total);</span><br><span class="line">	for i=1:feat_size</span><br><span class="line">		hxh = 0;</span><br><span class="line">		hxl = 0;</span><br><span class="line">		feat_greater = intersect(feat(i).greater, total);</span><br><span class="line">		feat_less = intersect(feat(i).less, total);</span><br><span class="line">		ge = length(feat_greater);</span><br><span class="line">		le = length(feat_less);</span><br><span class="line"></span><br><span class="line">		if (ge+le) == 0</span><br><span class="line">			continue</span><br><span class="line">		end</span><br><span class="line"></span><br><span class="line">		for j = 1:length(cates);</span><br><span class="line">			c = find(strcmp(cate, cates&#123;j&#125;));</span><br><span class="line">			xh = length(intersect(feat_greater, c));</span><br><span class="line">			xl = length(intersect(feat_less, c));</span><br><span class="line">			hxh -= plogp(div(xh, ge));</span><br><span class="line">			hxl -= plogp(div(xl, le));</span><br><span class="line">		end</span><br><span class="line">		% compute hx - h(y|x)</span><br><span class="line">		hxy = (ge/(ge+le))*hxh + ((le)/(ge+le))*hxl;</span><br><span class="line">		ig = hx - hxy;</span><br><span class="line"></span><br><span class="line">		if ig &gt; max_ig</span><br><span class="line">			max_ig = ig;</span><br><span class="line">			max_feature = i;</span><br><span class="line">			max_left= feat_less;</span><br><span class="line">			max_right = feat_greater;</span><br><span class="line">		end</span><br><span class="line"></span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	left = max_left;</span><br><span class="line">	right = max_right;</span><br><span class="line">	%fprintf(&#x27;feature:ig  %d %f %d %d ------ \n&#x27;, max_feature, max_ig, length(left), length(right));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	if debug == 1 printf(&quot;\033[0;32;1m-ig--%f \033[0m&quot;,  max_ig); end</span><br><span class="line">	if(max_ig &lt; 0.01)</span><br><span class="line">		%fprintf(&#x27;&lt;%s&gt;&#x27;, char(maxc(cate, cates, total)))</span><br><span class="line">		printf(&quot;\033[0;31;1m&lt;%d&gt;\033[0m&quot;,  maxc(cate, cates, total));</span><br><span class="line">		return</span><br><span class="line">	end</span><br><span class="line">	fprintf(&quot;\033[0;34;1m#%d \033[0m&quot;,  max_feature);</span><br><span class="line">	fprintf(&#x27;&#123;&#x27; )</span><br><span class="line">	decision(feat, feat_size, cate, left);</span><br><span class="line">	decision(feat, feat_size, cate, right);</span><br><span class="line">	fprintf(&#x27;&#125;&#x27;)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/" class="post-title-link" itemprop="url">A Tutorial on Singular Value Decomposition</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-01 21:49:00" itemprop="dateCreated datePublished" datetime="2017-05-01T21:49:00+02:00">2017-05-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Alogorithm/" itemprop="url" rel="index"><span itemprop="name">Alogorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="preface">Preface</h3>
<p>Under some circumstance, we want to compress data to save storage
space. For example, when iPhone7 was released, many were trapped in a
dilemma: Should I buy a 32G iPhone without enough free space or that of
128G with a lot of storage being wasted? I had been trapped in such
dilemma indeed. I still remember that I only had 8G storage totally when
I was using my first Android phone. What annoyed me most was my
thousands of photos. Well, I confess that I was being always a mad
picture taker. I knew that there were some technique which could
compress a picture through reducing pixel. However, it is not enough,
because, as you know, in some arbitrary position in a picture, we can
tell that the picture share the same color. An extreme Example: if we
have a pure color picture, what we just need know is the RGB value and
the size, then reproducing the picture is done without extra effort.
What I was dreaming is done perfectly by Singular Value
Decomposition(SVD).</p>
<h3 id="introduction">Introduction</h3>
<p>Before SVD, in this article, I will introduce some mathmatical
concepts in the first place which cover Linear transformation and
EigenVector&amp;EigenValue. This Background knowledge is meant to make
SVD straightforward. You can skip if you are familar with this
knowledge.</p>
<h3 id="linear-transformation">Linear transformation</h3>
<p>Given a matrice <span class="math inline">\(A\)</span> and vector
<span class="math inline">\(\vec{x}\)</span>, we want to compute the
mulplication of <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span></p>
<p><span
class="math display">\[\vec{x}=\begin{pmatrix}1\\3\end{pmatrix}\qquad
A=\begin{pmatrix}2 &amp; 1 \\\\ -1 &amp; 1
\end{pmatrix}\qquad\vec{y}=A\vec{x}\]</span></p>
<p>But when we do this multiplication, what happens? Acutually, when we
multiply <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span>, we are changing the coordinate
axes of the vector <span class="math inline">\(x\)</span> to another new
axes. Begin with a simpler example, we let</p>
<p><span class="math display">\[A=\begin{pmatrix}1 &amp; 0\\\\ 0
&amp;1\end{pmatrix}\]</span></p>
<p>then we have <span class="math display">\[A\vec{x}=\begin{pmatrix}1
&amp; 0\\\\ 0
&amp;1\end{pmatrix}\begin{pmatrix}1\\3\end{pmatrix}=\begin{pmatrix}1\\3\end{pmatrix}\]</span></p>
<p>You may have noticed that we can always get the same <span
class="math inline">\(\vec{x}\)</span> after left multiply by A. In this
case, we use coordinate axes <span
class="math inline">\(i=\begin{pmatrix}1 \\\\ 0\end{pmatrix}\)</span>
and <span class="math inline">\(j=\begin{pmatrix}0 \\\\
1\end{pmatrix}\)</span> as the figure below demonstrated. That is, if we
want to represent <span
class="math inline">\(\begin{pmatrix}1\\3\end{pmatrix}\)</span> under
the coordination, we can calculate the transformation as followed:</p>
<p><span class="math display">\[\begin{align} A\vec{x}=1\cdot i + 3\cdot
j = 1\cdot \begin{pmatrix}1 \\\\ 0\end{pmatrix} + 3\cdot
\begin{pmatrix}0 \\\\
1\end{pmatrix}=\begin{pmatrix}1\\3\end{pmatrix}\end{align}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-073041.jpg" /></p>
<p>As we know, we can put a vector to anywhere in space, and if we want
to calculate sum of two vectors, the simplest way is to connect the to
vector from one's head to the other's tail. Our example, we compute
<span class="math inline">\(A\vec{x}\)</span> means add two vector(green
imaginary lines) up. And the answer is still <span
class="math inline">\(\begin{pmatrix}1\\3\end{pmatrix}\)</span>.</p>
<p>Now we change <span class="math inline">\(i=\begin{pmatrix}2\\\\
-1\end{pmatrix}\)</span> and <span
class="math inline">\(j=\begin{pmatrix}1\\1\end{pmatrix}\)</span> as the
coordinate axes(the red vectors), which means <span
class="math inline">\(A=\begin{pmatrix}2 &amp; 1 \\\\ -1 &amp;
1\end{pmatrix}\)</span>. I put vectors(black ones) to this figure as
well. We can see what happens when we change a new coordinate axes.</p>
<p>First of all, we multiply <span class="math inline">\(j\)</span> by
<span class="math inline">\(3\)</span> and <span
class="math inline">\(i\)</span> by 1. Then we move vector j and let the
head of <span class="math inline">\(i\)</span> connect the tail of <span
class="math inline">\(3\cdot j\)</span>. We can now find what is the
coordination of <span class="math inline">\(1\cdot i+3\cdot
j\)</span>(the blue one). We now verify the result using mutiplication
of <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span>:</p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}2 &amp; 1 \\\\
-1 &amp; 1\end{pmatrix}\begin{pmatrix}1\\3\end{pmatrix}=1\cdot
\begin{pmatrix}2 \\\\ -1\end{pmatrix} + 3\cdot  \begin{pmatrix}1 \\\\
1\end{pmatrix}=\begin{pmatrix}5\\2\end{pmatrix}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-013710.jpg" />
Here, you can imagine that matrice <span
class="math inline">\(A\)</span> is just like a function <span
class="math inline">\(f(x)\rightarrow y\)</span>, when you subsitute
<span class="math inline">\(x\)</span>, we get the exact <span
class="math inline">\(y\)</span> using the principle <span
class="math inline">\(f(x)\rightarrow y\)</span>. In fact, the
multiplication is tranform the vector from one coordination to
another.</p>
<h4 id="exercise">Exercise</h4>
<ol type="1">
<li><span class="math inline">\(A=\begin{pmatrix}1 &amp; 2 \\\\ 3 &amp;
4\end{pmatrix}\)</span>, draw the picture to stretch and rotate <span
class="math inline">\(x=\begin{pmatrix}1\\3\end{pmatrix}\)</span>.</li>
<li>Find a <span class="math inline">\(A\)</span> matrix to rotate <span
class="math inline">\(\vec{x}=\begin{pmatrix}1\\3\end{pmatrix}\)</span>
to <span class="math inline">\(90^{\circ}\)</span> and <span
class="math inline">\(180^{\circ}\)</span>.</li>
<li>what if <span class="math inline">\(A=\begin{pmatrix}1 &amp; 2 \\\\
2 &amp; 4\end{pmatrix}\)</span>.</li>
</ol>
<h3 id="eigenvector-and-eigenvalue">EigenVector and EigenValue</h3>
<p>EigenVector and EigenValue is an extremely important concept in
linear algebra, and is commonly used everywhere including SVD we are
talking today. However, many do not know how to interpret it. In fact,
EigenVector and EigenValue is very easy as long as we know about what is
linear transformation.</p>
<h4 id="a-problem">A Problem</h4>
<p>Before start, let's take a look at a question: if we want to multiply
matrices for 1000 times, how to calculate effectively? <span
class="math display">\[AAA\cdots A= \begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp; 2\end{pmatrix}\cdots
\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\]</span></p>
<h4 id="intuition">Intuition</h4>
<p>Last section, we have talked that if we multiply a vector by a matrix
<span class="math inline">\(A\)</span>, means that we use <span
class="math inline">\(A\)</span> to stretch and rotate the vector in
order to represent the vector in a new coordinate axes. However, there
are some vectors for <span class="math inline">\(A\)</span>, they can
only be stretched but can not be rotated. Assume <span
class="math inline">\(A=\begin{pmatrix}3 &amp; 1 \\\\ 0 &amp;
2\end{pmatrix}\)</span>, let <span
class="math inline">\(\vec{x}=\begin{pmatrix}1 \\\\
-1\end{pmatrix}\)</span>. When we multiply <span
class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span></p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}3 &amp; 1 \\\\ 0
&amp; 2\end{pmatrix}\begin{pmatrix}1 \\\\
-1\end{pmatrix}=\begin{pmatrix}2 \\\\ -2\end{pmatrix}=2\cdot
\begin{pmatrix}1 \\\\ -1\end{pmatrix}\]</span></p>
<p>It turns out we can choose any vector along <span
class="math inline">\(\vec{x}\)</span>, the outcome is the same, for
example:</p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}3 &amp; 1 \\\\ 0
&amp; 2\end{pmatrix}\begin{pmatrix}-3 \\\\
3\end{pmatrix}=\begin{pmatrix}-6 \\\\ -6\end{pmatrix}=2\cdot
\begin{pmatrix}-3 \\\\ 3\end{pmatrix}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-052135.jpg" /></p>
<p>We name vectors like <span class="math inline">\(\begin{pmatrix}-3
\\\\ 3\end{pmatrix}\)</span> and <span
class="math inline">\(\begin{pmatrix}1 \\\\ -1\end{pmatrix}\)</span>
<strong>EigenVectors</strong> and 2 the conresponse
<strong>EigenValues</strong>. In practice, we usually choose unit
eigenvectors(length equals to 1) given that there are innumerable
EigenVectors along the line.</p>
<p>I won't cover how to compute these vectors and vaules and just list
the answer as followed</p>
<p><span class="math display">\[\begin{align}&amp;\begin{pmatrix}3 &amp;
1 \\0&amp; 2\end{pmatrix}
\begin{pmatrix}{-1}/{\sqrt(2)} \\\\ {1}/{\sqrt(2)}\end{pmatrix}=
2\begin{pmatrix}{-1}/{\sqrt(2)} \\\\ {1}/{\sqrt(2)}\end{pmatrix}
\qquad\qquad\vec{x_1}=\begin{pmatrix}{-1}/{\sqrt(2)} \\\\
{1}/{\sqrt(2)}\end{pmatrix} &amp;\lambda_1=2\\
&amp;\begin{pmatrix}3 &amp; 1 \\0&amp; 2\end{pmatrix}
\begin{pmatrix}1 \\\\ 0\end{pmatrix}\qquad=\qquad
3\begin{pmatrix}1 \\\\
0\end{pmatrix}\qquad\qquad\quad\,\,\,\vec{x_2}=\begin{pmatrix}1 \\\\
0\end{pmatrix}
&amp;\lambda_2=3
\end{align}\]</span> Notice that <span
class="math inline">\(|\vec{x_1}|=1\)</span> and <span
class="math inline">\(|\vec{x_2}|=1\)</span> #### EigenValue
Decomposition If we put two EigenVectors and corresponding EigenValues
together, we can get the following equation: <span
class="math display">\[AQ=\begin{pmatrix}3 &amp; 1 \\0&amp;
2\end{pmatrix}
\begin{pmatrix}
{-1}/{\sqrt(2)}&amp;1\\
{1}/{\sqrt(2)}&amp;0
\end{pmatrix}=
\begin{pmatrix}
{-1}/{\sqrt(2)}&amp;1 \\\\ {1}/{\sqrt(2)}&amp;0
\end{pmatrix}
\begin{pmatrix}
2 &amp; 0\\
0 &amp; 3
\end{pmatrix}=Q\Lambda
\]</span> Then we have <span class="math inline">\(AQ=Q\Lambda\)</span>,
the conclusion is still right if we introduce more dimensions, that is
<span class="math display">\[\begin{align}
A\vec{x_1}=\lambda\vec{x_1}\\
A\vec{x_2}=\lambda\vec{x_2}\\
\vdots\qquad\\
A\vec{x_k}=\lambda\vec{x_k}
\end{align}\]</span></p>
<p><span class="math display">\[Q=
\begin{pmatrix}
    x_{11}&amp; x_{21} &amp;\cdots x_{k1}&amp;\\
    x_{12}&amp; x_{22} &amp;\cdots x_{k2}&amp;\\
    &amp;\vdots&amp;&amp;\\
    x_{1m}&amp; x_{22} &amp;\cdots x_{km}&amp;
\end{pmatrix}
\qquad\Lambda=
\begin{pmatrix}
\lambda_1 &amp; 0 &amp; \cdots&amp;0\\
0 &amp;\lambda_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots\\
0&amp;\cdots&amp;\cdots&amp;\lambda_k
\end{pmatrix}\]</span></p>
<p>If we do something on the equation <span
class="math inline">\(AQ=Q\Lambda\)</span>, then we have: <span
class="math display">\[AQQ^{-1}=A=Q\Lambda Q^{-1}\]</span> It is
EigenVaule Decomposition. #### Resolution Now, Let's look at the
question in the beginning of this section <span
class="math display">\[\begin{align}
AAA\cdots A&amp;= \begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp; 2\end{pmatrix}\cdots
\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\\
AAA\cdots A &amp;= Q\Lambda Q^{-1}Q\Lambda Q^{-1}Q\Lambda Q^{-1}\cdots
Q\Lambda Q^{-1}=Q\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}Q^{-1}\\
AAA\cdots A &amp;=Q\Lambda\Lambda\cdots \Lambda Q^{-1}=
Q\begin{pmatrix}2^{1000} &amp; 0 \\\\0 &amp; 3^{1000}\end{pmatrix}Q^{-1}
\end{align}\]</span> The calculation is extremely simple using EVD.</p>
<h4 id="exercise-1">Exercise</h4>
<ol type="1">
<li>Research how to compute EigenVectors and EigenValues, then
compute<span class="math inline">\(\begin{pmatrix}1 &amp; 2 &amp; 3\\4
&amp; 5 &amp;6\\7 &amp; 8 &amp; 9\end{pmatrix}\)</span>.</li>
<li>Think about the decisive factor affects how many EigenValues we can
get.</li>
</ol>
<h3 id="singular-value-decompositon">Singular Value Decompositon</h3>
<p>Notice that EigenVector Decomposition is applied to decompose square
matrices. Is there any approach to decompose non-square matrices? The
answer is a YES, and the name is Singular Value Decompositon.</p>
<h4 id="intuition-1">Intuition</h4>
<p>First of all, let's take a look at what SVD looks like <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-rectangle1.png" />
From the picture, we can find that matrice <span
class="math inline">\(A\)</span> is decomposed to 3 components: <span
class="math inline">\(U\)</span>, <span
class="math inline">\(\Sigma\)</span> and <span
class="math inline">\(V^{T}\)</span>. <span
class="math inline">\(U\)</span> and<span
class="math inline">\(V^T\)</span> are both sqaure matrices and <span
class="math inline">\(\Sigma\)</span> has the same size as <span
class="math inline">\(A\)</span>. Still, I want to emphasize that <span
class="math inline">\(U\)</span> and<span
class="math inline">\(V^T\)</span> are both unitary matrix, which means
the Determinant of <span class="math inline">\(U\)</span> and <span
class="math inline">\(V^T\)</span> is 1 and <span
class="math inline">\(U^T=U^{-1}\quad V^T=V^{-1}\)</span>.</p>
<h4 id="deduction">Deduction</h4>
<p>In the Linear Transformation section, we can transform a vector to
another coordinate axes. Assume you have a non-square matrice, and you
want to transform A from vectors <span
class="math inline">\(V=(\vec{v_1},
\vec{v_2},\cdots,\vec{v_n})^T\)</span> to antoher coordinate axes which
is <span class="math inline">\(U=(\vec{u_1},
\vec{u_2},\cdots,\vec{u_n})^T\)</span>, the thing is, <span
class="math inline">\(\vec{v_i}\)</span> and <span
class="math inline">\(\vec{u_i}\)</span> have unit length, and all
directions are perpendicular, that is, each of <span
class="math inline">\(\vec{v_i}\)</span> are at right angles to other
<span class="math inline">\(\vec{v_j}\)</span>, we name such matrices as
orthogonal matrices. In addition, I need add a factor <span
class="math inline">\(\Sigma=(\sigma_1,\sigma_2,
\sigma_3,\cdots,\sigma_n)\)</span> which represent the times of each
direction of <span class="math inline">\(\vec{u_i}\)</span>, i.e., We
need transform A from <span class="math inline">\(V=(\vec{v_1},
\vec{v_2},\cdots,\vec{v_n})^T\)</span> to <span
class="math inline">\((\sigma_1 \vec{u_1},\sigma_2 \vec{u_2}, \sigma_3
\vec{u_3},...\sigma_n \vec{u_n})^T\)</span>. From the picture below we
can find that we want to transform from the circle coordinate axes to
the ellipse axes. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-circle.png" />
<span class="math display">\[\begin{align}
\vec{v_1} \vec{v_2} \vec{v_3},...\vec{v_n} \qquad\rightarrow \qquad
&amp;\vec{u_1},\vec{u_2},\vec{u_3},...\vec{u_n}\\
&amp;\sigma_1,\sigma_2, \sigma_3,...\sigma_n
\end{align}\]</span></p>
<p>Recall that we can transform <span class="math inline">\(A\)</span>
at every direction, then generate another direction as new coordinate
direction. So we have <span class="math display">\[ A \vec{v_1}=\sigma_1
\vec{u_1}\\
A \vec{v_2}=\sigma_2 \vec{u_2}\\
\vdots\\
A \vec{v_j}=\sigma_j \vec{u_j}\]</span></p>
<p><span class="math display">\[\begin{align}
&amp;\begin{pmatrix}\\\\A\\\\\end{pmatrix}\begin{pmatrix}\\\\
\vec{v_1},\vec{v_2},\cdots,\vec{v_n}\\\\\end{pmatrix}=\begin{pmatrix}\\\\
\vec{u_1}, \vec{u_2},\cdots,\vec{u_n}\\\\ \end{pmatrix}\begin{pmatrix}
\sigma_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_n
\end{pmatrix}\\
&amp;C^{m\times n}\qquad\quad C^{n\times n}\qquad\qquad\qquad C^{m\times
n}\qquad \qquad \qquad C^{n\times n}
\end{align}\]</span> Which is <span class="math display">\[A_{m\times
n}V_{n\times n} = \hat{U}_{m\times n}\hat{\Sigma}_{n\times
n}\]</span></p>
<p><span class="math display">\[\begin{align}
A_{m\times n}V_{n\times n} &amp;= \hat{U}_{m\times
n}\hat{\Sigma}_{n\times n}\\
(A_{m\times n}V_{n\times n}V_{n\times n}^{-1} &amp;= \hat{U}_{m\times
n}\hat{\Sigma}_{n\times n}V_{n\times n}^{-1}\\
A_{m\times n}&amp;=\hat{U}_{m\times n}\hat{\Sigma}_{n\times n}V_{n\times
n}^{-1}\\&amp;=\hat{U}_{m\times n}\hat{\Sigma}_{n\times n}V_{n\times
n}^{T}
\end{align}\]</span></p>
<p>We need do something to the equation in order to continue the
deduction. First we stretch matrice <span
class="math inline">\(\hat{\Sigma}\)</span> vertically to <span
class="math inline">\(m \times n\)</span> size. Then stretch <span
class="math inline">\(\hat{U}\)</span> horizonly to <span
class="math inline">\(m\times m\)</span>, we can set any value to the
right entries. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-27-RSVD.png" /></p>
<p>Due to the fact we need calculate <span
class="math inline">\(U^{-1}\)</span> and <span
class="math inline">\(V^{-1}\)</span>, the equation is adjusted to <span
class="math display">\[A_{m\times n} = U_{m\times m}\Sigma_{m\times
n}V^T_{n\times n}\]</span> For furture convenience, we need sort all
<span class="math inline">\(\sigma s\)</span>, which means: <span
class="math display">\[\sigma_1\geq\sigma_2\geq\sigma_3 \geq\cdots\geq
\sigma_m\]</span>. #### How to calculate <span
class="math inline">\(U\)</span>, <span
class="math inline">\(V^T\)</span> and <span
class="math inline">\(\Sigma\)</span> To Decompose matrice <span
class="math inline">\(A\)</span>, we need calculate <span
class="math inline">\(U\)</span>, <span
class="math inline">\(V^T\)</span> and <span
class="math inline">\(\Sigma\)</span>. Remember that <span
class="math inline">\(U^T = U^{-1}\)</span> and <span
class="math inline">\(V^T = V^{-1}\)</span>, we will use the property
next.</p>
<p><span class="math display">\[\begin{align}
A &amp;= U\Sigma V^T\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
AA^T&amp;=U\Sigma V^T(U\Sigma V^T)^T\\
&amp;=U\Sigma V^TV\Sigma^T U^T\\
&amp;=U\Sigma V^{-1}V\Sigma^T U^T\\
&amp;=U\Sigma I\Sigma^T U^T\\
&amp;=U\Sigma^2 U^T
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
(AA^T)U&amp;=(U\Sigma^2 U^T)U\\
&amp;=(U\Sigma^2 )U^{-1}U\\
&amp;=U\Sigma^2
\end{align}\]</span></p>
<hr />
<p><span class="math display">\[\begin{align}
A^TA
&amp;=(U\Sigma V^T)^TU\Sigma V^T\\
&amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T U^{-1}U\Sigma V^T\\
&amp;=V\Sigma^T I\Sigma V^T\\
&amp;=V\Sigma^2 V^T\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}(A^TA)V&amp;=(V\Sigma^2
V^T)V\\
&amp;=(V\Sigma^2)V^{-1}V\\
&amp;=V\Sigma^2
\end{align}\]</span></p>
<h3 id="image-compression">Image Compression</h3>
<p>Firstly, let's look at the process of compressing a picture, the left
picture is original grayscale image. On the right, under different
compress rate, we can see pictures after reproducing. Before compress,
the size of the picture is 1775K byte. Then the picture is almost the
same, when we compress which into 100K byte size, which means we can
save 90% storage space <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-29-image.gif" /></p>
<p>To compress a piture, you just decompose the matrice through SVD,
then instead of using the original <span
class="math inline">\(U_{m\times m}\)</span>, <span
class="math inline">\(\Sigma_{m\times n}\)</span> and <span
class="math inline">\(U_{n\times n}\)</span>, we shrink every matrice to
new size <span class="math inline">\(U_{m\times r}\)</span>, <span
class="math inline">\(\Sigma_{r\times r}\)</span> and <span
class="math inline">\(U_{r\times n}\)</span>. The final <span
class="math inline">\(size(R)\)</span> is still <span
class="math inline">\(m\times n\)</span>, but we abandon some entries
since these entries are not so important than these we have reserved.
<img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-30-rect.png" /></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%% octave: core code of svd compressions</span><br><span class="line">X = imread(filename);  </span><br><span class="line">[U S V] = svd(double(X));</span><br><span class="line">R = U(:,1:r)*S(1:r,1:r)*V(:,1:r)&#x27;;    </span><br></pre></td></tr></table></figure>
<h3 id="summary">Summary</h3>
<p>Today we have learned mathmatics backgroud on SVD, including linear
transformation and EigenVector&amp;EigenVaule. Before SVD, we first
talked about EigenValue Decomposition. Finally, Singular Vaule
Decomposition is very easy to be deduced. In the last section, we took
an example see how SVD be applied to image compression field.</p>
<p>Now, it comes to the topic how to save our storage of a 32G iPhone7,
the coclusion is obvious: using SVD compress image to shrink the size of
our photos.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>https://www.youtube.com/watch?v=EokL7E6o1AE</li>
<li>https://www.youtube.com/watch?v=cOUTpqlX-Xs</li>
<li>https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw</li>
<li>https://yhatt.github.io/marp/</li>
<li>https://itunes.apple.com/cn/itunes-u/linear-algebra/id354869137</li>
<li>http://www.ams.org/samplings/feature-column/fcarc-svd</li>
<li>https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/04/16/eigenvector-and-eigenvalue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/04/16/eigenvector-and-eigenvalue/" class="post-title-link" itemprop="url">怎样理解特征向量和特征值（翻译）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-04-16 21:03:38" itemprop="dateCreated datePublished" datetime="2017-04-16T21:03:38+02:00">2017-04-16</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文地址：<a
target="_blank" rel="noopener" href="http://math.stackexchange.com/a/23325">stackexchange</a></p>
<p>原文答案作者主页：<a
target="_blank" rel="noopener" href="http://math.stackexchange.com/users/742/arturo-magidin">Arturo
Magidin</a></p>
<h4 id="版权声明">版权声明</h4>
<p>本译文首发于我的个人博客chengmingbo.github.io, 版权属于原作者。 ####
简短的答案
特征向量可以让线性变换的理解变得简单。它们是沿着坐标轴（方向）的线性变换包括简单的伸/缩以及翻转；特征值提供的是这些线性变换影响因子。
如果你理解越多沿着坐标轴（方向）的线性变换行为，理解线性变换就变得越简单；所以你要做的是有足够多的线性无关的特征向量与单因素线性变换产生联系。</p>
<h4 id="长一点儿的答案">长一点儿的答案</h4>
<p>这个世界上有非常多的问题可以通过线性变换来建模，而特征向量提供了非常简单的解决方案。例如，考虑线性微分方程:
<span class="math display">\[\frac{\mathrm d x}{\mathrm d t} = ax +
by\]</span> <span class="math display">\[\frac{\mathrm d y}{\mathrm d t}
= cx + dy\]</span></p>
<p>可以找到很多描述此微分方程的系统，比如，两个物种数量的增长相互影响。具体来说，可能物种<span
class="math inline">\(x\)</span>是物种<span
class="math inline">\(y\)</span>的捕食者；周围越多的物种<span
class="math inline">\(x\)</span>，意味着越少的物种<span
class="math inline">\(y\)</span>可以得到繁衍壮大；问题是周围的物种<span
class="math inline">\(y\)</span>越少，那么对于物种<span
class="math inline">\(x\)</span>来说食物就会越少，所以物种<span
class="math inline">\(x\)</span>的繁衍就会越少；但是接下来因为物种<span
class="math inline">\(x\)</span>对物种<span
class="math inline">\(y\)</span>的生存压力降低，很快会导致<span
class="math inline">\(y\)</span>物种数量的增长；但是这就意味这物种<span
class="math inline">\(x\)</span>的食物变多了，所以物种xx的数量也跟着增长；如此这般，循环往复。特定的物理现象也能形成这样的系统，比如粒子在运动的流体中，粒子的速度矢量取决于其所处的流体中位置。</p>
<p>直接解决这种系统是非常复杂的。但是，假设如果你可以不用去关注变量<span
class="math inline">\(x\)</span>和变量<span
class="math inline">\(y\)</span>而是转而关注<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>（这里<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>与<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>线性相关，也就是说，<span
class="math inline">\(z=\alpha x + \beta y\)</span>, <span
class="math inline">\(\alpha\)</span>和<span
class="math inline">\(\beta\)</span>是常量，同时<span
class="math inline">\(w=\gamma x + \delta y\)</span>， <span
class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\delta\)</span>也是常量）。这样，我们的系统就变换成了如下的形式：
<span class="math display">\[\frac{\mathrm d z}{\mathrm d t} = \kappa
w\]</span> <span class="math display">\[\frac{\mathrm d w}{\mathrm d t}
= \lambda z\]</span></p>
<p>也就是说，你对系统做了<strong>解耦</strong>，这样你就可以单独的处理各个独立函数了。接下来就这个问题就变得非常简单：<span
class="math inline">\(z=Ae^{\kappa t}\)</span>，以及<span
class="math inline">\(w=Be^{\lambda t}\)</span>。下一步就是用<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>的公式，算出<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>。</p>
<p>这能做到么？事实上，这等于我们精确的找到了矩阵<span
class="math inline">\(\begin{pmatrix}a &amp; b\\
c&amp;d\end{pmatrix}\)</span>线性独立的两个特征向量！<span
class="math inline">\(z\)</span>和<span
class="math inline">\(w\)</span>是其特征向量，而<span
class="math inline">\(\kappa\)</span>和<span
class="math inline">\(\lambda\)</span>为相对应的特征值。通过使用一个表达式把<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span><strong>混合</strong>
起来，然后解耦成两个互相独立的函数，问题现在变得非常简单了。</p>
<p>这就是我们希望使用特征向量及特征值的本质：通过线性变换把问题<strong>解耦</strong>
成一系列沿着各个隔离<strong>方向</strong>的操作，使得各个方向问题都可独立解决。</p>
<p>大量的问题归根结底是解决<strong>线性独立操作</strong>，理解这些可以实实在在的帮助你理解矩阵/线性变换到底在做什么。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/04/05/pca-translation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/04/05/pca-translation/" class="post-title-link" itemprop="url">主成分分析（PCA）简明教程（翻译）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-04-05 21:58:33" itemprop="dateCreated datePublished" datetime="2017-04-05T21:58:33+02:00">2017-04-05</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>作者：Lindsay I Smith</p>
<p>时间：2002.2.26</p>
<p>译者：程明波</p>
<p><a target="_blank" rel="noopener" href="http://facepress.net/pdf/734.pdf">英文文章地址</a></p>
<p><a
target="_blank" rel="noopener" href="http://chengmingbo.githu.io/2017/04/05/pca-translation/">译文地址</a></p>
<h4 id="版权声明">版权声明</h4>
<p>本译文首发于我的个人博客chengmingbo.github.io, 版权属于原作者。</p>
<h3 id="第一章">第一章</h3>
<h4 id="前言">前言</h4>
<p>这是一篇帮助读者理解主成分分析（PCA）的教程。PCA是一种统计技术，在人脸识别和图像压缩等领域都有应用。同时，PCA也是一种高维数据模式发现的一种常用方法。</p>
<p>在讲PCA之前，本文先介绍了PCA用到的一些数学概念。其中包括标准差、协方差、特征向量和特征值等。这些背景知识意在帮助我们理解PCA部分，如果你对这些概念已经非常清晰可以跳过此部分。</p>
<p>示例贯穿于整个教程，以便于通过直观的例子讨论概念。如果你还想了解更多内容，霍华德.安东著有约翰威立国际出版公司出版的数学课本《Elementary
Linear Algebra 5e》提供了非常好的这方面数学背景知识。</p>
<h3 id="第二章">第二章</h3>
<h4 id="数学背景知识">数学背景知识</h4>
<p>本章试图介绍便于理解主成分分析计算过程所需的基本数学知识。各个主题互相独立，同时各主题会举例说明。理解为什么使用这些技术以及一些关于数据计算结果所告诉我们的比记住枯燥的数学原理更重要。尽管不是所有这些知识都应用于PCA，一些看起来不是直接相关知识是这些最重要技术的基石。</p>
<p>我安排其中的一节介绍统计学知识，主要着眼于分布的度量，或者说数据是怎样离散的，其他的部分主要讲矩阵代数的一些知识，包括特征值、特征向量以及一些PCA所需的重要矩阵性质。
#### 2.1 统计知识
整个统计学都基于你有一个很大数据集的前提，以及你想分析关于数据集中各个数据点的关系。我会介绍一些量度这些数据的一些方法，帮助你理解这些数据本身。</p>
<h5 id="标准差">2.1.1 标准差</h5>
<p>要了解标准差，我们需要一个数据集。统计学经常会使用总体中的一些采样。以选举为例，总体就是一个国家的所有人，因此一个采样就是统计学家用来度量的总体的一个子集。统计学伟大之处是我们只需度量（例如电话调查等）总体中的采样，你就可以计算出最接近所有整体的度量。</p>
<p>本节我假设我们的数据集是某个很大总体的采样。本节后面会提供总体以及采样的更多信息。这是一个示例数据集：
<span class="math display">\[X=[1\, 2\,4\, 6\, 12\, 15\, 25\, 45\, 68\,
67\, 65\, 98]\]</span> 我们简单地假设字符<span
class="math inline">\(X\)</span>表示包含这些所有数字的集合。如果我想表示这个集合中某个单独的数字，我会用<span
class="math inline">\(X\)</span>加上下标表示某个具体的数。例：<span
class="math inline">\(X_3\)</span>表示<span
class="math inline">\(X\)</span>集合中的第三个数，也就是数字<span
class="math inline">\(4\)</span>。注意，有些书用<span
class="math inline">\(X_0\)</span>表示第一个数字，我们这里用<span
class="math inline">\(X_1\)</span>。另外，我们用字符<span
class="math inline">\(n\)</span>表示集合中元素的数量。</p>
<p>我们可以计算一个集合的很多维度，比如，我们可以计算样本的均值。这里我假设读者明白什么是一个样本的均值。这里仅给出公式：
<span class="math display">\[\overline{X} =\frac{\sum_{i=1}^n
X_i}{n}\]</span> 注意，我们用字符<span
class="math inline">\(\overline{X}\)</span>标识集合的均值。这个公式表示：把所有的数字加起来再除以他们的数量。</p>
<p>很不幸，均值除了告诉我们某种中心以外，并没有告诉关于数据的更多信息。比如以下两个数据集合的均值(10)完全一样，但是显然它们区别很大。
<span class="math display">\[[0\, 8\, 12\, 20]\quad 和 \quad[8\, 9\,
11\, 12]\]</span></p>
<p>那么，这两个集合有何不同呢？这两个集合的离散程度是不同的。一个数据集的标准差（Standard
Deviation,
缩写SD）是衡量这个集合数据离散程度的一个指标。怎么计算呢？SD的定义是这样的：每个数据点到这份数据均值点的平均距离。计算每一个数据点到均值点的距离平方，然后相加，再除以<span
class="math inline">\(n-1\)</span>，再开方，公式如下： <span
class="math display">\[s=\sqrt{\frac{\sum_{i=1}^n
(X-\overline{X})^2}{(n-1)}}\]</span> 这里<span
class="math inline">\(s\)</span>常用来标识样本方差。可能有人会问：“为啥分母除以<span
class="math inline">\(n-1\)</span>而不是<span
class="math inline">\(n\)</span>呢？”
答案有点儿复杂，大体来说，如果你的数据集合是一个采样，比如，你取样于真实世界（比如调查500人的选举情况）得到一个子集，那么你就必须用<span
class="math inline">\(n-1\)</span>，因为这个结果比你用<span
class="math inline">\(n\)</span>更接近于你用全部的整体算出的标准差。但是，如果你不是计算一个样本的而是整体的标准差，这种情况下，你就应该除以<span
class="math inline">\(n\)</span>而不是<span
class="math inline">\(n-1\)</span>。如果想了解更多的关于标准差的内容，可以访问<a
target="_blank" rel="noopener" href="http://mathcentral.uregina.ca/RR/database/RR.09.95/weston2.html">这里</a>,链接文章用类似的方法讲了标准差，提供了不同分母计算的区别实验，同时还探讨了采样和总体的异同。</p>
<p>数据集1 <span class="math display">\[\begin{array}{lrr}
X &amp; (X-\overline{X}) &amp; (X-\overline{X})^2 \\
\hline
0 &amp; -10 &amp; 100\\
8 &amp; -2 &amp; 4\\
12 &amp; 2 &amp; 4\\
20 &amp; 10 &amp; 100\\
\hline
\bf{总计} &amp; &amp; 208\\
\hline
\bf{除以(n-1)} &amp; &amp; 69.333\\
\hline
\bf{平方根} &amp; &amp; 8.3266\\
\hline
\end{array}\]</span></p>
<p>数据集2 <span class="math display">\[\begin{array}{lrr}
X &amp; (X-\overline{X}) &amp; (X-\overline{X})^2 \\
\hline
8 &amp; -2 &amp; 4\\
9 &amp; -1 &amp; 1\\
11 &amp; 1 &amp; 1\\
12 &amp; 2 &amp; 4\\
\hline
\bf{总计} &amp; &amp; 10\\
\hline
\bf{除以(n-1)} &amp; &amp; 3.333\\
\hline
\bf{平方根} &amp; &amp; 1.8.257\\
\hline
\end{array}\]</span></p>
<p><span class="math display">\[\bf{表2.1 标准差计算}\]</span></p>
<p>从上表2.1，我们可以看到标准差的计算过程。</p>
<p>因此，和我们预想的一样，第一个数据的的标准差要比第二个大得多。原因是数据离散于均值点的程度更高。再举一个例子，数据集:
<span class="math display">\[[10\, 10\, 10\, 10]\]</span>
的均值也是10，但是它的标准差是0，
因为所有的数字是相同的。没有任何数据点偏离均值。</p>
<h5 id="方差">2.1.2 方差</h5>
<p>方差是数据离散程度的另一个度量。实际上，它和标准差几乎相同，公式如下：
<span class="math display">\[s^2=\frac{\sum_{i=1}^n
(X-\overline{X})^2}{(n-1)}\]</span>
你会注意到方差就是标准差的平方，标识上也有<span
class="math inline">\(s\)</span>(<span
class="math inline">\(s^2\)</span>)。<span
class="math inline">\(s^2\)</span>经常用来标识一个数据集的方差。方差和标准差都是用来衡量数据的离散程度。标准差使用的更普遍，方差也常使用。之所以介绍方差是因为下一节我们介绍的协方差是基于方差的。</p>
<h5 id="练习">练习</h5>
<p>计算下列数据集的均值、标准差和方差。</p>
<p>[12 23 34 44 59 70 98]</p>
<p>[12 15 25 27 32 88 99]</p>
<p>[15 35 78 82 90 95 97]</p>
<h5 id="协方差">2.1.3 协方差</h5>
<p>我们之前介绍的前两种度量方式只针对纯1维情况。1维数据集合可能是这种形式：屋里所有人的身高，或者上学期计算机科目101的成绩等等。但许多数据集是大于1维的情况，
对于这种数据集，统计分析的目标经常是分析不同的维度之间的关系。例如，我们可能有个数据集同时包含了课堂上学生的身高，以及他们论文的分数。接着我们就可以利用统计分析工具来观察学生的身高对他们的成绩是否有影响。</p>
<p>标准差和方差只是对单一维度的计算，因此，你只能对数据的每一个维度单独计算标准差。然而，如果有一种类似的度量能找到各维度相互在偏离均值的变化关系会非常有用。</p>
<p>协方差就是这样一种度量。协方差总是用来度量两个维度，如果你计算一个维度和他自己维度的协方差，这时协方差就退化为这一个维度的方差。因此，如果你有一个3维数据集<span
class="math inline">\((x,y,z)\)</span>,协方差的计算公式与方差非常相似。方差的计算公式也可以这样表示：
<span
class="math display">\[var(X)=\frac{\sum_{i=1}^n(X_i-\overline{X_i})(X_i-\overline{X_i})}{(n-1)}\]</span>
这里我简单的对平方项进行了展开。有了以上知识，我们现在可以写出协方差的公式了：
<span
class="math display">\[cov(X,Y)=\frac{\sum_{i=1}^n(X_i-\overline{X_i})(Y_i-\overline{Y_i})}{(n-1)}\]</span></p>
<p>除了第二个括号中的<span
class="math inline">\(X\)</span>全部被替换成了<span
class="math inline">\(Y\)</span>以外，协方差和方差的公式完全一样。我们可以这么表述：“对于每个数据项，把每个<span
class="math inline">\(x\)</span>和<span
class="math inline">\(x\)</span>均值的差与每个<span
class="math inline">\(y\)</span>和<span
class="math inline">\(y\)</span>均值的差相乘，再加和除以<span
class="math inline">\((n-1)\)</span>”。协方差是怎样的一种工作机理呢？我们这里用一些数据来举例。想象你通过调查得到一个2维数据。假设我们问了一堆学生他们花在科目COSC241的总小时数，以及他们的学期末成绩。现在我们有了两个维度，第一个维度是<span
class="math inline">\(H\)</span>，标识学习的小时数，第二个维度是M，标识学生的成绩。<strong>图2.2</strong>展示了我们假设的数据以及两个维度学习小时数和成绩之间的协方差<span
class="math inline">\(cov(H,M)\)</span>。</p>
<p>这张图告诉我们什么呢？协方差的值没有它的符号重要（正或负）。如果值是正的，比如我们这里，那么意味着两个维度一起增减。即，一般来说，如果学习的小时数增加，那么这个学生最后取得的成绩就会高。</p>
<p>但是如果协方差的值是负的，那么如果其中一个维度增加，另一个维度就会减少。如果我们刚刚计算的协方差的结果是负值。那我们的说法就变成了随着学习小时数的增加，期末成绩会降低。</p>
<p>最后一种情况，如果协方差是<span
class="math inline">\(0\)</span>，那么说明两个维度是相互独立的。</p>
<p>我们很容易画一张图如图2.1.3，得出结论：学习成绩随着学习的小时数增加而增加。但是，只有两维或三维这种低维的奢侈情况，我们才能通过可视化观察趋势。由于在一个数据集中可以计算任意两个维度的协方差，这种技术经常是高维数据可视化非常困难的情况下寻找维度之间关系的一种方法。</p>
<p>你可能会问，<span class="math inline">\(cov(X, Y)\)</span>与<span
class="math inline">\(cov(Y,X)\)</span>是否相等？简单一看我们就会发现，它们是完全相等的，因为两个式子计算的唯一不同是在<span
class="math inline">\(cov(Y,X)\)</span>中<span
class="math inline">\((X_i-\overline{X_i})(Y_i-\overline{Y_i})\)</span>被替换成了<span
class="math inline">\((Y_i-\overline{Y_i})(X_i-\overline{X_i})\)</span>。我们知道乘法满足交换率，也就是说，无论乘数和被乘数的位置怎么变化，结果都是一样，也就是说这两个协方差结果是相同的。</p>
<h5 id="section">2.1.4</h5>
<p>我们知道，协方差总是用来计算两个维度之间的关系。如果我们有一个超过2维的数据集合，那么我们要计算的协方差的值的数量就不止一个了。比如，一个三维的数据集（<span
class="math inline">\(x,y,z三个维度\)</span>)。你可以计算的协方差就有<span
class="math inline">\(cov(x,y)\)</span>、<span
class="math inline">\(cov(x,z)\)</span> 和 <span
class="math inline">\(cov(x,z)\)</span>。事实上，对于一个<span
class="math inline">\(n\)</span>维的数据集，你可以计算<span
class="math inline">\(\frac{n!}{(n-2)! * 2}\)</span>不同的值。</p>
<p>数据： <span class="math display">\[\begin{array}{lrr}
&amp;小时数(H)&amp;成绩(M)\\
\hline
数据 &amp; 9 &amp; 39\\
&amp; 15 &amp;56 \\
&amp; 25 &amp;93 \\
&amp; 14 &amp;61 \\
&amp; 10 &amp;50 \\
&amp; 18 &amp;75 \\
&amp; 0 &amp;32 \\
&amp; 16 &amp;85 \\
&amp; 5 &amp;42 \\
&amp; 19 &amp;70 \\
&amp; 16 &amp;66 \\
&amp; 20 &amp;80 \\
\hline
总数&amp;167&amp;749\\
\hline
平均 &amp; 13.92&amp; 62.42\\
\hline
\end{array}\]</span></p>
<p>协方差： <span class="math display">\[\begin{array}{cc|c|c|c}
H &amp; M &amp; (H_i - \overline{H}) &amp; (M_i-\overline{M}) &amp;
(H_i-\overline{H})(M_i-\overline{M})\\
\hline
9 &amp; 39 &amp; -4.92&amp; -23.42 &amp;115.23\\
15 &amp; 56 &amp; 1.08&amp; -6.42 &amp;-6.93\\
25 &amp; 93 &amp; 11.08&amp; -30.58 &amp;338.83\\
14 &amp; 61 &amp; 0.08&amp; -1.42 &amp;-0.11\\
10 &amp; 50 &amp; -3.92&amp; -12.42 &amp;48.69\\
18 &amp; 75 &amp; 4.08&amp; 12.58 &amp;51.33\\
0 &amp; 32 &amp; -13.92&amp; -30.42 &amp;423.45\\
16 &amp; 85 &amp; 2.08&amp; -22.58 &amp;46.97\\
5 &amp; 42 &amp; -8.92&amp; -20.42 &amp;182.15\\
19 &amp; 70 &amp; 5.08&amp; -7.58 &amp;38.51\\
16 &amp; 66 &amp; 2.08&amp; -3.58 &amp;7.45\\
20 &amp; 80 &amp; 6.08&amp; 17.58 &amp;106.89\\
\hline
总数 &amp; &amp; &amp; &amp; 1149.89\\
\hline
平均 &amp; &amp; &amp; &amp; 104.54\\
\end{array}\]</span></p>
<p>想求出所有不同维度的协方差，非常有用的方法是把他们全计算出来然后放入矩阵。我假设你对矩阵比较熟悉，以及矩阵怎样定义。因此，对于一个<span
class="math inline">\(n\)</span>维的数据集的协方差矩阵： <span
class="math display">\[C^{n\times n}=(c_{i,j}, c_{i,j}=cov(Dim_i,
Dim_j))\]</span>, 这里<span class="math inline">\(C^{n\times
n}\)</span>是一个<span class="math inline">\(n\)</span>行<span
class="math inline">\(n\)</span>列的矩阵，<span
class="math inline">\(Dim_x\)</span> 是第<span
class="math inline">\(x\)</span>维。上面非常不美观的公式说的是，如果你有一个<span
class="math inline">\(n\)</span>维数据集，那么协方差矩阵就是一个<span
class="math inline">\(n\)</span>行<span
class="math inline">\(n\)</span>列的矩阵，矩阵的每一个元素是两个维度之间的协方差计算结果。例如，矩阵的第2行第三列就是维度2和维度3之间的协方差计算结果。</p>
<p>一个例子。我们假设有一个3维的数据集，分别使用<span
class="math inline">\(x\)</span>,<span
class="math inline">\(y\)</span>,<span
class="math inline">\(z\)</span>表示3个维度。那么协方差矩阵是一个3行3列的矩阵，矩阵中的元素就是：
<span class="math display">\[\begin{pmatrix}
cov(x,x) &amp; cov(x,y) &amp; cov(x,z) \\
cov(y,x) &amp; cov(y,y) &amp; cov(y,z) \\
cov(z,x) &amp; cov(z,y) &amp; cov(z,z) \\
\end{pmatrix}\]</span></p>
<p>几个需要注意：主对角线计算的某一维和它自己的协方差，也就是这些维度的方差。剩下的元素，因为<span
class="math inline">\(cov(a,b)=cov(b,a)\)</span>，所以矩阵关于主对角线对称。</p>
<h5 id="练习-1">练习</h5>
<ol type="1">
<li>计算以下关于<span class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>的2维数据集的协方差，然后描述一下协方差结果可能推导出数据什么方面的结论。
<span class="math display">\[\begin{array}{c|c|c|c|c|c}
项目id &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\
\hline
x &amp; 10 &amp; 39 &amp; 19 &amp; 23 &amp; 28\\
y &amp; 43 &amp; 13 &amp; 32 &amp; 21 &amp; 20\\
\hline
\end{array}\]</span></li>
<li>计算下列3维数据的协方差矩阵： <span
class="math display">\[\begin{array}{c|c|c|c}
项目id &amp; 1 &amp; 2 &amp; 3 \\
\hline
x &amp; 1 &amp; -1 &amp; 4\\
y &amp; 2 &amp; 1 &amp; 3\\
z &amp; 1 &amp; 3 &amp; -1\\
\hline
\end{array}\]</span></li>
</ol>
<h4 id="矩阵代数">2.2 矩阵代数</h4>
<p>本节会介绍PCA所用到的一些矩阵代数的背景知识，我将重点介绍对给定矩阵计算特征向量和特征值的相关知识。这里我假设你了解矩阵的基本知识。
<span class="math display">\[\begin{align}\begin{pmatrix}
2 &amp; 3\\
2 &amp; 1\\
\end{pmatrix}\times
\begin{pmatrix}
1\\\\
3
\end{pmatrix}=
\begin{pmatrix}
11\\\\
5
\end{pmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}\begin{pmatrix}
2 &amp; 3\\
2 &amp; 1\\
\end{pmatrix}\times
\begin{pmatrix}
3\\\\
2
\end{pmatrix}=
\begin{pmatrix}
12\\\\
8
\end{pmatrix}=4\times
\begin{pmatrix}
3\\\\
2
\end{pmatrix}
\end{align}\]</span> <span
class="math display">\[\bf{图2.2：非特征向量和1个特征向量}\]</span>
<span class="math display">\[\begin{align}
2\times
\begin{pmatrix}
3\\\\
2
\end{pmatrix}=
\begin{pmatrix}
6\\\\
4
\end{pmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}\begin{pmatrix}
2 &amp; 3\\
2 &amp; 1\\
\end{pmatrix}\times
\begin{pmatrix}
6\\\\
4
\end{pmatrix}=
\begin{pmatrix}
24\\\\
16
\end{pmatrix}=4\times
\begin{pmatrix}
6\\\\
4
\end{pmatrix}
\end{align}\]</span></p>
<p><span class="math display">\[\bf{图2.3:
缩放特征向量后仍为特征向量}\]</span></p>
<h5 id="特征向量">2.2.1 特征向量</h5>
<p>如你所知，只要两个矩阵的大小相容，你就可以将两个矩阵相乘。特征向量是矩阵相乘的的特殊形式。我们现在考虑如图2.2所示的矩阵和向量相乘的情况。</p>
<p>第一个例子中，计算结果不是整数与原始矩阵相乘的形式，但到了第二的例子，计算结果的就是一个整数乘以与左边完全相同的一个向量。为何能产生这样的结果呢？实际上，向量就是2维空间的一个矢量。向量<span
class="math inline">\(\begin{pmatrix}3\\2\end{pmatrix}\)</span>(第二个相乘的例子)代表从原点<span
class="math inline">\((0,0)\)</span>一个指向<span
class="math inline">\((3,2)\)</span>的一个箭头，另一个矩阵可以被认为是变换矩阵。如果你在向量的左边乘以一个矩阵，结果就是把这个向量从其原始位置进行了变换。</p>
<p>上面说得就是变换就是特这向量的本质。想象一个变换矩阵，以及一个在直线<span
class="math inline">\(y=x\)</span>上的向量，矩阵左乘这个向量。如果你发现结果仍然位于<span
class="math inline">\(y=x\)</span>这条直线上，那么这就向是量的自反射。这个向量（所有的乘子，因为我们不关心向量的大小）就是这个变换矩阵的一个特征向量。</p>
<p>这些特征向量有什么性质呢？第一你要知道的就是只有方矩阵才有特征向量。其次是不是所有的方矩阵都有特征向量。最后，如果一个<span
class="math inline">\(n\times n\)</span>矩阵只要有，那么就一定有<span
class="math inline">\(n\)</span>个特征向量。如果一个<span
class="math inline">\(3\times
3\)</span>的矩阵有特征向量，那就有3个。</p>
<p>特征向量的另一个性质是：如果我在相乘之前对其缩放一定量，那么我可以仍然得到相同的乘积形式（如图2.3）。这是因为如果你缩放一个的向量，你做的仅仅是把这个向量变长，而没有改变其方向。最后，一个矩阵的所有的特征向量都是<em>垂直</em>的。也就是说，无论你有多少维的向量，他们都是互相形成直角。另一个更数学化的说法叫<em>正交</em>。这么描述非常重要，原因是我们可以更方便表述这些垂直的正交向量，而不用在<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴的坐标系中描述。在PCA介绍部分我们会用到这些。</p>
<p>另一点重要的是，数学家们在寻找特征向量时，他们总喜欢找长度为1的特征向量。原因我们已经知道，向量的长度并不是影响因素，方向才是。所以为了使特征向量有标准形式，我们的一般做法是将其缩放成长度为1的向量。这样，所有的特征向量就都有相同的长度了。下面我们把例子中的向量标准化。
<span class="math display">\[\begin{pmatrix}
3\\
2
\end{pmatrix}
\]</span> 是一个特征向量，这个向量的长度为： <span
class="math display">\[\sqrt{(3^2+2^2)}=\sqrt{13}\]</span>
所以我们把原始的向量除以这个长度，就得到了长度唯一的特征向量。 <span
class="math display">\[\begin{pmatrix}
3\\
2
\end{pmatrix}\div\sqrt{13}=
\begin{pmatrix}
{3}/{\sqrt{13}}\\
{2}/{\sqrt{13}}
\end{pmatrix}
\]</span></p>
<p>怎么找到这些神秘的特征向量呢？很不幸，只有当矩阵足够小时，特征向量才好找，比如不超过<span
class="math inline">\(3\times
3\)</span>的矩阵。如果矩阵大小再变大，通常的做法是用复杂的迭代方式求解，这些方法此教程不会讲解。如果你想在程序中使用计算特征向量的方法，很多数学库都有实现，<a
target="_blank" rel="noopener" href="http://webnz.com/robert/">一个有用的数学库包</a>。</p>
<p>如果想进一步了解特征向量和特征值以及正交等内容，请参考霍华德.安东著有约翰威立国际出版公司出版的数学课本《Elementary
Linear Algebra 5e》，ISBN 0-471-85223-6。</p>
<h5 id="特征值">2.2.2 特征值</h5>
<p>特征值和特征向量高度相关，其实我们已经在图2.2看到过特征值。还记得被矩阵缩放以后的特征向量有相同的大小么？在那个例子中，这个值是4。这里4就是特征向量相关的特征值。无论我们对特征向量怎么缩放，我们始终得到的特征值都一直是一样的，如图2.3的例子特征值一直是4。</p>
<p>现在我们发现特征向量和特征值总是成对出现。如果你现在需要某个编程库计算特征向量，通常特征值也被同时计算出来了。</p>
<h4 id="练习-2">练习</h4>
<p>对于下面的矩阵 <span class="math display">\[\begin{pmatrix}
3&amp;0&amp;-1\\
-4&amp;1&amp;2\\
-6&amp;0&amp;-2
\end{pmatrix}
\]</span> 判断下面是否有此矩阵的特征向量，如果有，请求出对应的特征值。
<span class="math display">\[
\begin{pmatrix}
2\\
2\\
-1
\end{pmatrix}\
\begin{pmatrix}
-1\\
0\\
2
\end{pmatrix}\
\begin{pmatrix}
-1\\
1\\
3
\end{pmatrix}\
\begin{pmatrix}
0\\
1\\
0
\end{pmatrix}\
\begin{pmatrix}
3\\
2\\
1
\end{pmatrix}
\]</span></p>
<h3 id="第三章-主成分分析principal-components-analysis">第三章
主成分分析（Principal Components Analysis）</h3>
<p>终于到了主成分分析（PCA）部分了，PCA可以在数据中识别模式，并通过此种方式突出数据中相似和不同的部分。由于很难用图像表示高维数据，也就意味着在高维数据中寻找模式变得非常困难，这时，PCA就成了极为强大的数据分析工具。</p>
<p>另一个PCA的优点是，如果你通过其找到了数据中的模式，你还可以用来压缩数据，即，在不损失太多信息的前提下降低数据的维度。这个技术被用于图像压缩，我们在稍后的章节中会有涉及。</p>
<p>本章我们将针对一个数据集，一步一步实现PCA计算。这里我不准备描述<em>为什么</em>
PCA表现出色。我做的是为你提供每一步都发生了什么，这样，将来如果你想使用此技术时，就会有足够多的知识帮助你做决策。</p>
<h4 id="方法">3.1 方法</h4>
<h5 id="第一步数据集">第一步：数据集</h5>
<p>在我们这个简单的例子中，我会使用我编造的一个数据集。这个数据集只有两维，之所以选择这份数据是因为我可以通过画出图形来分析PCA的每一步都发生了什么。
##### 第二步：减掉均值
如果想实现PCA，我们首先要把每一维的数据减掉均值。就是说要对每一维求平均值，接着把每一维的每个数据都减掉均值。我们这里所有的<span
class="math inline">\(x\)</span>值都要减掉<span
class="math inline">\(\overline{x}\)</span>(<span
class="math inline">\(x\)</span>维度所有数据的均值)，所有的<span
class="math inline">\(y\)</span>值都减掉<span
class="math inline">\(\overline{y}\)</span>。这样我们就构造了一个均值为0的数据集。
<span class="math display">\[\begin{align}
\bf{数据}=
\begin{array}{c|c}
x&amp; y\\
\hline
2.5 &amp; 2.4\\
0.5 &amp; 0.7\\
2.2 &amp; 2.9\\
1.9 &amp; 2.2\\
3.1 &amp; 3.0\\
2.3 &amp; 2.7\\
2 &amp; 1.6\\
1 &amp; 1.1\\
1.5 &amp; 1.6\\
1.1 &amp; 0.9\\
\end{array}\bf{调整后的数据=}
\begin{array}{c|c}
x&amp; y\\
\hline
0.69 &amp; 0.49\\
-1.31 &amp; -1.21\\
0.39 &amp; 0.99\\
0.09 &amp; 0.29\\
1.29 &amp; 1.09\\
0.49 &amp; 0.79\\
0.19 &amp; -0.31\\
-0.81 &amp; -0.81\\
-0.31 &amp; -0.31\\
-0.71 &amp; -1.01\\
\end{array}
\end{align}\]</span> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-062821.jpg" />
<span
class="math display">\[图3.1：PCA示例数据，左边为原始数据，右边为减掉均值的数据\]</span></p>
<h5 id="第三步计算协方差矩阵">第三步：计算协方差矩阵</h5>
<p>协方差矩阵我们在2.1.4小节已经讨论过。由于我们的数据是2维的，所有协方差矩阵就是<span
class="math inline">\(2\times
2\)</span>。协方差矩阵计算没特别说明的，我直接给出结果： <span
class="math display">\[cov=\begin{pmatrix}
0.616555556&amp;0.615444444\\
0.615444444&amp;0.716555556
\end{pmatrix}\]</span>
由于协方差矩阵非对角线元素都是正值，所以我们可以预期<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>一起增减。 #####
第四步：计算协方差矩阵的特征向量和特征值
协方差矩阵是方阵，所以我们可以计算其特征向量和特征值。这极为重要，因为他们可以告诉我们关于数据的有用信息。我一会儿会说明原因，现在我们来看一下特征值和特征向量：
<span class="math display">\[\begin{align}
\bf{特征值}=\begin{pmatrix}
0.0490833989\\
1.28402771
\end{pmatrix}\bf{\bf    特征向量}=\begin{pmatrix}
-0.7351178656 &amp; -0.6778873399\\
0.677873399 &amp; -0.735178656
\end{pmatrix}
\end{align}\]</span>
一定要注意到两个特征向量都是单位向量。也就是说他们的长度都是1。这个结果对PCA非常重要，幸运的是，大部分数学工具包计算特征向量提供的都是单位向量。</p>
<p>那么，这些计算结果都是什么意思呢？如果你观察图3.2的数据点，你会发现这些数据有非常强的模式。和我们用协方差预期的一致，这些数据确实一起增减。我利用数据同时也画了两个特征向量，这两个特征向量看起来像图3.2的对角线。我们在特征向量小节介绍过，两个特征向量是相互垂直的。但是，更重要的是特征向量为我们提供了数据中的模式信息。可以看出，其中一条线（译者注：大概45度倾角的这条线）看起来像画了拟合这些数据点的一条线。这个特征向量告诉我们两个数据维度沿着线的相关性（译者注：原文是两个数据集，我认为是两个数据维度）。第2个特征向量（译者注：大概135度倾角的这条线）给我们提供了另外一些重要性稍低的数据中的模式，数据点分布在线的两边。</p>
<p>因此，通过从特征矩阵中取出特征向量进行分析，我们已经提取出了刻画数据特点的线。接下来的步骤会包括数据变换以便于用我们这些线来表达数据。
##### 第五步：选择成分及构建特征的向量
现在我们来讨论数据压缩和降维的概念。如果你学习了前面小节的特征向量和特征值的相关信息，你会注意到特征向量是之间有很大不同。事实上，具有更大特征值对应的特征向量是数据集的主成分(Principal
Component)。在我们这个例子中，对应更大特征值特的特征向量是基本拟合数据点的这条线。这维特征向量描述了数据维度之间最重要的关系。
<img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-081716.jpg" />
<span
class="math display">\[图3.2:\bf{标准化（减掉均值）后的数据图以及协方差矩阵中特征向量图}\]</span></p>
<p>一般来说，一旦从协方差矩阵中找出特征向量，下一步我们要做的就是把他们对应的特征值从高到低排列。排序表明了成分（component）的重要性高低。现在，如果你如果愿意，可以忽略那些重要性没那么高的成分，你就会丢失一些信息，但是如果特征值非常小，你丢失的信息并不会太多。如果你扔掉一些成分，最终的数据集的维度会低于原始数据的维度。具体来说，如果你的原始数据有<span
class="math inline">\(n\)</span>维，因此你可以计算出<span
class="math inline">\(n\)</span>个特征向量和<span
class="math inline">\(n\)</span>个特征值，接下来如果你只选取前<span
class="math inline">\(p\)</span>维特征向量，那么最终的数据就变成了<span
class="math inline">\(p\)</span>维的数据集。</p>
<p>现在你需要做的是构造一个特征(feature)的向量，其实就是一个向量的矩阵。矩阵是通过挑选你希望留下的特征向量，组成一个每列1个特征向量的矩阵(译者注：最后一维我认为是第p维更好，可以与上面一段对应)。
<span
class="math display">\[\bf{特征的向量}=(eig_1,eig_2,eig_3,...,eig_p)\]</span></p>
<p>来看我们的例子，现在我们有2个特征向量，我们现在有两个选择，第一选择是两个特征向量都被用于构造特征的向量：
<span class="math display">\[cov=\begin{pmatrix}
-0.77873399&amp;-0.735178656\\
-0.735178656&amp;0.677873399
\end{pmatrix}\]</span>
或者，我们可以扔掉不重要的成分，那么特征的向量只有1列： <span
class="math display">\[cov=\begin{pmatrix}
-0.677873399\\
-0.735178656
\end{pmatrix}\]</span> 下一节我们将针对上面两种新的数据集进行讨论。
##### 第六步：生成新数据集
这是PCA的最后同时是最简单的一步。一旦我们选择了我们希望保留到成分（特征向量集），我们只需把特征的矩阵转置，左乘调整后的数据（原始数据减掉均值），然后再转置。
<span class="math display">\[\bf{最终数据=行特征的向量} \times
\bf{行调整后的数据}\]</span></p>
<p>这里<span
class="math inline">\(\bf{行特征的向量}\)</span>是特征的向量组成的矩阵进行转置，也就是说现在特征向量现在是以行的形式排列，最重要的特征向量在第一行。<span
class="math inline">\(\bf{行调整后的数据}\)</span>是经过均值调整后的数据，也进行了转置，也就是说数据项在每一列，而每行是一个独立的维度。抱歉数据转置可能来得有点儿突然，但是如果我们现在对特征向量的矩阵和数据进行转置，后面的公式就会简单很多，而不是一直带着个转置的上标符号<span
class="math inline">\(T\)</span>。<span
class="math inline">\(\bf{最终数据}\)</span>是最终的数据集合，其中每一列是一个数据项，每一行是一个维度。</p>
<p>做完这些我们可以得到什么呢？我们可以得到和我们选择向量完全相关的原始数据。我们的原始数据有<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴两个坐标的坐标系，所以我们的数据与这两个坐标的坐标系相关。其实你可以用任何你喜欢的两个坐标轴的坐标系来表示你的数据。如果坐标轴互相垂直，这种表示方法是最高效的，这就是为何特征向量间互相垂直这么重要。现在我们已经把我们的数据从跟<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴相关改为2个特征向量组成的坐标系相关。如果说我们已经通过降维构造了新的数据集，也就是说我们扔掉了一些特征向量，那么新数据只跟我们留下的特征向量相关。</p>
<p>为了展示我们的数据，我已经把两种可能的特征的向量都对数据做了变换。我已经对每种情况的结果进行的了转置，这样我就把数据恢复成表结构的组织形式。同时，我也把最终的数据点画了出来，这样我们就可以观察这些数据点与这些成分之间的关系。</p>
<p>两个特征向量都保留的情况转换后的结果见图3.3。这个图其实就是原始数据旋转后，这样特征向量就成了坐标轴。这种情况很好理解，因为我们在分解的过程中并没有丢失任何信息。</p>
<p>另外一种变换，我们只保留有最大特征值的特征向量，我们可以从图3.4中看的数据的结果。和预期的一样，这个数据只有一维。如果你用这份数据与两维特征向量都用变换后的数据对比，你会注意到，这个数据就是另一份数据的第一列。所以，如果你画出这个数据的图，这份数据只有一维，那么结果其实就是图3.3数据点<span
class="math inline">\(x\)</span>的坐标点。我们其实就是高效的抛弃了其他的坐标轴，也就是其他的特征向量。</p>
<p>那么我究竟做了什么呢?
本质上我们把数据进行了变换，使之可以用相关的模式进行表示，这些模式就是一些最适合描述这些数据之间关系的线。这么做非常有用，因为我们现在已经把数据点对每条线的贡献进行分类，然后进行组合。首先，我们仅仅有<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴，这还不错，但是每个<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>的数据点其实并无法告诉我们，每个数据点和其他数据点之间的关系。现在数据点的值可以精确告诉我们数据点处于趋势线的位置（上面或者下面）。如果是两个特征向量都用的情况，我们仅仅是把数据转换以便于我们使这些数据与特征向量相关，而不是<span
class="math inline">\(x\)</span>轴和<span
class="math inline">\(y\)</span>轴。但是只留一维特征向量的分解移除了较小特征向量的贡献，是我们的数据只与保留的一维数据相关。
##### 3.1.1 把旧数据找回来
显然，如果你用PCA对数据进行压缩，你一定想把原始数据恢复回来。（下一章我们看到例子）这些内容来自于<a
target="_blank" rel="noopener" href="http://www.vision.auc.dk/sig/Teaching/Flerdim/Current/hotelling/hotelling.html">这里</a>。
<span class="math display">\[\begin{align}
\bf{转换后的数据}=\begin{array}{c|c}
x&amp;y\\
\hline
-0.827970186 &amp; -0.175115307\\
1.77758033 &amp; 0.142857227\\
-0.992197494 &amp; 0.384374989\\
-0.274210416 &amp; 0.1304117207\\
-1.67580142 &amp; -0.209498461\\
-0.912949103 &amp; 0.17528282444\\
0.0991094375 &amp; -0.349824698\\
1.14457216 &amp; 0.0464172582\\
0.438046137 &amp; 0.0177646297\\
1.22382056 &amp; -0.162675287\\
\end{array}
\end{align}\]</span> <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-134547.jpg" />
<span
class="math display">\[图3.3：应用了PCA分析后并使用了两个特征向量的数据表以及绘制的新数据点\]</span></p>
转换后的数据(单一特征向量)
<span class="math display">\[\begin{array}{c}
x\\
\hline
-0.827970186\\
1.77758033\\
-0.992197494\\
-0.274210416\\
-1.67580142\\
-0.912949103\\
1.14457216\\
0.438046137\\
1.22382056
\end{array}\]</span>
<p><span
class="math display">\[图3.4：只用最重要的特征向量数据转换的数据\]</span></p>
<p>所以，我们怎么把原来数据恢复回来？在我们进行恢复原始数据之前，回忆只有我们将所有特征向量进行转换才能精确的把数据恢复回来。如果我们在最后转换时减少特征向量，那么恢复的数据已经失去很多信息。
回想一下，最后的变换是： <span
class="math display">\[\bf{最终数据=行特征的向量} \times
\bf{行调整后的数据}\]</span> 我们可以把公式反转过来，进而得到原始数据，
<span
class="math display">\[\bf{行调整后的数据}=\bf{行特征的向量}^{-1}\times\bf{最终数据}\]</span>
这里，<span
class="math inline">\(\bf{行特征的向量}^{-1}\)</span>是的<span
class="math inline">\(\bf{行特征的向量}\)</span>的逆。由于我们讨论的是特征向量，组成的特征的向量，所以，<span
class="math inline">\(\bf{行特征的向量}^{-1}\)</span>其实就是<span
class="math inline">\(\bf{行特征的向量}\)</span>的转置。当然，只有在矩阵中的所有元素是由单位特征向量组成是才成立。这样，恢复原始数据又变得容易了很多，现在公式变成了：
<span
class="math display">\[\bf{行调整后的数据}=\bf{行特征的向量}^{T}\times\bf{最终数据}\]</span>
这个公式在我们只保留部分特征向量的情况下仍然成立。也就是说，就算你扔掉了一下特征向量，上面的公式仍然成立。</p>
<p>我不会演示用所有特征向量恢复原始数据，因为这样计算的结果和开始的数据一模一样。但是，我们一起来看一下只保留了一维特征向量的情况下，是怎样损失信息的。图3.5展示了丢失信息的情况。我们把图中的数据点与图3.1对比一下就会发现，沿着主特征向量的变化被保留下来了（见图3.2特征向量及数据）沿着其他成分（另一个特征向量被扔掉了）的变化丢失了。</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-30-003805.jpg" />
<span class="math display">\[图3.5
从单独一维特征向量重新构造的数据\]</span></p>
<h4 id="练习-3">练习</h4>
<ol type="1">
<li>协方差矩阵的特征向量为我们提供了什么呢？</li>
<li>我们在PCA计算的过程中，哪一步可以决定压缩数据，压缩可以起到什么效果呢？</li>
<li>举例说明PCA在图像处理怎样用主成分表示，同时调研一下人脸识别中“特征脸”(Eigenfaces)主题。</li>
</ol>
<h3 id="第四章-计算机视觉应用">第四章 计算机视觉应用</h3>
<p>本章我们将简单的PCA在计算机视觉领域的应用，首先我们看一下图像是怎么表示的，然后我们我们看看能怎样用PCA处理这些图像。本章关于人脸识别的的信息主要来自于1997年IEEE
9月Vol 85, No. 9《Face Recognition: Eigenface, Elastic Matching, and
Neural Nets》。图像表示来自于爱迪生-韦斯利出版社1987年出版的由Rafael C.
Gonzalez 和Paul Wintz合著的《Digital Image
Processing》想了解更多信息，KL变换相关知识也是非常好的参考。图像压缩相关知识来自于<a
target="_blank" rel="noopener" href="http://www.vision.auc.dk/sig/Teaching/Flerdim/Current/hotelling/hotelling.html">这里</a>，此网站还提供了大量用不同数量特征向量重新构造图像的方法。</p>
<h4 id="表示">4.1 表示</h4>
<p>在我们把一系列矩阵技术应用于计算机视觉上时，我们必须考虑图像的表示方法。一个正方形，<span
class="math inline">\(N\times N\)</span>的图像可以被表示成<span
class="math inline">\(N^2\)</span>维的向量。 <span
class="math display">\[X=(x_1,x_2,x_3,...,x_{N^2})\]</span>
这里，第一行前<span class="math inline">\(N\)</span>个<span
class="math inline">\((x_1--x_n)\)</span>一个挨着一个的像素点组成了1维的图像，下<span
class="math inline">\(N\)</span>个元素是下一行，以此类推。每个像素点的值代表图像三原色的亮度，也可能是只是灰度图像，那么只需要1个单独的值即可表示。
#### 4.2 PCA寻找模式
假设我们有20个图像。每个图像的像素非常高。对每个图像，我们都建立一个图像向量表示相应图像。接着我们就可以把所有的图像放到一个像这样的大矩阵中：
<span class="math display">\[图像矩阵=
\begin{pmatrix}
ImageVec_1\\
ImageVec_2\\
\vdots\\
ImageVec_{20}\\
\end{pmatrix}\]</span>
我们现在就可以开始以这条图像矩阵为基始，应用PCA，先构造协方差矩阵，然后得到原始数据相关的特征向量。为什么用PCA分析有用呢？假设我们要做人脸识别，那我们的原始数据就是很多人脸。接下来的问题是，给一张我新的图片，那么这是原始人脸数据中谁的人脸呢（注意，这新的图片不是我们开始给的20个人脸图片）？计算机视觉的处理方法是衡量新的图片和原始图片的差别，但并不是在原始坐标系进行对比，而是在PCA分析的生成的坐标系下衡量。</p>
<p>在实际应用中，PCA生成的坐标系下识别人脸会好非常多，因为PCA分析已经提供了原始图片中不同和相似等相关性。主成分分析已经识别出了数据中的统计学模式。</p>
<p>因为所有的向量都是<span
class="math inline">\(N^2\)</span>维的，我们最后会得到<span
class="math inline">\(N^2\)</span>个特征向量，在实践中，我们可以扔掉其中不重要的一些特征向量，识别效果仍然非常好。
#### 4.3 PCA图像压缩
使用PCA做图像压缩常常也被称作霍特林变换或者是KL变换（Karhunen-Leove
transform）. 如果我们有20个图像，每个图像有<span
class="math inline">\(N^2\)</span>个像素，所以我们就可以构造<span
class="math inline">\(N^2\)</span>个向量，每个向量20维。每个向量由每个图片的相同像素点的图片亮度值组成。这与我们之前的例子不同，因为之前我们是有一个图像的向量，向量里的每项都是不同的像素。然而我们现在是有一个每个像素的向量，向量里的每项是都是来自于不同的图片。</p>
<p>如果现在我们在一个数据集上应用PCA，那么，我们将会得到20个特征向量，因为，每个向量都是20维的。如果想要压缩数据，我们可以选择只用其中一部分特征向量变换，假设是15个特征向量。这样我得到的最终数据只有15维，达到了节省空间的目的。但是，当要恢复原始数据是，图像已经丢了一些信息。这种压缩技术叫做有损压缩，因为解压后的图片已经不是和原始图片完全一样的图片了，一般来说会变差。</p>
<h3 id="附录-a">附录 A</h3>
<h4 id="实现代码">实现代码</h4>
<p>这份代码用于可替换Matlab的自由软件Scilab。我用这份代码生成了文章的所有例子。除了第一个宏，剩下的都是我(原文作者)写的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">// This macro taken from</span><br><span class="line">// http://www.cs.montana.edu/ ̃harkin/courses/cs530/scilab/macros/cov.sci // No alterations made</span><br><span class="line">// Return the covariance matrix of the data in x, where each column of x</span><br><span class="line">// is one dimension of an n-dimensional data set.  That is, x has x columns</span><br><span class="line">// and m rows, and each row is one sample.</span><br><span class="line">//</span><br><span class="line">// For example, if x is three dimensional and there are 4 samples.</span><br><span class="line">// x=[123;456;789;101112]</span><br><span class="line">// c=cov(x)</span><br><span class="line">function [c]=cov (x)</span><br><span class="line">// Get the size of the array</span><br><span class="line">sizex=size(x);</span><br><span class="line">// Get the mean of each column</span><br><span class="line">meanx = mean (x, &quot;r&quot;);</span><br><span class="line">// For each pair of variables, x1, x2, calculate</span><br><span class="line">// sum ((x1 - meanx1)(x2-meanx2))/(m-1)</span><br><span class="line">for var = 1:sizex(2),</span><br><span class="line">        x1 = x(:,var);</span><br><span class="line">        mx1 = meanx (var);</span><br><span class="line">        for ct = var:sizex (2),</span><br><span class="line">                x2 = x(:,ct);</span><br><span class="line">                mx2 = meanx (ct);</span><br><span class="line">                v = ((x1 - mx1)’ * (x2 - mx2))/(sizex(1) - 1);</span><br><span class="line">end, c=cv;</span><br><span class="line">end,</span><br><span class="line">cv(var,ct) = v;</span><br><span class="line">cv(ct,var) = v;</span><br><span class="line">// do the lower part of c also.</span><br><span class="line">// This a simple wrapper function to get just the eigenvectors</span><br><span class="line">// since the system call returns 3 matrices</span><br><span class="line">function [x]=justeigs (x)</span><br><span class="line">// This just returns the eigenvectors of the matrix</span><br><span class="line">[a, eig, b] = bdiag(x);</span><br><span class="line">x= eig;</span><br><span class="line">// this function makes the transformation to the eigenspace for PCA</span><br><span class="line">// parameters:</span><br><span class="line">// adjusteddata = mean-adjusted data set</span><br><span class="line">// eigenvectors = SORTED eigenvectors (by eigenvalue)</span><br><span class="line">// dimensions  = how many eigenvectors you wish to keep</span><br><span class="line">//</span><br><span class="line">// The first two parameters can come from the result of calling</span><br><span class="line">// PCAprepare on your data.</span><br><span class="line">// The last is up to you.</span><br><span class="line">function [finaldata] = PCAtransform(adjusteddata,eigenvectors,dimensions) finaleigs = eigenvectors(:,1:dimensions);</span><br><span class="line">prefinaldata = finaleigs’*adjusteddata’;</span><br><span class="line">finaldata = prefinaldata’;</span><br><span class="line">// This function does the preparation for PCA analysis</span><br><span class="line">// It adjusts the data to subtract the mean, finds the covariance matrix,</span><br><span class="line">// and finds normal eigenvectors of that covariance matrix.</span><br><span class="line">// It returns 4 matrices</span><br><span class="line">// meanadjust = the mean-adjust data set</span><br><span class="line">// covmat = the covariance matrix of the data</span><br><span class="line">// eigvalues = the eigenvalues of the covariance matrix, IN SORTED ORDER</span><br><span class="line">// normaleigs = the normalised eigenvectors of the covariance matrix,</span><br><span class="line">// IN SORTED ORDER WITH RESPECT TO</span><br><span class="line">// THEIR EIGENVALUES, for selection for the feature vector.</span><br><span class="line">//</span><br><span class="line">// NOTE: This function cannot handle data sets that have any eigenvalues</span><br><span class="line">// equal to zero. It’s got something to do with the way that scilab treats</span><br><span class="line">// the empty matrix and zeros.</span><br><span class="line">//</span><br><span class="line">function [meanadjusted,covmat,sorteigvalues,sortnormaleigs] = PCAprepare (data) // Calculates the mean adjusted matrix, only for 2 dimensional data</span><br><span class="line">means = mean(data,&quot;r&quot;);</span><br><span class="line">meanadjusted = meanadjust(data);</span><br><span class="line">covmat = cov(meanadjusted);</span><br><span class="line">eigvalues = spec(covmat);</span><br><span class="line">normaleigs = justeigs(covmat);</span><br><span class="line">sorteigvalues = sorteigvectors(eigvalues’,eigvalues’);</span><br><span class="line">sortnormaleigs = sorteigvectors(eigvalues’,normaleigs);</span><br><span class="line">// This removes a specified column from a matrix</span><br><span class="line">// A = the matrix</span><br><span class="line">// n = the column number you wish to remove</span><br><span class="line">function [columnremoved] = removecolumn(A,n)</span><br><span class="line">inputsize = size(A);</span><br><span class="line">numcols = inputsize(2);</span><br><span class="line">temp = A(:,1:(n-1));</span><br><span class="line">for var = 1:(numcols - n)</span><br><span class="line">        temp(:,(n+var)-1) = A(:,(n+var));</span><br><span class="line">columnremoved = temp;</span><br><span class="line">// This finds the column number that has the</span><br><span class="line">// highest value in it’s first row.</span><br><span class="line">function [column] = highestvalcolumn(A)</span><br><span class="line">inputsize = size(A);</span><br><span class="line">numcols = inputsize(2);</span><br><span class="line">maxval = A(1,1);</span><br><span class="line">maxcol = 1;</span><br><span class="line">for var = 2:numcols</span><br><span class="line">        if A(1,var) &gt; maxval</span><br><span class="line">                maxval = A(1,var);</span><br><span class="line">end,</span><br><span class="line">        end,</span><br><span class="line">column = maxcol</span><br><span class="line">maxcol = var;</span><br><span class="line">25</span><br><span class="line">end,</span><br><span class="line">// This sorts a matrix of vectors, based on the values of</span><br><span class="line">// another matrix</span><br><span class="line">//</span><br><span class="line">// values = the list of eigenvalues (1 per column)</span><br><span class="line">// vectors = The list of eigenvectors (1 per column)</span><br><span class="line">//</span><br><span class="line">// NOTE:  The values should correspond to the vectors</span><br><span class="line">// so that the value in column x corresponds to the vector</span><br><span class="line">// in column x.</span><br><span class="line">function [sortedvecs] = sorteigvectors(values,vectors)</span><br><span class="line">inputsize = size(values);</span><br><span class="line">numcols  = inputsize(2);</span><br><span class="line">highcol = highestvalcolumn(values);</span><br><span class="line">sorted = vectors(:,highcol);</span><br><span class="line">remainvec = removecolumn(vectors,highcol);</span><br><span class="line">remainval = removecolumn(values,highcol);</span><br><span class="line">for var = 2:numcols</span><br><span class="line">        highcol = highestvalcolumn(remainval);</span><br><span class="line">        sorted(:,var) = remainvec(:,highcol);</span><br><span class="line">        remainvec = removecolumn(remainvec,highcol);</span><br><span class="line">        remainval = removecolumn(remainval,highcol);</span><br><span class="line">end,</span><br><span class="line">sortedvecs = sorted;</span><br><span class="line">// This takes a set of data, and subtracts</span><br><span class="line">// the column mean from each column.</span><br><span class="line">function [meanadjusted] = meanadjust(Data)</span><br><span class="line">inputsize = size(Data);</span><br><span class="line">numcols = inputsize(2);</span><br><span class="line">means = mean(Data,&quot;r&quot;);</span><br><span class="line">tmpmeanadjusted = Data(:,1) - means(:,1);</span><br><span class="line">for var = 2:numcols</span><br><span class="line">        tmpmeanadjusted(:,var) = Data(:,var) - means(:,var);</span><br><span class="line">meanadjusted = tmpmeanadjusted</span><br><span class="line">end,</span><br></pre></td></tr></table></figure>
<p><a
target="_blank" rel="noopener" href="http://cmb.oss-cn-qingdao.aliyuncs.com/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89.pdf">本文PDF</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/03/18/Naive-Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/03/18/Naive-Bayes/" class="post-title-link" itemprop="url">Naive Bayes</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-03-18 21:06:08" itemprop="dateCreated datePublished" datetime="2017-03-18T21:06:08+01:00">2017-03-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="introduction">Introduction</h3>
<p>You are sitting in front your sceen, annoyed by a bunch of spam
mails. You wonder if there are any appoaches to get rid of so much many
offended emails. Last time you doped out a extremely good idea. You set
a series of words to identify those emails: every mail invovled by words
"coupon" was trown to trash. However, on one hand, there were only about
10% spam including "coupon", one the other hand, you had trashed two
significant emails, due to which, you lost two business valued about two
million dollars. The thing was that, your inbox seems being overrun by
those spams. Who can rescue you from endless deleting spams
everyday?</p>
<h3 id="intuition">Intuition</h3>
<p>Actually, you were close to the answer when you were putting all
emails to trash which included the word "coupon". Today, we learn about
an efficient method to solve the problem systematically. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-18-110227.jpg" />
Maybe we can search through internet, find all words which are about
advertisment in email. You can restrict that if and only if those which
includes more than 4 word in the list of spam words can be put into
trash can. Maybe finally you can design a rule system to recoginize
those spams without miss important ones. But it seems so boring a job to
do this, moreover, it may be not personalized. If some those who are
working for saling discount stuffs, he/she may find it doesn't work
through applying your effective rules. How about thinking of probablity
of those words emerge in all your inbox. Some words such as "coupon" may
contribute more but not entirety, simultaneously, affordable may
contribute less but not none. Notice that profit emerge in spam emails
and normal emails both sometimes. We let <span
class="math inline">\(y=0\)</span> denote an normal email while <span
class="math inline">\(y=1\)</span> the opposite. And if a word such as
"coupon" emerges in a mail, we set <span
class="math inline">\(coupon=1\)</span>, otherwise <span
class="math inline">\(coupon=0\)</span>. Suppose you have an email, we
define the probability to : <span class="math display">\[\begin{align}
&amp;p1=p(y=0|free=1, discount=1, affordable=1, customer=0, KPI=0,
budget=0,...,bias=0)=?\\
&amp;p2=p(y=1|free=1, discount=1, affordable=1, customer=0, KPI=0,
budget=0,...,bias=0)=?
\end{align}\]</span> Our aim is to decide which is bigger <span
class="math inline">\(p1\)</span> or <span
class="math inline">\(p2\)</span>. Let's describe the problem as
followed: &gt; We want to decide the probality of a mail spam or normal
when word "free" is in the mail, "discount" is in the mail, "affordable"
is in the mail, customer is not in the mail, ..., "bias" is not.</p>
<p>The description above is only about one email. For some other emails,
maybe "free" and "discount" both did not emerge at all.</p>
<h3 id="definitioin">Definitioin</h3>
<p>To generalize the problem, we let <span
class="math inline">\(x_i\)</span> denote a word in emails. Suppose we
now know all words in your inbox, say 10 thousand words. Then we have
<span class="math inline">\(x_1\)</span> denote if "free" is in a mail,
<span class="math inline">\(x_1=0\)</span> denotes negative while <span
class="math inline">\(x_1=1\)</span> denotes positive. So we have <span
class="math inline">\(x_1,x_2,x_3,...,x_{10000}\)</span>, which denotes
the status of each word in a mail. Then the problem is transferred as
followed: <span class="math display">\[\begin{align}
&amp;p1=p(y=0|x_1=1, x_2=1, x_3=1, x_4=0, x_5=0,
x_6=0,...,x_{10000}=0)=?\\
&amp;p2=p(y=1|x_1=1, x_2=1, x_3=1, x_4=0, x_5=0,
x_6=0,...,x_{10000}=0)=?
\end{align}\]</span> Suppose we have N words in your inbox, then we want
to decide: <span class="math display">\[\begin{align}
&amp;p1=p(y=0|x_1, x_2,...,x_{N})=?\\
&amp;p2=p(y=1|x_1, x_2,...,x_{N})=?
\end{align}\]</span> So how to sovle the probability problem?</p>
<h3 id="naive-bayes">Naive Bayes</h3>
<p>Recall Bayes rules: <span class="math display">\[\begin{equation}
p(y|x)=\frac{p(x,y)}{p(x)}=\frac{p(x|y)p(y)}{p(x)}
\end{equation}\]</span> we apply the equation to our problem, then we
have: <span class="math display">\[\begin{align}
&amp;p(y=1|x_1,x_2,...,x_N)\\
&amp; =\frac{p(x_1,x_2,...,x_N,y=1)}{p(x_1,x_2,...,x_N)}\\
&amp; =\frac{p(x_1,x_2,...,x_N|y)p(y=1)}{p(x_1,x_2,...,x_N)}\\
\\
&amp;p(y=0|x_1,x_2,...,x_N)\\
&amp; =\frac{p(x_1,x_2,...,x_N,y=0)}{p(x_1,x_2,...,x_N)}\\
&amp; =\frac{p(x_1,x_2,...,x_N|y)p(y=0)}{p(x_1,x_2,...,x_N)}
\end{align}\]</span> Notice that {p(x_1,x_2,...,x_N) is positive, and a
constant as well. so our aim is transferred to: <span
class="math display">\[\begin{align}
&amp;Max(p(y=1|x_1,x_2,...,x_N), p(y=0|x_1,x_2,...,x_N)\\
&amp;=Max(p(x_1,x_2,...,x_N|y=1)p(y=1),p(x_1,x_2,...,x_N|y=0)p(y=0)
\end{align}\]</span></p>
<p>First of all, we talk about how to get <span
class="math inline">\(p(y=0)\)</span> and <span
class="math inline">\(p(y=1)\)</span>. We have <span
class="math inline">\(N=10000\)</span>, suppose there are <span
class="math inline">\(900\)</span> spams and <span
class="math inline">\(91000\)</span> normal emails, then: <span
class="math display">\[\begin{align}
&amp;p(y=0)=\frac{count(spam\ email)}{count(all\
emails)}=\frac{900}{10000}=0.09\\
&amp;p(y=1)=\frac{count(normal\ email)}{count(all\
emails)}=\frac{9100}{10000}=0.91
\end{align}\]</span> And right now our task left is to compute <span
class="math inline">\(p(x_1,x_2,...,x_N|y=0)\)</span> and <span
class="math inline">\(p(x_1,x_2,...,x_N|y=1)\)</span>, that means we
want to know if a mail is a normal one or not, what is the probability
of the combination of these <span class="math inline">\(N=10000\)</span>
words. In other word, <span class="math inline">\(x_1=0\ or\
1\)</span>,<span class="math inline">\(x_2=0\ or\ 1\)</span> and so on.
So we have to compute <span
class="math inline">\(2*2^{10000}=2*1.995*10^{3010}\)</span>
probabilities under our circumstance. It seems the scale is so large
that we can not handle it. So we have an extremely adventurous
assumption: &gt; Each x in an email only can be decided by y.</p>
<p>Then we have: <span class="math display">\[\begin{align}
p(x_1,x_2,...,x_N|y=0)=p(x_1|y=0)\cdot p(x_2|y=0)\cdots p(x_N|y=0)\\
p(x_1,x_2,...,x_N|y=1)=p(x_1|y=1)\cdot p(x_2|y=1)\cdots p(x_N|y=1)
\end{align}\]</span> Now, we just need compute 2*10000 probabilities, it
is surely a mission possible now. So how to compute <span
class="math inline">\(p(x_i=0\ or\ 1|y=0\ or\ 1)\)</span>? Take word
"free" for example, we want to compute: <span
class="math display">\[\begin{align}
&amp;p(free=0|y=0)\\
&amp;=\frac{p(free=0, y=0)}{p(y=0)}\\
&amp;=\frac{count\ of\ emails\ have\ no\ word\ &quot;free&quot;\ in\
normal\ emails}{count\ of\ normal\ emails}\\
&amp;p(free=0|y=1)\\
&amp;=\frac{p(free=0, y=1)}{p(y=1)}\\
&amp;=\frac{count\ of\ emails\ have\ no\ word\ &quot;free&quot;\ in\
spams}{count\ of\ spam\ emails}
\end{align}\]</span> As long as we have computed all of these values, we
can use the equation to decide which email should be trashed. Suppose we
have computed the probabilites of <span
class="math inline">\(p1\)</span> and <span
class="math inline">\(p2\)</span>: <span
class="math display">\[\begin{align}
&amp;p1\triangleq p(x_1,x_2,...,x_N|y=1)p(y=1)\triangleq p(y=0)\cdot
\Pi_{i=1}^N p(x_i|y=0)=0.00091\\
&amp;p2\triangleq p(x_1,x_2,...,x_N|y=0)p(y=0)\triangleq p(y=1)\cdot
\Pi_{i=1}^N p(x_i|y=1)=0.00000032
\end{align}\]</span> Then we consider that the email is more of a spam
mail. ### Summary Today we have talked about how to run a Naive Bayes
algorithm to decide if an email is a spam. We suppose each word in a
mail is no of business of the other, which is a simple assumption named
<strong>conditional independence assumption</strong> but not the
reality(e.g. "coupon" maybe emerges with "save" and "money" due to their
inner association). However, the algorthm is very effective. ### Future
Thinking Suppose if a word "wooooo" haven't emerged in inbox, then the
probability will reach <span class="math inline">\(p1=p2=0\)</span>, how
to solve it? Suppose you have a task to differiate oranges from apples
and pears using color and shape, how to design the algorithm? suppose x
is continuous rather than discrete, Naive Bayes still works or not?</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class=""></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Mingbo Cheng</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"chengmingbo/gitment-comments","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
