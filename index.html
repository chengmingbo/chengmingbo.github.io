<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="Mingbo" type="application/atom+xml">






<meta name="description" content="Mingbo">
<meta property="og:type" content="website">
<meta property="og:title" content="Mingbo">
<meta property="og:url" content="http://commanber.com/index.html">
<meta property="og:site_name" content="Mingbo">
<meta property="og:description" content="Mingbo">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mingbo">
<meta name="twitter:description" content="Mingbo">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://commanber.com/">





  <title>Mingbo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Mingbo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-slides">
          <a href="/slides/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-area-chart"></i> <br>
            
            slides
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2019/03/10/deutschland1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/10/deutschland1/" itemprop="url">德国留学记录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-10T10:41:11+01:00">
                2019-03-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="办不完是琐事，跳不尽的坑"><a href="#办不完是琐事，跳不尽的坑" class="headerlink" title="办不完是琐事，跳不尽的坑"></a>办不完是琐事，跳不尽的坑</h2><p>来德国之前，还有贵人相助，避免了很多不必要的麻烦。但是，仍然遇到很多之前没有预料到的事情。从杜塞尔多夫，师姐男朋友接机到了airbnb住处开始，激活电话卡原本在中国小而又小的问题，在这边儿折腾了我俩两个星期。去见教授，教授说需要先找到房子落户后才能签合同，而且银行卡也得需要用落户证明。而没有准时入学，我买的TK公保无法生效，我爱人的私保care-concept也还没交钱。而找房子本身又是一个艰巨的任务。接下来还有很多事情要办，好在大部分的事情都按计划进行。</p>
<h4 id="1-入住"><a href="#1-入住" class="headerlink" title="1. 入住"></a>1. 入住</h4><p>airbnb提前订好房子，师姐男朋友找了好久才找到精确定位。到了门口，发现不知道怎么找房东。而手机目前没有网。之前在淘宝买的giffgaff卡没有提前激活。在机场找淘宝客服激活，告知需要提前两天，赶紧告知能否帮忙加速激活下。但是到了airbnnb的房子，是不要指望现在有网了。只能请师姐夫开一会儿WIFI登录一下airbnb，想办法联系一下房东。鼓捣了好久，才发现房东说自己不在家，在图书馆学习。留言请房东过来。师姐夫回程的人正催他回去，只好在门口等着了。还好房东很快回来，大包小包10个，几趟才搬上来。房间很大，大概有35平。大概聊一下，房东来德国读研究生，英语授课。他运气比较好，过来airbnb，直接就把房东的房子整租下来了。然后拿其中一间作为airbnb挣些外快。</p>
<h4 id="2-电话卡"><a href="#2-电话卡" class="headerlink" title="2. 电话卡"></a>2. 电话卡</h4><p>师姐提前帮忙给我和爱人买了两张预付费的telekom卡。据师姐说，只需要有护照，视频验证一下就可以用了。马不停蹄，我马上开始打开电话卡研究。根据说明书打开网站，一步一步的输入。本想15分钟内也能解决战斗，30分钟后，我已经换了3个浏览器了，要不然就是打不开视频页面，或者说打开了页面对方听不到我，或者看不着我。好容易连上一个客服，可以看到听到我，验证了5分钟信息后，发现我填写的信息，她根本看不到，结果就是，让我重新换个浏览器继续验证。最好只好用手机chrome浏览器连接到客服，这次好嘛，什么都是好的，就是他们听不到我。结果是客服V5，在我不能说的情况下，硬是给我验证通过了。前前后后折腾了3个小时，终于搞定，可以打电话和上网了。</p>
<p>可是说的我爱人的电话卡，可比这个艰难多了。当天弄完我的，想着马上把老婆的电话卡激活。有了前面的成功经验，直接拿我的电话浏览器输入我老婆的信息。可是，这次不灵了，然后我把我两个人的所有电子设备上的浏览器都试了一遍，无解，连个人影都没看到。我还在疯狂的尝试的时候，老婆已经崩溃，说先睡觉吧，找时间再弄。没想到这个找时间弄，一弄就弄了两个星期。每天，我都会拿出1个多小时去填激活的材料，填一遍，点视频验证。搞到后来，基本所有填的信息我都能背下来。好嘛，照样显示等待0分钟，没有任何其他反应。然后我们尝试第二条路，去telekom的营业厅碰碰运气。上来人家就问有没有an，当然还没落户了，所以灰头土脸的出来了。突然有一天，有个人过来对话，我们还没来得及回应，人家就把线断了。然后又是漫长无用的尝试。3.8妇女节，可能是上天觉得过节，不能让女性再受折磨了，这次竟然有人出来了，然后花了大概10分钟，一切搞定。</p>
<p>小小的电话卡竟然折腾我们这么长时间，也是奇葩了。周末人家还不工作，弄得我抓耳挠腮，想干嘛都找不到人。</p>
<h4 id="3-食物"><a href="#3-食物" class="headerlink" title="3. 食物"></a>3. 食物</h4><p>很多人觉得美食在亚洲，德国是美食荒漠。来了这几天感受是，还是可以找到很多美食，和中国做法是不一样，但是各有特色。我的最爱是döner – 一种土耳其人带到德国，并在德国发扬光大的食物。有时候我在想，很多人在欧洲生活一段时间头发会秃，是因为缺乏维生素。可是döner的搭配如果作为每天的主食，怎么会缺乏维生素呢。döner很类似中国的肉夹馍，但是馅料课丰富多了，包馅料的馍跟西安美食不同的是，饼虽然是提前蒸好的，但是做之前会用锅把两面加热至酥脆。味道的层次有多了一分。而说到味道的层次，可能需要说一下内容了。首先，切开的馍馍两面需要抹上特殊的酱料，酱料不同家有不同的配方，最后呈现出的味道也就产生变化了。肉是一直在烤的，据我观察，他们会提前把肉一层一层的叠罗汉，叠罗汉中间会放一些调味品以及淀粉物质用来黏合各个层之间。叠好的罗汉像似一个倒挂的大花瓶，外层烤好后，厨师就会刮掉一层。保证放在döner里的都是有焦面的肉。而馅料里面的东西更加丰富多彩，如大头菜丝，西红柿块，圆葱丝等等。由于döner个头实在太大，除非长了个牛嘴，不然很难一口咬下去涉及到所有的层次部分。最关键的是什么呢，döner是超级便宜的，大概3欧左右，一个人就可以吃的很饱。想想中国的肉夹馍，我连吃3个，肚子还咕咕叫。<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/doner.jpeg" alt></p>
<p>汉堡是德国的一个海港城市，以前很多生意人出海都是通过汉堡。而汉堡包实际上就是从汉堡港出发的德国人带给世界的美食礼物。无从考证到底具体是哪里人发明了汉堡，但想吃到正宗的汉堡，无疑应该是在发源地德国了。在亚琛市中心乱逛发现了一家专门做汉堡的店面，然后就马上种草了。中午的时候我强烈要求要去这家汉堡店品尝一下当地美食。这家专门做汉堡的店果然没让我们失望。汉堡和刚刚说的döner类似，都是有丰富的层次，但是汉堡层次更加鲜明。牛肉饼是其中担当，奶酪西红柿，酸黄瓜，奶油以及各种东西都可以一层一层叠在一起。而酱料也是其中最不可或缺的溶剂，把所有口味融合在一起。这种专业做汉堡的店做出来的汉堡和我们在麦当劳吃的速食汉堡是有天壤之别的。可以这么说，麦当劳吃的汉堡，只能用来充饥，换句话说，就是让我们饿不死。可是这里的汉堡是真正的美食享受。再加上薯条配上六七种自选酱料。用餐体验完全不可同日而语。<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/haburger.jpeg" alt></p>
<h4 id="4-公交"><a href="#4-公交" class="headerlink" title="4. 公交"></a>4. 公交</h4><p>来到德国才发现中国的公交费用可真是便宜到家了。两三公里的距离每个人2.8欧，什么概念，两个人一起出趟门，再回来，如果都是公交出行，好嘛，90块人民币没了。这要是天天坐公交谁能坐得起。可是不坐公交，在北京习惯徒步几公里，来了这里，突然感觉不灵了。最后还是决定坐公交吧。本来想着注册成学生就可以马上有学期票了，不但便宜，还可以整个北威州横着走。没成想，同事跟我说，如果注册成学生，需要找个第二导师，他去年5月份就来了，到现在还是没有注册成学生，依然每月买月票。不想继续每天上车买票了，其实上车买票还有个问题，就是耽误时间，德语的目的地发音又发不懂，整个一车人都得等着。所以去ASEAG总部去买了个月票。月票购买实在简单，70欧元，不记名。想问下，能不能晚上或者周末带人，告知从今年的二月份已经没有带人的说法了。只能自己买自己的了。公交是一笔大花销，可是相对与住房，就是小巫见大巫了。</p>
<p>待续… …</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/08/07/Expectation-and-variance-of-poisson-distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/07/Expectation-and-variance-of-poisson-distribution/" itemprop="url">Expectation and Variance of Poisson Distribution</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-07T21:49:00+02:00">
                2017-08-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Pmf of Poisson Distribution is as follows:</p>
<p>$$f(X=k;\lambda)=\frac{\lambda^k e^{-\lambda}}{k!}$$</p>
<p>Our aim is to derive the the expectation of $E(X)$ and the variance $Var(X)$.  Given that the formula of expectation:<br>$$<br>E(X)=\sum_{k=0}^{\infty} k \frac{\lambda^k e^{-\lambda }}{k!}<br>$$</p>
<p>Notice that when $k=0$, the formula is equal to 0, that is:</p>
<p>$$\sum_{k=0}^{\infty} k \frac{\lambda^ke^{-\lambda}}{k!}\Large|_{k=0}=0$$</p>
<p>Then, the formula become as followed:</p>
<p>$$E(X)=\sum_{k=1}^{\infty} k \frac{\lambda^ke^{-\lambda}}{k!}$$</p>
<p>$$\begin{aligned}E(X)&amp;=\sum_{k=0}^{\infty} k \frac{\lambda^ke^{-\lambda}}{k!}=\sum_{k=0}^{\infty} \frac{\lambda^ke^{-\lambda}}{(k-1)!}\&amp;=\sum_{k=0}^{\infty}  \frac{\lambda^{k-1}\lambda e^{-\lambda}}{(k-1)!}\&amp;=\lambda e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}\end{aligned}$$</p>
<p>Now we need take advantage of Taylor Expansion, recall that:</p>
<p>$$e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\cdots+\frac{x^{k-1}}{(k-1)!}=\sum_{k=1}^{\infty}\frac{x^{k-1}}{(k-1)!}$$</p>
<p>Compare $E(X)$, we can get:</p>
<p>$$E(X)=\lambda e^{-\lambda}e^\lambda=\lambda$$</p>
<p>As known that $Var(X)=E(X^2)-(E(x))^2$,  we just get $E(X^2)$. Given that:</p>
<p>$$E(X)=\sum_{k=1}^{\infty} k \frac{\lambda^ke^{-\lambda}}{k!}=\lambda$$</p>
<p>we can use this formula to derive the $E(X^2)$,</p>
<p>$$\begin{aligned}E(X)=&amp;\sum_{k=1}^{\infty} k \frac{\lambda^ke^{-\lambda}}{k!}=\lambda\\Leftrightarrow&amp;\sum_{k=1}^{\infty} k \frac{\lambda^k}{k!}=\lambda e^{\lambda}\\Leftrightarrow&amp;\frac{\partial\sum_{k=1}^{\infty} k \frac{\lambda^k}{k!}}{\partial \lambda}=\frac{\partial \lambda e^{\lambda}}{\partial \lambda}\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k-1}}{k!}=e^\lambda+\lambda e^\lambda\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k-1}e^{-\lambda}}{k!}=1+\lambda \\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k}e^{-\lambda}}{k!}=\lambda+\lambda^2=E(X^2)\end{aligned}$$</p>
<p>then,</p>
<p>$$Var(X)=E(X^2)-(E(X))^2=\lambda+\lambda^2-(\lambda)^2=\lambda$$</p>
<p>Thus,  we have proved that the Expectation and the Variance of Poisson Distribution are both $\lambda$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/06/17/sample-variance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/17/sample-variance/" itemprop="url">样本方差为什么除以N-1?（翻译）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-17T17:52:38+02:00">
                2017-06-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>原文作者：<a href="http://www.visiondummy.com/" target="_blank" rel="noopener">Vincent Spruy</a></p>
<p>译者：程明波</p>
<p><a href="http://www.visiondummy.com/2014/03/divide-variance-n-1/" target="_blank" rel="noopener">英文文章地址</a></p>
<p><a href="http://commanber.com/2017/06/17/sample-variance/">译文地址</a></p>
<p>译者注：由于历史原因，高斯分布(Gaussian Distribution)，正态分布(Normal Distribution)皆指概率密度函数形如$\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$的分布。文中我会采用正态分布的提法。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>本文，呼应标题，我将推导著名正态分布数据均值和方差的计算公式。如果一些读者对于这个问题的“为什么”并不感兴趣，仅仅是对“什么时候使用”感兴趣，那答案就非常简单了：</p>
<p>如果你想预估一份数据的均值和方差(典型情况)，那么方差公式除的是$N-1$，即：</p>
<p>$$\sigma^2 = \frac{1}{N-1}\sum_{i=1}^N (x_i - \mu)^2$$</p>
<p>另一种情况，如果整体的真实均值已知，那么方差公式除的就是$N$，即：</p>
<p>$$\sigma^2 = \frac{1}{N}\sum_{i=1}^N (x_i - \mu)^2$$</p>
<p>然而，前一种情况，会是你遇到更典型的情形。一会儿，我会举一个预估高斯白噪音的离散程度例子。例子中高斯白噪音的均值是已知的0，这种情况下，我们只需要估计方差。</p>
<p>如果数据是正态分布，我们可以完全用均值$\mu$和方差$\sigma^2$刻画这个分布。其中，方差是标准差$\sigma$的平方，标准差代表了每个数据点偏离均值点的平均距离，也就是说，方差表示了数据离散程度。对于正态分布，68.3%的数据的值会介于$\mu-\sigma$和$\mu+\sigma$之间。下面图片展示是一个正态分布的概率密度函数，他的均值是$\mu=10$,方差是$\sigma^2=3^2=9$：</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-06-20-110159.jpg" alt></p>
<p><strong>图1.</strong> 正态分布概率密度函数. 对于正态分布数据，68%的样本落在均值$\pm$方差。</p>
<p>通常，我们拿不到全部的全体数据。上面的例子中，典型的情况是我们有一些观察数据，但是，我们没有上图中x轴上所有可能的观察数据。例如我们可能有下面一些观察数据：</p>
<p>表1</p>
<table>
<thead>
<tr>
<th>观察数据ID</th>
<th>观察值</th>
</tr>
</thead>
<tbody>
<tr>
<td>观察数据 1</td>
<td>10</td>
</tr>
<tr>
<td>观察数据 2</td>
<td>12</td>
</tr>
<tr>
<td>观察数据 3</td>
<td>7</td>
</tr>
<tr>
<td>观察数据 4</td>
<td>5</td>
</tr>
<tr>
<td>观察数据 5</td>
<td>11</td>
</tr>
</tbody>
</table>
<p>现在如果我们通过把所有值相加并除以观察的次数，得到经验均值：</p>
<p>$$\mu=\frac{10+12+7+5+11}{5}=9\tag{1}$$.</p>
<p>通常，我们会假设经验均值接近分布的未知的真实均值，因此，我们可以假设观测数据来自于均值为$\mu=9$的正态分布。在这个例子中，分布真实均值是10， 也就是说，经验均值实际上接近于真实均值。</p>
<p>数据的方差计算如下：</p>
<p>$$\begin{aligned}\sigma^2&amp;= \frac{1}{N-1}\sum_{i=1}^N (x_i - \mu)^2\&amp;= \frac{(10-9)^2+(12-9)^2+(7-9)^2+(5-9)^2+(11-9)^2}{4})\&amp;= 8.5.\end{aligned}\tag{2}$$</p>
<p>同样，我们一般假设经验方差接近于基于分布真实未知方差。在此例中，真实方差是9，所以，经验方差也是接近于真实方差。</p>
<p>那么我们手上的问题现在就是为什么我们用于计算经验均值和经验方差的公式是正确的。事实上，另一个我们经常用于计算方差的公式是这样定义的：</p>
<p>$$\begin{aligned}\sigma^2 &amp;= \frac{1}{N}\sum_{i=1}^N (x_i - \mu)^2 \&amp;= \frac{(10-9)^2+(12-9)^2+(7-9)^2+(5-9)^2+(11-9)^2}{4}) \&amp;= 6.8.\end{aligned}\tag{3}$$</p>
<p>公式(2)和公式(3)的唯一不同是前一个公式除的是$N-1$，而后一个除的是$N$。两个公式都是对的，只是根据不同的场景使用不同的公式。</p>
<p>接下来的部分，我们针对给定一个正态分布的样本集，完成对其未知方差和均值最好估计的完整推导。我们将会看到，一些情况下，方差除的是$N$，另一些情况除的是$N-1$。</p>
<p>用一个公式近似一个参数(均值或方差)叫做估计量。下面，我们定义一个分布的真实但未知的参数为$\hat{\mu}$和$\hat{\sigma}^2$。而估计量，例如，经验的平均和经验方差，定义为$\mu$和$\sigma^2$。</p>
<p>为了找到最优的估计量，首先，一个整体均值为$\mu$标准差为$\sigma$的正态分布，对于特定的观察点$x_i$，我们需要一个分析相似的表达式。对于一个已知参数的正态分布一般定义为$N(\mu,\sigma^2)$。似然函数为：</p>
<p>$$x_i \sim N(\mu,\sigma^2) \Rightarrow P(x_i; \mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}.\tag{4}$$</p>
<p>为了计算均值和方差，显然，我们需要这个分布一个以上的样本。接下来，设$\vec{x}=(x_1,x_2,\cdots,x_N)$为包含所有的可用样本的向量（例如：表一中所有的值）。如果所有这些样本统计独立，我们可以写出联合似然函数为所有似然函数的乘积：</p>
<p>$$\begin{aligned}P(\vec{x};\mu,\sigma^2)&amp;=P(x_1,x_2,\cdots,x_n;\mu,\sigma^2)\&amp;=P(x_1;\mu,\sigma^2)P(x_2;\mu,\sigma^2)\cdots P(x_N;\mu,\sigma^2)\&amp;=\prod_{i=1}^{N}P(x_i;\mu,\sigma^2)\end{aligned}.\tag{5}$$</p>
<p>把公式(4)代入公式(5)，可得出联合概率密度函数的分析表达式：</p>
<p>$$\begin{aligned}P({\vec{x};\mu,\sigma})&amp;=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x_i-\mu)^2}\&amp;=\frac{1}{(2\pi\sigma^2)^{\frac{N}{2}}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i-\mu)^2}\end{aligned}.\tag{6}$$</p>
<p>公式(6)在接下来的部分将非常重要。我们会用它推导关于正态分布著名的估计量均值和方差。</p>
<h3 id="最小方差，无偏估计量"><a href="#最小方差，无偏估计量" class="headerlink" title="最小方差，无偏估计量"></a>最小方差，无偏估计量</h3><p>决定一个估计量是不是“好”估计量，首先我们需要定义什么是真正的“好” 估计量。说一个估计量好，依赖于两个度量，叫做其偏差(bias)和方差(variance)(是的，我们要讨论均值估计量的方差，以及方差估计量的方差)。本节将简单的讨论这两个度量。</p>
<h4 id="参数偏差"><a href="#参数偏差" class="headerlink" title="参数偏差"></a>参数偏差</h4><p>想象一下，如果我们能拿到全体不同的(互斥)数据子集。类比之前的的例子，假设，除了【表1】中的数据，我们还有完全不同观察结果表2及表3。那么，一个关于均值好的估计量，应该使得这个估计量平均下来等于真实的均值。我们可以接受其中一个自己的经验均值不等于真实均值，但是，一个好的估计量应该保证：对于所有子集均值估计的平均值等于真实均值。这个限制条件用数学化的表示，就是估计量的期望值(Expected Value)应该等于参数值：</p>
<p>$$E(\mu)=\hat{\mu}\qquad E(\sigma^2)=\hat{\sigma}^2.\tag{7}$$</p>
<p>如果满足上面的条件，那么这些估计量就被称之为“无偏估计”。反之，如果上面的条件不满足，这些估计量叫做“有偏的”，也就是说平均来看，他们或者低估或者高估了参数的真实值。</p>
<h4 id="参数方差"><a href="#参数方差" class="headerlink" title="参数方差"></a>参数方差</h4><p>无偏估计量保证平均来看，它们估计的值等于真是参数。但是，这并不意味着每次估计是一个好的估计。比如，如果真实均值为10，一个无偏估计量可以估计全体的其中一个子集的均值为50，而另一个均值为-30。期望的估计的值确实是10，也等于真是的参数值，但是，估计量的质量明显依赖每次估计的离散程度。对于全体5个不同子集，一个估计量产生的估计值(10,15,5,12,8)是无偏的和另一个估计量产生的估计值（50，-30，100，-90，20）（译者注：原文作者最后一个是10，我计算换成20，这样均值才是10）。但是第一个估计量的所有估计值明显比第二个估计量的估计值更接近真实值。</p>
<p>因此，一个好的估计量不仅需要有低偏差，同时也需要低方差。这个方差表示为平均平方误差的估计量：</p>
<p>$$Var(\mu)=E[(\hat{\mu}-\mu)^2]$$</p>
<p>$$Var(\sigma^2)=E[(\hat{\sigma}-\sigma)^2]$$</p>
<p>因此一个好的估计量是低偏差，低方差的。如果存在最优的估计量，那么这个估计应该是无偏的，而且方差比所有的其他可能估计量都要低。这样的一个估计量被称之为最小方差，无偏（MVU）估计量。下一节，我们将会针对一个正态分布推导均值和方差估计量的数学表达式。我们将会看到，一个正态分布的方差MVU估计量在一些假设下需要除以$N$，而在另一些假设下需要除以$N-1$。</p>
<h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p>基于整体的一个子集，尽管有大量的获取一个参数估计量的技术，所有这些技术中最简单的可能就数最大似然估计了。</p>
<p>观察值$\vec{x}$的概率在公式(6)定义为$P(\vec{x};\mu,\sigma^2)$. 如果我们在此函数中固定$x$和$\sigma^2$，当使$\vec{x}$变化时，我们就可以获得图(1)的正态分布。但是，我们也可以固定$\vec{x}$，使$\mu$和（或）$\sigma^2$变化。比如，我们可以选择类似前面例子中的$\vec{x}=(10,12,7,5,11)$。我们选择固定$\mu=10$，同时使$\sigma^2$变化。图(2)展示了当$x$和$\mu$固定时，$\sigma^2$对于这个分布取不同值的变化曲线：</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-06-20-110329.jpg" alt></p>
<p>图 2. 此图表示了似然函数在特定观察数据$\vec{x}$，下固定$\mu=10$，$\sigma^2$变化曲线。</p>
<p>上图，我们通过固定$\mu=10$，令$\sigma^2$变化计算了$P(\vec{x};\sigma^2)$的似然函数。在结果曲线的每一个数据点代表了似然度，观察值$\vec{x}$是一个正态分布在参数$\sigma^2$下的样本。那么对应最大似然度的参数值最有可能是从我们定义的分布中产生数据的参数。因此，我们能通过找到似然度曲线的最大值决定最优的$\sigma^2$。在此例中，最大值在$\sigma^2=7.8$，这样标准差就是$\sqrt{(\sigma^2)=2.8}$。事实上，如果给定$\mu=10$，通过传统的方法计算，我们会发明方差就是7.8：</p>
<p>$$\frac{(10-10)^2+(12-10)^2+(7-10)^2+(5-10)^2+(11-10)^2}{5}=7.8$$</p>
<p>因此，基于样本数据的方差计算公式只需要简单的通过找到最大的似然函数的最高点。此外，除了固定$\mu$，我们可以使$\mu$和$\sigma^2$同时变化。然后找到两个估计量对应在两个维度的似然函数的最大值。</p>
<p>要找一个函数的最大值，也很简单，只需要求导使其等于0。如果想找一个有两个变量函数的最大值，我们需要计算每个变量的偏导，再把两个偏导全部设置为0。接下来，设$\hat{\mu}_{ML}$为通过极大似然方法得到的总体均值的最优估计量，设$\hat{\sigma}^2_ML$为方差的最优估计量。要最大化似然函数，我们可以简单的计算它的(偏)导数，然后赋值为0，如下：</p>
<p>$$\begin{aligned} &amp;\hat{\mu}<em>{ML} = \arg\max</em>\mu P(\vec{x}; \mu, \sigma^2)\ &amp;\Rightarrow \frac{\partial P(\vec{x}; \mu, \sigma^2)}{\partial \mu} = 0 \end{aligned}$$</p>
<p>及</p>
<p>$$\begin{aligned} &amp;\hat{\sigma}^2_{ML} = \arg\max_{\sigma^2} P(\vec{x}; \mu, \sigma^2)\ &amp;\Rightarrow \frac{\partial P(\vec{x}; \mu, \sigma^2)}{\partial \sigma^2} = 0 \end{aligned}$$</p>
<p>下一节，我们将利用这个技术得到$\mu$和$\sigma^2$的MVU估计量。我们考虑两种情形：</p>
<p>第一种情形，我们假设分布的真正的均值$\hat{\mu}$是已知的，因此，我们只需要估计方差，那么问题就变成在参数为$\sigma^2$的一维的极大似然函数中对应找其最大值。这种情况不经常出现，但是，在实际应用中确实存在。例如，如果我们知道一个信号(比如：一幅图中一个像素的颜色值)本来应该有特定的值，但是，信号被白噪音污染了（均值为0的高斯噪音），这时分布的均值是已知的，我们只需要估计方差。</p>
<p>第二种情形就是处理均值和方差的真实值都不知道的情况。这种情况最常见，这时，我们需要基于样本数据估计均值和方差。</p>
<p>后面我们将看到，每种情形产生不同的MVU估计量。具体来说，第一种情形方差估计量需要除以$N$来标准化MVU。而第二种除的是$N-1$。</p>
<h3 id="均值已知的方差估计"><a href="#均值已知的方差估计" class="headerlink" title="均值已知的方差估计"></a>均值已知的方差估计</h3><h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><p>如果分布的均值真实值已知，那么似然函数只有一个参数$\sigma^2$。求最大似然估计量也就是解决：</p>
<p>$$\hat{\sigma^2}<em>{ML}=\arg\max</em>{\sigma^2} P(\vec{x};\sigma^2).\tag{8}$$</p>
<p>但是，根据公式(6)的定义，如果计算$P(\vec{x};\sigma^2)$涉及到计算函数中指数的偏导。事实上，计算对数似然函数比计算似然函数本身的导数要简单的多。因为对数函数是单调递增函数，其最大值取值位置与原似然函数是一样的。因此我们用下面的式子替换：</p>
<p>$$\hat{\sigma}^2_{ML}=\arg\max_{\sigma^2}\log(P(\vec{x};\sigma^2)).\tag{9}$$</p>
<p>下面，我令$s=\sigma^2$简化式子。我们通过计算公式(6)的对数的导数赋值为0来最大化对数似然函数：</p>
<p>$$\begin{aligned}&amp;\frac{\partial \log(P(\vec{x};\sigma^2))}{\partial \sigma^2}=0\&amp;\Leftrightarrow\frac{\partial\log(P(\vec{x};s))}{\partial s}=0\&amp;\Leftrightarrow\frac{\partial}{\partial s}\log\left(\frac{1}{(2\pi s)^{\frac{N}{2}}}e^{-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2} \right)=0\&amp;\Leftrightarrow\frac{\partial}{\partial s}\log\left(\frac{1}{(2\pi)^{\frac{N}{2}}}\right)+\frac{\partial}{\partial s}\log\left(\frac{1}{\sqrt{s}^\frac{N}{2}}\right)+\frac{\partial}{\partial s} \log\left(e^{-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2}\right )=0\&amp;\Leftrightarrow0+\frac{\partial}{\partial s}\log\left((s)^{-\frac{N}{2}}\right)+\frac{\partial}{\partial s}\left(-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2\right)=0\&amp;\Leftrightarrow -\frac{N}{2}\log (s)+\frac{1}{2 s^2}\sum_{i=1}^{N}(x_i-\mu)^2=0\&amp;\Leftrightarrow -\frac{N}{2s}+\frac{1}{2s^2}\sum_{i=1}^{N}(x_i-\mu)^2=0\&amp;\Leftrightarrow \frac{N}{2s^2}\left(-s+\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2\right)=0\&amp;\Leftrightarrow\frac{N}{2s^2}\left(\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2-s\right)=0\end{aligned}$$</p>
<p>很明显，如果$N&gt;0$，那么上面等式唯一的解就是：</p>
<p>$$s=\sigma^2=\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2.\tag{10}$$</p>
<p>注意到，实际上$\hat{\sigma}^2$的极大似然估计估计量就是传统上一般计算方差的公式。这里标准化因子是$\frac{1}{N}$.</p>
<p>但是，极大似然估计并不保证得出的是一个无偏估计量。另外，就算得到的估计量是无偏的，极大似然估计也不能保证估计是最小方差，即MVU。因此，我们需要检查公式(10)的的估计量是否是无偏的。</p>
<h4 id="表现评价"><a href="#表现评价" class="headerlink" title="表现评价"></a>表现评价</h4><p>我们需要检查公式(7)的等式是否成立，来确定是否公式(10)中的估计量是无偏的。即判断：</p>
<p>$$E(s)=\hat{s}.$$</p>
<p>我们把公式(10)代入到$E(s)$，计算：</p>
<p>$$\begin{aligned}E[s] &amp;= E \left[\frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2 \right] = \frac{1}{N} \sum_{i=1}^N E \left[(x_i - \mu)^2 \right] = \frac{1}{N} \sum_{i=1}^N E \left[x_i^2 - 2x_i \mu + \mu^2 \right]\&amp;= \frac{1}{N} \left( N E[x_i^2] -2N \mu E[x_i] + N \mu^2 \right)\&amp;= \frac{1}{N} \left( N E[x_i^2] -2N \mu^2 + N \mu^2 \right)\&amp;= \frac{1}{N} \left( N E[x_i^2] -N \mu^2 \right)\end{aligned}$$</p>
<p>另外，真实方差$\hat{s}$有一个<a href="https://en.wikipedia.org/wiki/Variance#Definition" target="_blank" rel="noopener">非常重要的性质</a>为$\hat{s}=E[x_i^2]-E[x_i]^2$，可变换公式为$E[x_i^2]=\hat{s}+E[x_i]^2=\hat{s}+\mu^2$。使用此性质我们可能从上面的公式推出：</p>
<p>$$\begin{aligned}E[s]&amp;=\frac{1}{N}(N E[x_i^2]-N\mu^2)\&amp;=\frac{1}{N}(N\hat{s}+N\mu^2-N\mu^2)\&amp;=\frac{1}{N}(N\hat{s})\&amp;=\hat{s}\end{aligned}$$</p>
<p>满足了公式(7)的条件$E[s]=\hat s$，因此，我们得到的数据方差$\hat s$的统计量是无偏的。此外，因为极大似然估计的如果是一个无偏的估计量，那么也是最小方差(MVU)，也就是说，我们得到的估计量比任何一个其他的估计量都大。</p>
<p>因此，在分布真实均值已知的情况下，我们不用除以$N-1$，而是用除$N$计算正态分布的方差。</p>
<h3 id="均值未知的方差估计"><a href="#均值未知的方差估计" class="headerlink" title="均值未知的方差估计"></a>均值未知的方差估计</h3><h4 id="参数估计-1"><a href="#参数估计-1" class="headerlink" title="参数估计"></a>参数估计</h4><p>上一节，分布的真实均值已知，因此，我们只需要估计数据的方差。但是，如果真实的均值未知，我们均值的估计量就也需要计算了。</p>
<p>此外，方差的估计量需要使用均值的估计量。我们会看到，这时，之前我们得到的方差的估计量就不再无偏了。我们一会儿会通过除以N-1，而不是N来稍微的增加方差估计量的值，从而使方差估计无偏。</p>
<p>与之前一样，基于log似然函数，我们用极大似然估计计算两个估计量。首先我们先计算$\hat\mu$的极大似然估计量：</p>
<p>$$\begin{aligned}&amp;\frac{\partial \log(P(\vec{x}; s, \mu))}{\partial \mu} = 0\&amp;\Leftrightarrow \frac{\partial}{\partial \mu} \log \left( \frac{1}{(2 \pi s)^{\frac{N}{2}}} e^{-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2} \right) = 0\&amp;\Leftrightarrow \frac{\partial}{\partial \mu} \log \left( \frac{1}{(2 \pi)^{\frac{N}{2}}} \right) + \frac{\partial}{\partial \mu} \log \left(e^{-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2} \right) = 0\&amp;\Leftrightarrow \frac{\partial}{\partial \mu} \left(-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2 \right) = 0\&amp;\Leftrightarrow -\frac{1}{2s}\frac{\partial}{\partial \mu} \left(\sum_{i=1}^N(x_i - \mu)^2 \right) = 0\&amp;\Leftrightarrow -\frac{1}{2s} \left(\sum_{i=1}^N -2(x_i - \mu) \right) = 0\&amp;\Leftrightarrow \frac{1}{s} \left(\sum_{i=1}^N (x_i - \mu) \right) = 0 \&amp;\Leftrightarrow \frac{N}{s} \left( \frac{1}{N} \sum_{i=1}^N (x_i) - \mu \right) = 0 \end{aligned}$$</p>
<p>显然，如果$N&gt;0$，那么上面的等式只有一种解：</p>
<p>$$\mu=\frac{1}{N}\sum_{i=1}^{N}x_i.\tag{11}$$</p>
<p>注意到，实际的这是计算一个分布均值的著名公式。虽然我们知道这个公式，但我们现在证明了极大似然估计量估计了一个正态分布未知均值的真实值。现在我们先假定我们之前公式(10)计算的方差$\hat s$的估计量仍然是MVU方差估计量。但下一节我们会证明这个估计量已经是有偏的了。</p>
<h4 id="表现评价-1"><a href="#表现评价-1" class="headerlink" title="表现评价"></a>表现评价</h4><p>我们需要通过检查估计量$\mu$对真实$\hat \mu$的估计是否无偏来确定公式(7)的条件能否成立：</p>
<p>$$E[\mu]=E\left[\frac{1}{N}\sum_{i=1}^{N}x_i\right]=\frac{1}{N}\sum_{i=1}^N E[x_i]=\frac{1}{N}N E[x_i]=\frac{1}{N} N \hat\mu=\hat\mu.$$</p>
<p>既然$E[\mu]=\hat\mu$，那么也就是说我们对分布均值的估计量是无偏的。因为极大似然估计可以保证在估计是无偏的情况下得到的是最小方差估计量，所以我们就已经是证明了$\mu$是均值的MVU估计量。</p>
<p>现在我们检查基于经验均值$\mu$，而不是真实均值$\hat\mu$的方差估计量$s$对真实方差$\hat s$的估计身上仍然是无偏的。我们只需要把得到的估计量$\mu$带入到之前在公式(10)推导出的公式：</p>
<p>$$\begin{aligned} s &amp;= \sigma^2 = \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\&amp;=\frac{1}{N}\sum_{i=1}^N \left(x_i - \frac{1}{N} \sum_{i=1}^N (x_i) \right)^2\&amp;=\frac{1}{N}\sum_{i=1}^N \left[x_i^2 - 2 x_i \frac{1}{N} \sum_{i=1}^N (x_i) + \left(\frac{1}{N} \sum_{i=1}^N (x_i) \right)^2 \right]\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \frac{2\sum_{i=1}^N x_i \sum_{i=1}^N x_i}{N^2} + \left(\frac{\sum_{i=1}^N x_i}{N} \right)^2\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \frac{2\sum_{i=1}^N x_i \sum_{i=1}^N x_i}{N^2} + \left(\frac{\sum_{i=1}^N x_i}{N} \right)^2\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \left(\frac{\sum_{i=1}^N x_i}{N} \right)^2\end{aligned}$$</p>
<p>现在我们需要再次检查公式(7)的条件是否成立，来决定估计量是否无偏：</p>
<p>$$\begin{aligned} E[s]&amp;= E \left[ \frac{\sum_{i=1}^N x_i^2}{N} - \left(\frac{\sum_{i=1}^N x_i}{N} \right)^2 \right ]\&amp;= \frac{\sum_{i=1}^N E[x_i^2]}{N} - \frac{E[(\sum_{i=1}^N x_i)^2]}{N^2} \end{aligned}$$</p>
<p>记得我们在之前用过方差一个非常重要的性质，真实方差$\hat s$可以写成$\hat s = E[x_i^2]-E[x_i]^2$，即，$E[x_i^2]=\hat s + E[x_i]^2=\hat s +\mu^2$。利用这个性质我们可以推出：</p>
<p>$$\begin{aligned} E[s] &amp;= \frac{\sum_{i=1}^N E[x_i^2]}{N} - \frac{E[(\sum_{i=1}^N x_i)^2]}{N^2}\&amp;= s + \mu^2 - \frac{E[(\sum_{i=1}^N x_i)^2]}{N^2}\&amp;= s + \mu^2 - \frac{E[\sum_{i=1}^N x_i^2 + \sum_i^N \sum_{j\neq i}^N x_i x_j]}{N^2}\&amp;= s + \mu^2 - \frac{E[N(s+\mu^2) + \sum_i^N \sum_{j\neq i}^N x_i x_j]}{N^2}\&amp;= s + \mu^2 - \frac{N(s+\mu^2) + \sum_i^N \sum_{j\neq i}^N E[x_i] E[x_j]}{N^2}\&amp;= s + \mu^2 - \frac{N(s+\mu^2) + N(N-1)\mu^2}{N^2}\&amp;= s + \mu^2 - \frac{N(s+\mu^2) + N^2\mu^2 -N\mu^2}{N^2}\&amp;= s + \mu^2 - \frac{s+\mu^2 + N\mu^2 -\mu^2}{N}\&amp;= s + \mu^2 - \frac{s}{N} - \frac{\mu^2}{N} - \mu^2 + \frac{\mu^2}{N}\&amp;= s - \frac{s}{N}\&amp;= s \left( 1 - \frac{1}{N} \right)\&amp;= s \left(\frac{N-1}{N} \right) \end{aligned}$$</p>
<p>显然$E[s]\neq\hat s$，上面公式可知分布的方差估计量不再是无偏的了。事实上，平均来看，这个估计量低估了真实方差，比例为$\frac{N-1}{N}$。当样本的数量趋于无穷时($N\rightarrow\infty$)，这个偏差趋近于0。但是对于小的样本集，这个偏差就意义了，需要被消除。</p>
<h4 id="修正偏差"><a href="#修正偏差" class="headerlink" title="修正偏差"></a>修正偏差</h4><p>因为偏差不过是一个因子，我们只需通过对公式(10)的估计量乘以偏差的倒数。这样我们就可以定义一个如下的无偏的估计量$s\prime$：</p>
<p>$$\begin{aligned} s\prime &amp;= \left ( \frac{N-1}{N} \right )^{-1} s\s\prime &amp;= \left ( \frac{N-1}{N} \right )^{-1} \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\s\prime &amp;=\left ( \frac{N}{N-1} \right ) \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\s\prime &amp;= \frac{1}{N-1}\sum_{i=1}^N(x_i - \mu)^2\end{aligned}$$</p>
<p>  这个估计量现在就是无偏的了，事实上，这个公式与传统计算方差的公式非常像，不同的是除的是$N-1$而不是$N$。然而，你可能注意到这个估计量不再是最小方差估计量，但是这个估计量是所有无偏估计量中最小方差的一个。如果我们除以$N$，那么估计量就是有偏的了，如果我们除以$N-1$，估计量就不是最小方差估计量。但大体来说，一个有偏的估计量要比一个稍高一点方差的估计量要糟糕的多。因此，如果当总体的均值是未知的情况下，方差除的是$N-1$，而不是$N$。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文，我们推导了如果从分布数据中计算常见的方差和均值公式。此外，我们还证明了在方差估计中，标准化因子在总体均值已知时是$\frac{1}{N}$，在均值也需要估计时是$\frac{1}{N-1}$。</p>
<p><a href="http://cmb.oss-cn-qingdao.aliyuncs.com/%E6%A0%B7%E6%9C%AC%E6%96%B9%E5%B7%AE%E4%B8%BA%E4%BB%80%E4%B9%88%E9%99%A4%E4%BB%A5N-1%3F%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89.pdf" target="_blank" rel="noopener">本文PDF</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/05/21/decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/05/21/decision-tree/" itemprop="url">Decision Tree (ID3)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-21T15:10:50+02:00">
                2017-05-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p>In April 9th, 2017, incident occurred in United Airlines where crew of UA beat up a passenger and dragged him out of the plane before which was about to take off attracted attention all around the world. Many would gave out doubt: why a company being so rude to passengers can exist in this world? Actually, UA is going well is just because they have an extremely precise emergency situation procedure which is calculate by compute depending on big-data analysis. Computer can help us make decisions though, it has no emotions, which is effective in most cases, but can not be approved by our human beings. Let’s take a look at how algorithm make a decision:<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-United%20Airlines.png" alt><br>It is a decision tree, which simply represents the procedure of how UA algorithm make the decision. First of all, before taking off, four employees of UA need fly from Chicago to Kentucky. Then the algorithm check if there is any seats left, if so, passengers were safe for the moment. But UA3411 was full, the algorithm began assessing the importance of employees or passengers. Obviously, the algorithm think crew is more important due to business consideration. Then how to choose who should be evicted from the plane. The algorithm was more complicated than the tree I drew, however, Asian or not was one of the criterion. But why? Because Asian are pushovers. The passenger agreed at first, however, when he heard that he had to wait for one day, he realized that he could not treat his patient, then he refused. Then he was beat up and dragged off the plane.</p>
<p>As you have seen, it is a decision tree, which is similar to human decision-making process. Decision tree is a simple but powerful algorithm in machine learning. In fact, you are often using decision tree theory when making decision, for example<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-homework.png" alt></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Decision tree is a classification and regression algorithm, we build a tree through statistics. Today we only talk about how to classify dataset using Decision Tree. First we will introduce some information theory background knowledge, then we use iris data build a decision tree using IDC3 algorithm.</p>
<h2 id="Iris-data"><a href="#Iris-data" class="headerlink" title="Iris data"></a>Iris data</h2><p><a href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="noopener">Iris dataset</a> is a very famous dataset deposited on UCI machine learning repository, which described three kinds of iris. there are four columns corresponding for features as followed：</p>
<ul>
<li>sepal length in cm</li>
<li>sepal width in cm</li>
<li>petal length in cm</li>
<li>petal width in cm</li>
</ul>
<p>The last column represents iris categories:</p>
<ul>
<li>Iris-setosa (n=50)</li>
<li>Iris-versicolor (n=50)</li>
<li>Iris-virginica (n=50)</li>
</ul>
<p>Here, our task is to use the dataset to train a model and generate a decision tree.  During the process we need calculate some statistics values to decide how to generate a better one.</p>
<p>The dataset is very small so that you can easily download it and take a look.</p>
<h2 id="Entropy-and-Information-Gain"><a href="#Entropy-and-Information-Gain" class="headerlink" title="Entropy and Information Gain"></a>Entropy and Information Gain</h2><h4 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h4><p>Before Decision Tree, I’d like to talk about some concept in Information Theory. Entropy is a concept from thermodynamics at first, C.E.Shannon introduced which into information theory which represent redundancy in 1948. It sounds a very strange concept. In fact, it is very easy to understand. For example, during the knockout stages in world Cup Games, there are 16 teams. Now I let you guess which team will win the champion which assume I know the answer, how many times do you need to get the outcome? First of all, you cut 16 teams to 8-8 parts, you asked me if the team in first 8 teams or the other. I told you that the team was in the other 8 teams. Then you cut the the 8 teams again, you ask me if the team is in the first 4 teams or the other, I told you that the champion would be in the first 4 teams, and so forth and so on. And how many times is the entropy of who wining the champion.</p>
<p>$$ Entropy(champion) = {\rm log}_2^{16}=4 $$</p>
<p>That is, we can use 4 bits to represents which team will win the game. Clever you may ask why we divide team to two parts other than three or four parts. That is because we use binary represents the world in computer world. $ 2^4=16 $ means we can use 4 bit represents 16 conditions. We can use entropy represent all information in this world. And if you have known that which team will win the campion, the entropy is 0, because, you do not need any more information to deduce the outcome.</p>
<p>Entropy represents uncertainty indeed. Ancient China, we have to record history on bamboo    slips, which demanded us decrease words. That means entropy of every single ancient Chinese character is higher than words we are saying today. That is, if we lost just some of these words, we would lose lots of stories. There are many songs starts with:”Yoo, yoo, check now”, which barely offer us information, which means we can drop those words and interpret the these songs precisely as well. The entropy of these sentence is low.</p>
<p>Assume $X$ is discrete random variable, the distribution is:<br>$$P(X=x_i)=p_i$$<br>then the entropy of X is:<br>$$H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2 p_i$$<br>where if p_0=0, we define 0log0 = 0.</p>
<p>It seems that the equation has nothing to do with the entropy we have calculated in the champion example. Now let’s calculate the example. First of all $X$ represents the probability of each team which would win the game. we assume all teams were at the same level, so we have<br>$$p(X=x_1)=p(X=x_2)=p(X=x_3)=\cdots = p(X=x_{16})=\frac{1}{16}$$<br>the entropy is<br>$$H(X)=-\sum_{i=1}^{16}\frac{1}{16}{\rm log}_2 \frac{1}{16}=-16\times\frac{1}{16}\times {\rm log}_2 {2^{-4}}=4$$</p>
<p>Bingo, the the answer is same. In fact, if we know some more information, the entropy is lower than 4. for example, the probability of Germany is higher than some Asian teams.</p>
<h4 id="Entropy-and-Iris-Data"><a href="#Entropy-and-Iris-Data" class="headerlink" title="Entropy and Iris Data"></a>Entropy and Iris Data</h4><p>Now we calculate entropy of Iris Data which will be used to fit a decision tree in following sections. We concern about the categories(setosa, versicolor and virginica). Remember the equation of how to calculate entropy:<br>$$H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2 p_i$$</p>
<p>Three kinds of flowers are all 50s, so the probability of each category is the same:<br>$$p_1=p_2=p_3=\frac{50}{50+50+50}=\frac{1}{3}$$<br>Then, the entropy is pretty easy to calculate<br>$$H(X)=-1\times (\frac{1}{3}{\rm log}_2\frac{1}{3}+\frac{1}{3}{\rm log}_2\frac{1}{3}+\frac{1}{3}{\rm log}_2\frac{1}{3})=1.5850$$</p>
<h4 id="Conditional-Entropy"><a href="#Conditional-Entropy" class="headerlink" title="Conditional Entropy"></a>Conditional Entropy</h4><p>The meaning of Conditional Entropy is as its name.  With respect with random variable$(X, Y)$, the joint distribution is<br>$$P(X=x_i, Y=y_j)=p_{ij}, i=1,2,3\cdots m; j=1,2,3,\cdots n$$<br>Conditional Entropy H(Y|X) represents that given we have known random variable $X$ , the disorder or uncertainty of $Y$.  The definition is as followed:<br>$$H(Y|X)=\sum_{i=1}^m p_i H(Y|X=x_i)$$<br>Here, $p_i=P(X=x_i)$.</p>
<h4 id="Conditional-Entropy-and-Iris-Data"><a href="#Conditional-Entropy-and-Iris-Data" class="headerlink" title="Conditional Entropy and Iris Data"></a>Conditional Entropy and Iris Data</h4><p>We calculate some Conditional Entropy as examples. First of all, I random choose 15 columns of sepal length with respect to their categories. the result is as followed：</p>
<table>
<thead>
<tr>
<th>No.</th>
<th>sepal length in cm</th>
<th>categories</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr>
<td>2</td>
<td>7.20</td>
<td>Iris-virginica</td>
</tr>
<tr>
<td>3</td>
<td>5.00</td>
<td>Iris-versicolor</td>
</tr>
<tr>
<td>4</td>
<td>5.00</td>
<td>Iris-setosa</td>
</tr>
<tr>
<td>5</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr>
<td>6</td>
<td>5.70</td>
<td>Iris-setosa</td>
</tr>
<tr>
<td>7</td>
<td>5.20</td>
<td>Iris-versicolor</td>
</tr>
<tr>
<td>8</td>
<td>5.50</td>
<td>Iris-versicolor</td>
</tr>
<tr>
<td>9</td>
<td>4.80</td>
<td>Iris-setosa</td>
</tr>
<tr>
<td>10</td>
<td>4.60</td>
<td>Iris-setosa</td>
</tr>
<tr>
<td>11</td>
<td>6.50</td>
<td>Iris-versicolor</td>
</tr>
<tr>
<td>12</td>
<td>5.20</td>
<td>Iris-setosa</td>
</tr>
<tr>
<td>13</td>
<td>7.70</td>
<td>Iris-virginica</td>
</tr>
<tr>
<td>14</td>
<td>6.40</td>
<td>Iris-virginica</td>
</tr>
<tr>
<td>15</td>
<td>6.00</td>
<td>Iris-versicolor</td>
</tr>
</tbody>
</table>
<p>The octave code is<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%% octave</span><br><span class="line">[a,b,c,d, cate] = textread(<span class="string">"iris.data"</span>, <span class="string">"%f%f%f%f%s"</span>,<span class="string">"delimiter"</span>, <span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>:<span class="number">15</span></span><br><span class="line">  <span class="keyword">x</span> = floor(<span class="keyword">rand</span>()*<span class="number">150</span>);</span><br><span class="line">  fprintf(<span class="string">'%f %s\n'</span>, a(<span class="keyword">x</span>), cate<span class="string">&#123;x&#125;</span> );</span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p>We just take this 15 items for examples, I assume that we divide sepal length into two parts: greater than mean and less than mean. The mean is<br>$$mean = (5.90+7.2+\cdots+6.00)/15 = 5.7733$$<br>There are 8 elements less then 5.7733 and 7 bigger ones. That is</p>
<table>
<thead>
<tr>
<th>mean</th>
<th>idx of greater than mean</th>
<th>idx of less than mean</th>
</tr>
</thead>
<tbody>
<tr>
<td>5.7733</td>
<td>1,2,5,11,13,14,15</td>
<td>3,4,6,7,8,9,10,12</td>
</tr>
</tbody>
</table>
<p>We let $x_1=greater$(1,2,5,11,13,14,15),  $x_2=less$(3,4,6,7,8,9,10,12) then<br>$$H(Y|X=x_1)=-(p_1 {\rm log}_2 p_1 + p_2 {\rm log}_2 p_2 + p_3 {\rm log}_2 p_3)=\frac{4}{7}{\rm log}_2\frac{4}{7}+\frac{3}{7}{\rm log}_2\frac{3}{7}+0{\rm log}_2 0=0.98523$$<br>$$H(Y|X=x_2)=-(p_1 {\rm log}_2 p_1 + p_2 {\rm log}_2 p_2+p_3 {\rm log}_2 p_3)=\frac{3}{8}{\rm log}_2\frac{3}{8}+0{\rm log}_2 0+\frac{5}{8}{\rm log}_2\frac{5}{8}=0.95443$$</p>
<p>The Conditional Entropy then is<br>$$H(Y|X)=\sum_{i=1}^{2}p_i H(Y|x_i)=\frac{7}{15}\times 0.98523+\frac{8}{15}\times 0.95443=0.96880$$</p>
<h4 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h4><p>Just as its name implies, Information Gain means the information we have gained after adding some features. That is, we can vanish some uncertainty when we add some information. For example, I want you to guess an NBA player, the uncertainty is very high, however, there are only several persons in the list if I tell you that he is a Chinese. You gained information after knowing the Chinese feature to decrease the uncertainty. The calculation of Information Gain is<br>$$IG(Y, X)= H(Y)-H(Y|X)$$<br>Here, we want to decide $Y$ with feature $X$. It is easy, just Entropy of $Y$ minus Conditional Entropy $Y$ given $X$. The meaning is obvious too: $H(Y)$ represents uncertainty, $H(Y|X)$ represents uncertainty of $Y$ given $X$, the difference is the Information Gain.</p>
<h4 id="Information-Gain-and-Iris-Data"><a href="#Information-Gain-and-Iris-Data" class="headerlink" title="Information Gain and Iris Data"></a>Information Gain and Iris Data</h4><p>In this section, I will apply Information Gain equations to the whole Iris data. First of all, let $Y$ represent categories of iris, and $X_1,X_2,X_3, X_4$ represent sepal length, sepal width, petal length petal width respectively.</p>
<p>We have computed that $H(Y)=1.0986$, next, we will calculate 4 Conditional Entropy $H(Y|X_1),H(Y|X_2),H(Y|X_3),H(Y|X_4)$. In light of continuousness of $X$, we divide them by mean of each feature. Then<br>$$\overline{X_1}=5.8433,\,\overline{X_2}=3.0540,\,\overline{X_3}=3.7587,\,\overline{X_4}=1.1987$$</p>
<p>$$H(Y|X_1)=-\sum_{i=1}^3 p_i H(Y|X_{1i})=-(\frac{70}{150}(\frac{0}{70}{\rm log}_2\frac{0}{70}+\frac{26}{70}{\rm log}_2\frac{26}{70} +\frac{44}{70}{\rm log}_2\frac{44}{70})+\frac{80}{150}(\frac{50}{80}{\rm log}_2\frac{50}{80}+\frac{24}{80}{\rm log}_2\frac{24}{80}+\frac{6}{80}{\rm log}_2\frac{6}{80}))=1.09757$$</p>
<p>$$H(Y|X_2)=-\sum_{i=1}^3 p_i H(Y|X_{2i})=-(\frac{67}{150}(\frac{42}{67}{\rm log}_2\frac{42}{67}+\frac{8}{67}{\rm log}_2\frac{8}{67}+\frac{17}{67}{\rm log}_2\frac{17}{67}+\frac{83}{150}(\frac{8}{83}{\rm log}_2\frac{8}{83}+\frac{42}{83}{\rm log}_2\frac{42}{83}+\frac{33}{83}{\rm log}_2\frac{33}{83}))=1.32433$$</p>
<p>$$H(Y|X_3)=-\sum_{i=1}^3 p_i H(Y|X_{3i})=-(\frac{93}{150}(\frac{0}{93}{\rm log}_2\frac{0}{93}+\frac{43}{93}{\rm log}_2\frac{43}{93}+\frac{50}{93}{\rm log}_2\frac{50}{93}+\frac{57}{150}(\frac{50}{57}{\rm log}_2\frac{50}{57}+\frac{7}{57}{\rm log}_2\frac{7}{57}+\frac{0}{57}{\rm log}_2\frac{0}{57}))=0.821667$$</p>
<p>$$H(Y|X_4)=-\sum_{i=1}^3 p_i H(Y|X_{4i})=-(\frac{90}{150}(\frac{0}{90}{\rm log}_2\frac{0}{90}+\frac{40}{90}{\rm log}_2\frac{40}{90}+\frac{50}{90}{\rm log}_2\frac{50}{90}+\frac{60}{150}(\frac{50}{60}{\rm log}_2\frac{50}{60}+\frac{10}{60}{\rm log}_2\frac{10}{60}+\frac{0}{60}{\rm log}_2\frac{0}{60}))=0.854655<br>$$<br>Information Gains is easy to get<br>$$IG(Y, X_1)=H(Y)-H(Y|X_1)=1.5850-1.09757=0.487427$$</p>
<p>$$IG(Y, X_2)=H(Y)-H(Y|X_2)=1.5850-1.32433=0.260669$$</p>
<p>$$IG(Y, X_3)=H(Y)-H(Y|X_3)=1.5850-0.821667=0.763333$$</p>
<p>$$IG(Y, X_4)=H(Y)-H(Y|X_4)=1.5850-0.854655=0.730345$$<br>By now, we find that $IG(Y, X_3)$ is bigger than others, which means feature $X_3$ supplies more information.</p>
<h2 id="ID3-Iterative-Dichotomiser-3"><a href="#ID3-Iterative-Dichotomiser-3" class="headerlink" title="ID3(Iterative Dichotomiser 3)"></a>ID3(Iterative Dichotomiser 3)</h2><p>ID3 algorithm was developed by Ross Quinlan in 1986, which is a very classic algorithm as well as C4.5 and CART. We First apply Information Gain of each feature with respect to iris data. Then to choose the maximum to divide data into 2 parts. For each part we apply Information Gain recursively until we put all parents data to one node. Now that we have know Information Gain from the last section, obviously we choose X3 as the feature dividing data into 2 parts in the first place.</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-17-2.png" alt></p>
<p>Let’s take a look at the first cut using feature $X_3$. We have 150 items at first, after comparing if $X_3&gt;3.7587$, we divide data into two parts, one has 93 items, the other got 57. From the data, we know that there is no setosa in node B, meanwhile, no virginica in node C, which means that this feature is very good for split data due to exclude setosa and virginica.</p>
<table>
<thead>
<tr>
<th></th>
<th>Node B</th>
<th>Node C</th>
</tr>
</thead>
<tbody>
<tr>
<td>setosa</td>
<td>0</td>
<td>50</td>
</tr>
<tr>
<td>versicolor</td>
<td>43</td>
<td>7</td>
</tr>
<tr>
<td>virginica</td>
<td>50</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The end condition of the algorithm is decided by IG. When IG is less then some threshold or if there is only one category left, we can end the algorithm. If IG less than some value(e.g. 0.01) and more than one category left simultaneously, we have to choose a final category to be the leaf, the rule is to set the category having samples more than the others.</p>
<p>Take Node H for example, we set IG threshold to 0.01 in the first place. Then we calculate the Information Gain for each feature, the biggest IG from feature 2(sepal width in cm), which is 0.003204 and less than 0.01. So we have to set H as a leaf. There are 0 Iris-setosa, 25 Iris-versicolor and 44  Iris-virginica in the leaf, so we set the bigger one(i.e. Iris-virginica) to the leaf.</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-19-2.png" alt></p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Today we have talked about what is decision tree algorithm. Firstly, I introduce three background concept Entropy, Conditional Entropy and Information Gain. Next we apply ID3 algorithm to Iris data to build a decision.</p>
<p>One of the most significant advantages of decision tree is that we can explain the result. If the algorithm decided UA should beat the their passengers, they could trace the tree to find the path of reason chain. It is very useful to tell consumers why we recommend them something, under such circumstance, we can use decision tree to train a model.  </p>
<p>There is a shortcoming that Information Gain tends to use feature with more values. In order to resolve the problem, Ross Quinlan improved the algorithm through Information Gain Rate Rather than IG. <a href="https://en.wikipedia.org/wiki/Leo_Breiman" target="_blank" rel="noopener">Breiman</a> introduced CART algorithm subsequently, which can be applied to classification as well as regression. Recently, Scientists have developed more powerful algorithm such as Random Forest and Gradient Boosting Decision Tree etc.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>《统计学习方法》，李航</li>
<li>《数学之美》，吴军</li>
<li><a href="http://www.shogun-toolbox.org/static/notebook/current/DecisionTrees.html" target="_blank" rel="noopener">http://www.shogun-toolbox.org/static/notebook/current/DecisionTrees.html</a></li>
<li><a href="https://en.wikipedia.org/" target="_blank" rel="noopener">https://en.wikipedia.org/</a></li>
</ol>
<h2 id="Appendix-code"><a href="#Appendix-code" class="headerlink" title="Appendix code"></a>Appendix code</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% octave main function file</span></span><br><span class="line"><span class="comment">%% iris data dowload link: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</span></span><br><span class="line">[a,b,c,d, cate] = textread(<span class="string">"iris.data"</span>, <span class="string">"%f%f%f%f%s"</span>,<span class="string">"delimiter"</span>, <span class="string">","</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">%for i=1:15</span></span><br><span class="line"><span class="comment">%	x = floor(rand()*150);</span></span><br><span class="line"><span class="comment">%	fprintf('%f %s\n', a(x), cate&#123;x&#125; );</span></span><br><span class="line"><span class="comment">%end;</span></span><br><span class="line"></span><br><span class="line">features = [a, b, c, d];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="built_in">length</span>(features(<span class="number">1</span>, :))</span><br><span class="line">	col = features(:, <span class="built_in">i</span>);</span><br><span class="line">	me = <span class="built_in">mean</span>(col);</span><br><span class="line">	<span class="built_in">disp</span>(me);</span><br><span class="line">	feat(<span class="built_in">i</span>).greater = <span class="built_in">find</span>(col &gt; me);</span><br><span class="line">	feat(<span class="built_in">i</span>).less = <span class="built_in">find</span>(col &lt;= me);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">total = (<span class="number">1</span>:<span class="number">150</span>)';</span><br><span class="line">decision(feat, <span class="built_in">length</span>(features(<span class="number">1</span>, :)), cate, total);</span><br><span class="line">fprintf(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% octave: decsion tree file</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">decision</span><span class="params">(feat, feat_size, cate, total)</span></span></span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">length</span>(total) == <span class="number">0</span></span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line">	fprintf(<span class="string">'(-%d-)'</span>, <span class="built_in">length</span>(total));</span><br><span class="line">	<span class="comment">%plogp = @(x)[x*log2(x)];</span></span><br><span class="line">	<span class="function"><span class="keyword">function</span> <span class="title">e</span> = <span class="title">plogp</span><span class="params">(pi)</span></span></span><br><span class="line">		<span class="keyword">if</span> <span class="built_in">pi</span> == <span class="number">0</span></span><br><span class="line">			e = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">			e = <span class="built_in">pi</span>*<span class="built_in">log2</span>(<span class="built_in">pi</span>);</span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">function</span> <span class="title">d</span> = <span class="title">div</span><span class="params">(a, b)</span></span></span><br><span class="line">		<span class="keyword">if</span> b == <span class="number">0</span></span><br><span class="line">			d = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">			d = a/b;</span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">	debug = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">function</span> <span class="title">m</span> = <span class="title">maxc</span><span class="params">(cate, cates, total)</span></span></span><br><span class="line">		maxidx = <span class="number">1</span>;</span><br><span class="line">		max_c = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:<span class="built_in">length</span>(cates)</span><br><span class="line">			c =<span class="built_in">find</span>(strcmp(cate, cates&#123;<span class="built_in">i</span>&#125;));</span><br><span class="line">			cl = <span class="built_in">length</span>(<span class="built_in">intersect</span>(c, total));</span><br><span class="line">			<span class="keyword">if</span> debug == <span class="number">1</span> fprintf(<span class="string">'\n%d##%d  %s###'</span>,<span class="built_in">i</span>, cl, char(cates&#123;<span class="built_in">i</span>&#125;)) <span class="keyword">end</span></span><br><span class="line">			<span class="comment">%if (debug == 1 &amp;&amp; cl &lt;10 &amp;&amp; cl &gt;0) disp(intersect(c, total)') end</span></span><br><span class="line">			<span class="keyword">if</span> cl &gt; max_c</span><br><span class="line">				max_c = cl;</span><br><span class="line">				maxidx = <span class="built_in">i</span>;</span><br><span class="line">			<span class="keyword">end</span></span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">		<span class="keyword">if</span> debug == <span class="number">1</span> fprintf(<span class="string">'\n****%d    %d******\n'</span>, maxidx, max_c) <span class="keyword">end</span></span><br><span class="line">		<span class="comment">%m = cates(maxidx);</span></span><br><span class="line">		m = maxidx;</span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">% compute h(y)</span></span><br><span class="line">	cates = unique(cate);</span><br><span class="line">	hx = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">length</span>(cates)</span><br><span class="line">		c = <span class="built_in">find</span>(strcmp(cate, cates&#123;<span class="built_in">i</span>&#125;));</span><br><span class="line">		rc = <span class="built_in">intersect</span>(c, total);</span><br><span class="line">		hx -= plogp(<span class="built_in">length</span>(rc)/<span class="built_in">length</span>(total));</span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line">	<span class="comment">%fprintf('hx = %f\n', hx)			</span></span><br><span class="line">	<span class="comment">% compute h(y|x)</span></span><br><span class="line">	max_feature = <span class="number">1</span>;</span><br><span class="line">	max_ig = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">	max_left = <span class="built_in">intersect</span>(feat(<span class="number">1</span>).greater, total);</span><br><span class="line">	max_right = <span class="built_in">intersect</span>(feat(<span class="number">1</span>).less, total);</span><br><span class="line">	<span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:feat_size</span><br><span class="line">		hxh = <span class="number">0</span>;</span><br><span class="line">		hxl = <span class="number">0</span>;</span><br><span class="line">		feat_greater = <span class="built_in">intersect</span>(feat(<span class="built_in">i</span>).greater, total);</span><br><span class="line">		feat_less = <span class="built_in">intersect</span>(feat(<span class="built_in">i</span>).less, total);</span><br><span class="line">		ge = <span class="built_in">length</span>(feat_greater);</span><br><span class="line">		le = <span class="built_in">length</span>(feat_less);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (ge+le) == <span class="number">0</span></span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">length</span>(cates);</span><br><span class="line">			c = <span class="built_in">find</span>(strcmp(cate, cates&#123;<span class="built_in">j</span>&#125;));</span><br><span class="line">			xh = <span class="built_in">length</span>(<span class="built_in">intersect</span>(feat_greater, c));</span><br><span class="line">			xl = <span class="built_in">length</span>(<span class="built_in">intersect</span>(feat_less, c));</span><br><span class="line">			hxh -= plogp(div(xh, ge));</span><br><span class="line">			hxl -= plogp(div(xl, le));</span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line">		<span class="comment">% compute hx - h(y|x)</span></span><br><span class="line">		hxy = (ge/(ge+le))*hxh + ((le)/(ge+le))*hxl;</span><br><span class="line">		ig = hx - hxy;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> ig &gt; max_ig</span><br><span class="line">			max_ig = ig;</span><br><span class="line">			max_feature = <span class="built_in">i</span>;</span><br><span class="line">			max_left= feat_less;</span><br><span class="line">			max_right = feat_greater;</span><br><span class="line">		<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">	left = max_left;</span><br><span class="line">	right = max_right;</span><br><span class="line">	<span class="comment">%fprintf('feature:ig  %d %f %d %d ------ \n', max_feature, max_ig, length(left), length(right));</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> debug == <span class="number">1</span> printf(<span class="string">"\033[0;32;1m-ig--%f \033[0m"</span>,  max_ig); <span class="keyword">end</span></span><br><span class="line">	<span class="keyword">if</span>(max_ig &lt; <span class="number">0.01</span>)</span><br><span class="line">		<span class="comment">%fprintf('&lt;%s&gt;', char(maxc(cate, cates, total)))</span></span><br><span class="line">		printf(<span class="string">"\033[0;31;1m&lt;%d&gt;\033[0m"</span>,  maxc(cate, cates, total));</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	<span class="keyword">end</span></span><br><span class="line">	fprintf(<span class="string">"\033[0;34;1m#%d \033[0m"</span>,  max_feature);</span><br><span class="line">	fprintf(<span class="string">'&#123;'</span> )</span><br><span class="line">	decision(feat, feat_size, cate, left);</span><br><span class="line">	decision(feat, feat_size, cate, right);</span><br><span class="line">	fprintf(<span class="string">'&#125;'</span>)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/" itemprop="url">A Tutorial on Singular Value Decomposition</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-01T21:49:00+02:00">
                2017-05-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h3><p>Under some circumstance, we want to compress data to save storage space. For example, when iPhone7 was released, many were trapped in a dilemma: Should I buy a 32G iPhone without enough free space or that of 128G with a lot of storage being wasted? I had been trapped in such dilemma indeed. I still remember that I only had 8G storage totally when I was using my first Android phone. What annoyed me most was my thousands of photos. Well, I confess that I was being always a mad picture taker. I knew that there were some technique which could compress a picture through reducing pixel. However, it is not enough, because, as you know, in some arbitrary position in a picture, we can tell that the picture share the same color. An extreme Example: if we have a pure color picture, what we just need know is the RGB value and the size, then reproducing the picture is done without extra effort. What I was dreaming is done perfectly by Singular Value Decomposition(SVD).</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Before SVD, in this article, I will introduce some mathmatical concepts in the first place which cover Linear transformation and EigenVector&amp;EigenValue. This Background knowledge is meant to make SVD straightforward. You can skip if you are familar with this knowledge.</p>
<h3 id="Linear-transformation"><a href="#Linear-transformation" class="headerlink" title="Linear transformation"></a>Linear transformation</h3><p>Given a matrice $A$ and vector $\vec{x}$, we want to compute the mulplication of $A$ and $\vec{x}$ </p>
<p>$$\vec{x}=\begin{pmatrix}1\3\end{pmatrix}\qquad A=\begin{pmatrix}2 &amp; 1 \\ -1 &amp; 1 \end{pmatrix}\qquad\vec{y}=A\vec{x}$$</p>
<p>But when we do this multiplication, what happens? Acutually, when we multiply $A$ and $\vec{x}$, we are changing the coordinate axes of the vector $x$ to another new axes. Begin with a simpler example, we let</p>
<p>$$A=\begin{pmatrix}1 &amp; 0\\ 0 &amp;1\end{pmatrix}$$</p>
<p>then we have<br>$$A\vec{x}=\begin{pmatrix}1 &amp; 0\\ 0 &amp;1\end{pmatrix}\begin{pmatrix}1\3\end{pmatrix}=\begin{pmatrix}1\3\end{pmatrix}$$</p>
<p>You may have noticed that we can always get the same $\vec{x}$ after left multiply by A. In this case, we use coordinate axes $i=\begin{pmatrix}1 \\ 0\end{pmatrix}$ and $j=\begin{pmatrix}0 \\ 1\end{pmatrix}$ as the figure below demonstrated. That is, if we want to represent $\begin{pmatrix}1\3\end{pmatrix}$ under the coordination, we can calculate the transformation as followed:</p>
<p>\begin{align} A\vec{x}=1\cdot i + 3\cdot j = 1\cdot \begin{pmatrix}1 \\ 0\end{pmatrix} + 3\cdot \begin{pmatrix}0 \\ 1\end{pmatrix}=\begin{pmatrix}1\3\end{pmatrix}\end{align}</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-073041.jpg" alt></p>
<p>As we know, we can put a vector to anywhere in space, and if we want to calculate sum of two vectors, the simplest way is to connect the to vector from one’s head to the other’s tail. Our example, we compute $A\vec{x}$ means add two vector(green imaginary lines) up. And the answer is still $\begin{pmatrix}1\3\end{pmatrix}$.</p>
<p>Now we change $i=\begin{pmatrix}2\\ -1\end{pmatrix}$ and $j=\begin{pmatrix}1\1\end{pmatrix}$ as the coordinate axes(the red vectors), which means $A=\begin{pmatrix}2 &amp; 1 \\ -1 &amp; 1\end{pmatrix}$. I put vectors(black ones) to this figure as well. We can see what happens when we change a new coordinate axes.</p>
<p>First of all, we multiply $j$ by $3$ and $i$ by 1. Then we move vector j and let the head of $i$ connect the tail of $3\cdot j$. We can now find what is the coordination of $1\cdot i+3\cdot j$(the blue one). We now verify the result using mutiplication of $A$ and $\vec{x}$:</p>
<p>$$A\vec{x}=\begin{pmatrix}2 &amp; 1 \\ -1 &amp; 1\end{pmatrix}\begin{pmatrix}1\3\end{pmatrix}=1\cdot \begin{pmatrix}2 \\ -1\end{pmatrix} + 3\cdot  \begin{pmatrix}1 \\ 1\end{pmatrix}=\begin{pmatrix}5\2\end{pmatrix}$$</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-013710.jpg" alt><br>Here, you can imagine that matrice $A$ is just like a function $f(x)\rightarrow y$, when you subsitute $x$, we get the exact $y$ using the principle $f(x)\rightarrow y$. In fact, the multiplication is tranform the vector from one coordination to another. </p>
<h4 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h4><ol>
<li>$A=\begin{pmatrix}1 &amp; 2 \\ 3 &amp; 4\end{pmatrix}$, draw the picture to stretch and rotate $x=\begin{pmatrix}1\3\end{pmatrix}$.</li>
<li>Find a $A$ matrix to rotate $\vec{x}=\begin{pmatrix}1\3\end{pmatrix}$ to $90^{\circ}$ and $180^{\circ}$. </li>
<li>what if $A=\begin{pmatrix}1 &amp; 2 \\ 2 &amp; 4\end{pmatrix}$.</li>
</ol>
<h3 id="EigenVector-and-EigenValue"><a href="#EigenVector-and-EigenValue" class="headerlink" title="EigenVector and EigenValue"></a>EigenVector and EigenValue</h3><p>EigenVector and EigenValue is an extremely important concept in linear algebra, and is commonly used everywhere including SVD we are talking today. However, many do not know how to interpret it. In fact, EigenVector and EigenValue is very easy as long as we know about what is linear transformation. </p>
<h4 id="A-Problem"><a href="#A-Problem" class="headerlink" title="A Problem"></a>A Problem</h4><p>Before start, let’s take a look at a question: if we want to multiply matrices for 1000 times, how to calculate effectively?<br>$$AAA\cdots A= \begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}\begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}\begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}\cdots \begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}$$</p>
<h4 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h4><p>Last section, we have talked that if we multiply a vector by a matrix $A$, means that we use $A$ to stretch and rotate the vector in order to represent the vector in a new coordinate axes. However, there are some vectors for $A$, they can only be stretched but can not be rotated. Assume $A=\begin{pmatrix}3 &amp; 1 \\ 0 &amp; 2\end{pmatrix}$, let $\vec{x}=\begin{pmatrix}1 \\ -1\end{pmatrix}$. When we multiply $A$ and $\vec{x}$</p>
<p>$$A\vec{x}=\begin{pmatrix}3 &amp; 1 \\ 0 &amp; 2\end{pmatrix}\begin{pmatrix}1 \\ -1\end{pmatrix}=\begin{pmatrix}2 \\ -2\end{pmatrix}=2\cdot \begin{pmatrix}1 \\ -1\end{pmatrix}$$</p>
<p>It turns out we can choose any vector along $\vec{x}$, the outcome is the same, for example:</p>
<p>$$A\vec{x}=\begin{pmatrix}3 &amp; 1 \\ 0 &amp; 2\end{pmatrix}\begin{pmatrix}-3 \\ 3\end{pmatrix}=\begin{pmatrix}-6 \\ -6\end{pmatrix}=2\cdot \begin{pmatrix}-3 \\ 3\end{pmatrix}$$</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-052135.jpg" alt></p>
<p>We name vectors like $\begin{pmatrix}-3 \\ 3\end{pmatrix}$ and $\begin{pmatrix}1 \\ -1\end{pmatrix}$ <strong>EigenVectors</strong> and 2 the conresponse <strong>EigenValues</strong>. In practice, we usually choose unit eigenvectors(length equals to 1) given that there are innumerable EigenVectors along the line.</p>
<p>I won’t cover how to compute these vectors and vaules and just list the answer as followed</p>
<p>\begin{align}&amp;\begin{pmatrix}3 &amp; 1 \0&amp; 2\end{pmatrix}<br>\begin{pmatrix}{-1}/{\sqrt(2)} \\ {1}/{\sqrt(2)}\end{pmatrix}=<br>2\begin{pmatrix}{-1}/{\sqrt(2)} \\ {1}/{\sqrt(2)}\end{pmatrix} \qquad\qquad\vec{x_1}=\begin{pmatrix}{-1}/{\sqrt(2)} \\ {1}/{\sqrt(2)}\end{pmatrix} &amp;\lambda_1=2\<br>&amp;\begin{pmatrix}3 &amp; 1 \0&amp; 2\end{pmatrix}<br>\begin{pmatrix}1 \\ 0\end{pmatrix}\qquad=\qquad<br>3\begin{pmatrix}1 \\ 0\end{pmatrix}\qquad\qquad\quad\,\,\,\vec{x_2}=\begin{pmatrix}1 \\ 0\end{pmatrix}<br>&amp;\lambda_2=3<br>\end{align}<br>Notice that $|\vec{x_1}|=1$ and $|\vec{x_2}|=1$</p>
<h4 id="EigenValue-Decomposition"><a href="#EigenValue-Decomposition" class="headerlink" title="EigenValue Decomposition"></a>EigenValue Decomposition</h4><p>If we put two EigenVectors and corresponding EigenValues together, we can get the following equation:<br>$$AQ=\begin{pmatrix}3 &amp; 1 \0&amp; 2\end{pmatrix}<br>\begin{pmatrix}<br>{-1}/{\sqrt(2)}&amp;1\<br>{1}/{\sqrt(2)}&amp;0<br>\end{pmatrix}=<br>\begin{pmatrix}<br>{-1}/{\sqrt(2)}&amp;1 \\ {1}/{\sqrt(2)}&amp;0<br>\end{pmatrix}<br>\begin{pmatrix}<br>2 &amp; 0\<br>0 &amp; 3<br>\end{pmatrix}=Q\Lambda<br>$$<br>Then we have $AQ=Q\Lambda$, the conclusion is still right if we introduce more dimensions, that is<br>\begin{align}<br>A\vec{x_1}=\lambda\vec{x_1}\<br>A\vec{x_2}=\lambda\vec{x_2}\<br>\vdots\qquad\<br>A\vec{x_k}=\lambda\vec{x_k}<br>\end{align}</p>
<p>$$Q=<br>\begin{pmatrix}<br>    x_{11}&amp; x_{21} &amp;\cdots x_{k1}&amp;\<br>    x_{12}&amp; x_{22} &amp;\cdots x_{k2}&amp;\<br>    &amp;\vdots&amp;&amp;\<br>    x_{1m}&amp; x_{22} &amp;\cdots x_{km}&amp;<br>\end{pmatrix}<br>\qquad\Lambda=<br>\begin{pmatrix}<br>\lambda_1 &amp; 0 &amp; \cdots&amp;0\<br>0 &amp;\lambda_2&amp;\cdots&amp;0\<br>\vdots&amp;\vdots&amp;\ddots\<br>0&amp;\cdots&amp;\cdots&amp;\lambda_k<br>\end{pmatrix}$$</p>
<p>If we do something on the equation $AQ=Q\Lambda$, then we have:<br>$$AQQ^{-1}=A=Q\Lambda Q^{-1}$$<br>It is EigenVaule Decomposition.</p>
<h4 id="Resolution"><a href="#Resolution" class="headerlink" title="Resolution"></a>Resolution</h4><p>Now, Let’s look at the question in the beginning of this section<br>\begin{align}<br>AAA\cdots A&amp;= \begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}\begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}\begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}\cdots \begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}\<br>AAA\cdots A &amp;= Q\Lambda Q^{-1}Q\Lambda Q^{-1}Q\Lambda Q^{-1}\cdots Q\Lambda Q^{-1}=Q\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \0 &amp; 2\end{pmatrix}Q^{-1}\<br>AAA\cdots A &amp;=Q\Lambda\Lambda\cdots \Lambda Q^{-1}=<br>Q\begin{pmatrix}2^{1000} &amp; 0 \\0 &amp; 3^{1000}\end{pmatrix}Q^{-1}<br>\end{align}<br>The calculation is extremely simple using EVD.</p>
<h4 id="Exercise-1"><a href="#Exercise-1" class="headerlink" title="Exercise"></a>Exercise</h4><ol>
<li>Research how to compute EigenVectors and EigenValues, then compute$\begin{pmatrix}1 &amp; 2 &amp; 3\4 &amp; 5 &amp;6\7 &amp; 8 &amp; 9\end{pmatrix}$. </li>
<li>Think about the decisive factor affects how many EigenValues we can get.</li>
</ol>
<h3 id="Singular-Value-Decompositon"><a href="#Singular-Value-Decompositon" class="headerlink" title="Singular Value Decompositon"></a>Singular Value Decompositon</h3><p>Notice that EigenVector Decomposition is applied to decompose square matrices. Is there any approach to decompose non-square matrices? The answer is a YES, and the name is Singular Value Decompositon.</p>
<h4 id="Intuition-1"><a href="#Intuition-1" class="headerlink" title="Intuition"></a>Intuition</h4><p>First of all, let’s take a look at what SVD looks like<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-rectangle1.png" alt><br>From the picture, we can find that matrice $A$ is decomposed to 3 components: $U$, $\Sigma$ and $V^{T}$. $U$ and$V^T$ are both sqaure matrices and $\Sigma$ has the same size as $A$. Still, I want to emphasize that $U$ and$V^T$ are both unitary matrix, which means the Determinant of $U$ and $V^T$ is 1 and $U^T=U^{-1}\quad V^T=V^{-1}$.</p>
<h4 id="Deduction"><a href="#Deduction" class="headerlink" title="Deduction"></a>Deduction</h4><p>In the Linear Transformation section, we can transform a vector to another coordinate axes. Assume you have a non-square matrice, and you want to transform A from  vectors $V=(\vec{v_1}, \vec{v_2},\cdots,\vec{v_n})^T$ to antoher coordinate axes which is $U=(\vec{u_1}, \vec{u_2},\cdots,\vec{u_n})^T$, the thing is, $\vec{v_i}$ and $\vec{u_i}$ have unit length, and all directions are perpendicular, that is, each of $\vec{v_i}$ are at right angles to other $\vec{v_j}$, we name such matrices as orthogonal matrices. In addition, I need add a factor $\Sigma=(\sigma_1,\sigma_2, \sigma_3,\cdots,\sigma_n)$ which represent the times of each direction of $\vec{u_i}$, i.e., We need transform A from $V=(\vec{v_1}, \vec{v_2},\cdots,\vec{v_n})^T$ to $(\sigma_1 \vec{u_1},\sigma_2 \vec{u_2}, \sigma_3 \vec{u_3},…\sigma_n \vec{u_n})^T$. From the picture below we can find that we want to transform from the circle coordinate axes to the ellipse axes.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-circle.png" alt><br>\begin{align}<br>\vec{v_1} \vec{v_2} \vec{v_3},…\vec{v_n} \qquad\rightarrow \qquad &amp;\vec{u_1},\vec{u_2},\vec{u_3},…\vec{u_n}\<br>&amp;\sigma_1,\sigma_2, \sigma_3,…\sigma_n<br>\end{align}</p>
<p>Recall that we can transform $A$ at every direction, then generate another direction as new coordinate direction. So we have<br>$$ A \vec{v_1}=\sigma_1 \vec{u_1}\<br>A \vec{v_2}=\sigma_2 \vec{u_2}\<br>\vdots\<br> A \vec{v_j}=\sigma_j \vec{u_j}$$</p>
<p>\begin{align}<br>&amp;\begin{pmatrix}\\A\\\end{pmatrix}\begin{pmatrix}\\ \vec{v_1},\vec{v_2},\cdots,\vec{v_n}\\\end{pmatrix}=\begin{pmatrix}\\ \vec{u_1}, \vec{u_2},\cdots,\vec{u_n}\\ \end{pmatrix}\begin{pmatrix}<br>\sigma_1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; \sigma_2 &amp; \cdots &amp; 0 \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>0 &amp; 0 &amp; \cdots &amp; \sigma_n<br>\end{pmatrix}\<br>&amp;C^{m\times n}\qquad\quad C^{n\times n}\qquad\qquad\qquad C^{m\times n}\qquad \qquad \qquad C^{n\times n}<br>\end{align}<br>Which is<br>$$A_{m\times n}V_{n\times n} = \hat{U}<em>{m\times n}\hat{\Sigma}</em>{n\times n}$$</p>
<p>\begin{align}<br>A_{m\times n}V_{n\times n} &amp;= \hat{U}<em>{m\times n}\hat{\Sigma}</em>{n\times n}\<br>(A_{m\times n}V_{n\times n}V_{n\times n}^{-1} &amp;= \hat{U}<em>{m\times n}\hat{\Sigma}</em>{n\times n}V_{n\times n}^{-1}\<br>A_{m\times n}&amp;=\hat{U}<em>{m\times n}\hat{\Sigma}</em>{n\times n}V_{n\times n}^{-1}\&amp;=\hat{U}<em>{m\times n}\hat{\Sigma}</em>{n\times n}V_{n\times n}^{T}<br>\end{align}</p>
<p>We need do something to the equation in order to continue the deduction. First we stretch matrice $\hat{\Sigma}$ vertically to $m \times n$ size. Then stretch $\hat{U}$ horizonly to $m\times m$, we can set any value to the right entries.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-27-RSVD.png" alt></p>
<p>Due to the fact we need calculate $U^{-1}$ and $V^{-1}$, the equation is adjusted to<br>$$A_{m\times n} = U_{m\times m}\Sigma_{m\times n}V^T_{n\times n}$$<br>For furture convenience, we need sort all $\sigma s$, which means:<br>$$\sigma_1\geq\sigma_2\geq\sigma_3 \geq\cdots\geq \sigma_m$$.</p>
<h4 id="How-to-calculate-U-V-T-and-Sigma"><a href="#How-to-calculate-U-V-T-and-Sigma" class="headerlink" title="How to calculate $U$, $V^T$ and $\Sigma$"></a>How to calculate $U$, $V^T$ and $\Sigma$</h4><p>To Decompose matrice $A$, we need calculate $U$, $V^T$ and $\Sigma$. Remember that $U^T = U^{-1}$ and $V^T = V^{-1}$, we will use the property next. </p>
<p>\begin{align}<br>A &amp;= U\Sigma V^T\<br>\end{align}</p>
<p>\begin{align}<br>AA^T&amp;=U\Sigma V^T(U\Sigma V^T)^T\<br>&amp;=U\Sigma V^TV\Sigma^T U^T\<br>&amp;=U\Sigma V^{-1}V\Sigma^T U^T\<br>&amp;=U\Sigma I\Sigma^T U^T\<br>&amp;=U\Sigma^2 U^T<br>\end{align}</p>
<p>\begin{align}<br>(AA^T)U&amp;=(U\Sigma^2 U^T)U\<br>&amp;=(U\Sigma^2 )U^{-1}U\<br>&amp;=U\Sigma^2<br>\end{align}</p>
<hr>
<p>\begin{align}<br>A^TA<br>&amp;=(U\Sigma V^T)^TU\Sigma V^T\<br>&amp;=V\Sigma^T U^TU\Sigma V^T\<br>&amp;=V\Sigma^T U^TU\Sigma V^T\<br>&amp;=V\Sigma^T U^{-1}U\Sigma V^T\<br>&amp;=V\Sigma^T I\Sigma V^T\<br>&amp;=V\Sigma^2 V^T\<br>\end{align}</p>
<p>\begin{align}(A^TA)V&amp;=(V\Sigma^2 V^T)V\<br>&amp;=(V\Sigma^2)V^{-1}V\<br>&amp;=V\Sigma^2<br>\end{align}</p>
<h3 id="Image-Compression"><a href="#Image-Compression" class="headerlink" title="Image Compression"></a>Image Compression</h3><p>Firstly, let’s look at the process of compressing a picture, the left picture is original grayscale image. On the right, under different compress rate, we can see pictures after reproducing. Before compress, the size of the picture is 1775K byte. Then the picture is almost the same, when we compress which into 100K byte size, which means we can save 90% storage space<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-29-image.gif" alt></p>
<p>To compress a piture, you just decompose the matrice through SVD, then instead of using the original $U_{m\times m}$, $\Sigma_{m\times n}$ and $U_{n\times n}$, we shrink every matrice to new size $U_{m\times r}$, $\Sigma_{r\times r}$ and $U_{r\times n}$. The final $size(R)$ is still $m\times n$, but we abandon some entries since these entries are not so important than these we have reserved.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-30-rect.png" alt> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%% octave: core code of svd compressions</span><br><span class="line">X = imread(filename);  </span><br><span class="line">[U S V] = svd(double(X));</span><br><span class="line">R = U(:,1:r)*S(1:r,1:r)*V(:,1:r)&apos;;</span><br></pre></td></tr></table></figure>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Today we have learned mathmatics backgroud on SVD, including linear transformation and EigenVector&amp;EigenVaule. Before SVD, we first talked about EigenValue Decomposition. Finally, Singular Vaule Decomposition is very easy to be deduced. In the last section, we took an example see how SVD be applied to image compression field. </p>
<p>Now, it comes to the topic how to save our storage of a 32G iPhone7, the coclusion is obvious: using SVD compress image to shrink the size of our photos. </p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a href="https://www.youtube.com/watch?v=EokL7E6o1AE" target="_blank" rel="noopener">https://www.youtube.com/watch?v=EokL7E6o1AE</a></li>
<li><a href="https://www.youtube.com/watch?v=cOUTpqlX-Xs" target="_blank" rel="noopener">https://www.youtube.com/watch?v=cOUTpqlX-Xs</a></li>
<li><a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" target="_blank" rel="noopener">https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw</a></li>
<li><a href="https://yhatt.github.io/marp/" target="_blank" rel="noopener">https://yhatt.github.io/marp/</a></li>
<li><a href="https://itunes.apple.com/cn/itunes-u/linear-algebra/id354869137" target="_blank" rel="noopener">https://itunes.apple.com/cn/itunes-u/linear-algebra/id354869137</a></li>
<li><a href="http://www.ams.org/samplings/feature-column/fcarc-svd" target="_blank" rel="noopener">http://www.ams.org/samplings/feature-column/fcarc-svd</a></li>
<li><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="noopener">https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/04/16/eigenvector-and-eigenvalue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/16/eigenvector-and-eigenvalue/" itemprop="url">怎样理解特征向量和特征值（翻译）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-16T21:03:38+02:00">
                2017-04-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>原文地址：<a href="http://math.stackexchange.com/a/23325" target="_blank" rel="noopener">stackexchange</a></p>
<p>原文答案作者主页：<a href="http://math.stackexchange.com/users/742/arturo-magidin" target="_blank" rel="noopener">Arturo Magidin</a></p>
<h4 id="版权声明"><a href="#版权声明" class="headerlink" title="版权声明"></a>版权声明</h4><p>本译文首发于我的个人博客commanber.com, 版权属于原作者。</p>
<h4 id="简短的答案"><a href="#简短的答案" class="headerlink" title="简短的答案"></a>简短的答案</h4><p>特征向量可以让线性变换的理解变得简单。它们是沿着坐标轴（方向）的线性变换包括简单的伸/缩以及翻转；特征值提供的是这些线性变换影响因子。<br>如果你理解越多沿着坐标轴（方向）的线性变换行为，理解线性变换就变得越简单；所以你要做的是有足够多的线性无关的特征向量与单因素线性变换产生联系。</p>
<h4 id="长一点儿的答案"><a href="#长一点儿的答案" class="headerlink" title="长一点儿的答案"></a>长一点儿的答案</h4><p>这个世界上有非常多的问题可以通过线性变换来建模，而特征向量提供了非常简单的解决方案。例如，考虑线性微分方程:<br>    $$\frac{\mathrm d x}{\mathrm d t} = ax + by$$<br>  $$\frac{\mathrm d y}{\mathrm d t} = cx + dy$$</p>
<p>可以找到很多描述此微分方程的系统，比如，两个物种数量的增长相互影响。具体来说，可能物种$x$是物种$y$的捕食者；周围越多的物种$x$，意味着越少的物种$y$可以得到繁衍壮大；问题是周围的物种$y$越少，那么对于物种$x$来说食物就会越少，所以物种$x$的繁衍就会越少；但是接下来因为物种$x$对物种$y$的生存压力降低，很快会导致$y$物种数量的增长；但是这就意味这物种$x$的食物变多了，所以物种xx的数量也跟着增长；如此这般，循环往复。特定的物理现象也能形成这样的系统，比如粒子在运动的流体中，粒子的速度矢量取决于其所处的流体中位置。</p>
<p>直接解决这种系统是非常复杂的。但是，假设如果你可以不用去关注变量$x$和变量$y$而是转而关注$z$和$w$（这里$z$和$w$与$x$和$y$线性相关，也就是说，$z=\alpha x + \beta y$, $\alpha$和$\beta$是常量，同时$w=\gamma x + \delta y$， $\gamma$和$\delta$也是常量）。这样，我们的系统就变换成了如下的形式：<br>$$\frac{\mathrm d z}{\mathrm d t} = \kappa w$$<br>$$\frac{\mathrm d w}{\mathrm d t} = \lambda z$$</p>
<p>也就是说，你对系统做了<strong>解耦</strong>，这样你就可以单独的处理各个独立函数了。接下来就这个问题就变得非常简单：$z=Ae^{\kappa t}$，以及$w=Be^{\lambda t}$。下一步就是用$z$和$w$的公式，算出$x$和$y$。</p>
<p>这能做到么？事实上，这等于我们精确的找到了矩阵$\begin{pmatrix}a &amp; b\ c&amp;d\end{pmatrix}$线性独立的两个特征向量！$z$和$w$是其特征向量，而$\kappa$和$\lambda$为相对应的特征值。通过使用一个表达式把$x$和$y$<strong>混合</strong> 起来，然后解耦成两个互相独立的函数，问题现在变得非常简单了。</p>
<p>这就是我们希望使用特征向量及特征值的本质：通过线性变换把问题<strong>解耦</strong> 成一系列沿着各个隔离<strong>方向</strong>的操作，使得各个方向问题都可独立解决。</p>
<p>大量的问题归根结底是解决<strong>线性独立操作</strong>，理解这些可以实实在在的帮助你理解矩阵/线性变换到底在做什么。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/04/05/pca-translation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/05/pca-translation/" itemprop="url">主成分分析（PCA）简明教程（翻译）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-05T21:58:33+02:00">
                2017-04-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：Lindsay I Smith</p>
<p>时间：2002.2.26</p>
<p>译者：程明波</p>
<p><a href="http://facepress.net/pdf/734.pdf" target="_blank" rel="noopener">英文文章地址</a></p>
<p><a href="http://commanber.com/2017/04/05/pca-translation/">译文地址</a></p>
<h4 id="版权声明"><a href="#版权声明" class="headerlink" title="版权声明"></a>版权声明</h4><p>本译文首发于我的个人博客commanber.com, 版权属于原作者。</p>
<h3 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h3><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>这是一篇帮助读者理解主成分分析（PCA）的教程。PCA是一种统计技术，在人脸识别和图像压缩等领域都有应用。同时，PCA也是一种高维数据模式发现的一种常用方法。</p>
<p>在讲PCA之前，本文先介绍了PCA用到的一些数学概念。其中包括标准差、协方差、特征向量和特征值等。这些背景知识意在帮助我们理解PCA部分，如果你对这些概念已经非常清晰可以跳过此部分。</p>
<p>示例贯穿于整个教程，以便于通过直观的例子讨论概念。如果你还想了解更多内容，霍华德.安东著有约翰威立国际出版公司出版的数学课本《Elementary Linear Algebra 5e》提供了非常好的这方面数学背景知识。</p>
<h3 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h3><h4 id="数学背景知识"><a href="#数学背景知识" class="headerlink" title="数学背景知识"></a>数学背景知识</h4><p>本章试图介绍便于理解主成分分析计算过程所需的基本数学知识。各个主题互相独立，同时各主题会举例说明。理解为什么使用这些技术以及一些关于数据计算结果所告诉我们的比记住枯燥的数学原理更重要。尽管不是所有这些知识都应用于PCA，一些看起来不是直接相关知识是这些最重要技术的基石。</p>
<p>我安排其中的一节介绍统计学知识，主要着眼于分布的度量，或者说数据是怎样离散的，其他的部分主要讲矩阵代数的一些知识，包括特征值、特征向量以及一些PCA所需的重要矩阵性质。</p>
<h4 id="2-1-统计知识"><a href="#2-1-统计知识" class="headerlink" title="2.1 统计知识"></a>2.1 统计知识</h4><p>整个统计学都基于你有一个很大数据集的前提，以及你想分析关于数据集中各个数据点的关系。我会介绍一些量度这些数据的一些方法，帮助你理解这些数据本身。</p>
<h5 id="2-1-1-标准差"><a href="#2-1-1-标准差" class="headerlink" title="2.1.1 标准差"></a>2.1.1 标准差</h5><p>要了解标准差，我们需要一个数据集。统计学经常会使用总体中的一些采样。以选举为例，总体就是一个国家的所有人，因此一个采样就是统计学家用来度量的总体的一个子集。统计学伟大之处是我们只需度量（例如电话调查等）总体中的采样，你就可以计算出最接近所有整体的度量。</p>
<p>本节我假设我们的数据集是某个很大总体的采样。本节后面会提供总体以及采样的更多信息。这是一个示例数据集：<br>$$X=[1\, 2\,4\, 6\, 12\, 15\, 25\, 45\, 68\, 67\, 65\, 98]$$<br>我们简单地假设字符$X$表示包含这些所有数字的集合。如果我想表示这个集合中某个单独的数字，我会用$X$加上下标表示某个具体的数。例：$X_3$表示$X$集合中的第三个数，也就是数字$4$。注意，有些书用$X_0$表示第一个数字，我们这里用$X_1$。另外，我们用字符$n$表示集合中元素的数量。</p>
<p>我们可以计算一个集合的很多维度，比如，我们可以计算样本的均值。这里我假设读者明白什么是一个样本的均值。这里仅给出公式：<br>$$\overline{X} =\frac{\sum_{i=1}^n X_i}{n}$$<br>注意，我们用字符$\overline{X}$标识集合的均值。这个公式表示：把所有的数字加起来再除以他们的数量。</p>
<p>很不幸，均值除了告诉我们某种中心以外，并没有告诉关于数据的更多信息。比如以下两个数据集合的均值(10)完全一样，但是显然它们区别很大。<br>$$[0\, 8\, 12\, 20]\quad 和 \quad[8\, 9\, 11\, 12]$$</p>
<p>那么，这两个集合有何不同呢？这两个集合的离散程度是不同的。一个数据集的标准差（Standard Deviation, 缩写SD）是衡量这个集合数据离散程度的一个指标。怎么计算呢？SD的定义是这样的：每个数据点到这份数据均值点的平均距离。计算每一个数据点到均值点的距离平方，然后相加，再除以$n-1$，再开方，公式如下：<br>$$s=\sqrt{\frac{\sum_{i=1}^n (X-\overline{X})^2}{(n-1)}}$$<br>这里$s$常用来标识样本方差。可能有人会问：“为啥分母除以$n-1$而不是$n$呢？” 答案有点儿复杂，大体来说，如果你的数据集合是一个采样，比如，你取样于真实世界（比如调查500人的选举情况）得到一个子集，那么你就必须用$n-1$，因为这个结果比你用$n$更接近于你用全部的整体算出的标准差。但是，如果你不是计算一个样本的而是整体的标准差，这种情况下，你就应该除以$n$而不是$n-1$。如果想了解更多的关于标准差的内容，可以访问<a href="http://mathcentral.uregina.ca/RR/database/RR.09.95/weston2.html" target="_blank" rel="noopener">这里</a>,链接文章用类似的方法讲了标准差，提供了不同分母计算的区别实验，同时还探讨了采样和总体的异同。</p>
<p>数据集1<br>$$<br>\begin{array}{lrr}<br>X &amp; (X-\overline{X}) &amp; (X-\overline{X})^2 \<br>\hline<br>0 &amp; -10 &amp; 100\<br>8 &amp; -2 &amp; 4\<br>12 &amp; 2 &amp; 4\<br>20 &amp; 10 &amp; 100\<br>\hline<br>\bf{总计} &amp; &amp; 208\<br>\hline<br>\bf{除以(n-1)} &amp; &amp; 69.333\<br>\hline<br>\bf{平方根} &amp; &amp; 8.3266\<br>\hline<br>\end{array}<br>$$</p>
<p>数据集2<br>\begin{array}{lrr}<br>X &amp; (X-\overline{X}) &amp; (X-\overline{X})^2 \<br>\hline<br>8 &amp; -2 &amp; 4\<br>9 &amp; -1 &amp; 1\<br>11 &amp; 1 &amp; 1\<br>12 &amp; 2 &amp; 4\<br>\hline<br>\bf{总计} &amp; &amp; 10\<br>\hline<br>\bf{除以(n-1)} &amp; &amp; 3.333\<br>\hline<br>\bf{平方根} &amp; &amp; 1.8.257\<br>\hline<br>\end{array}<br>$$\bf{表2.1 标准差计算}$$</p>
<p>从上表2.1，我们可以看到标准差的计算过程。</p>
<p>因此，和我们预想的一样，第一个数据的的标准差要比第二个大得多。原因是数据离散于均值点的程度更高。再举一个例子，数据集:<br>$$[10\, 10\, 10\, 10]$$<br>的均值也是10，但是它的标准差是0， 因为所有的数字是相同的。没有任何数据点偏离均值。</p>
<h5 id="2-1-2-方差"><a href="#2-1-2-方差" class="headerlink" title="2.1.2 方差"></a>2.1.2 方差</h5><p>方差是数据离散程度的另一个度量。实际上，它和标准差几乎相同，公式如下：<br>$$s^2=\frac{\sum_{i=1}^n (X-\overline{X})^2}{(n-1)}$$<br>你会注意到方差就是标准差的平方，标识上也有$s$($s^2$)。$s^2$经常用来标识一个数据集的方差。方差和标准差都是用来衡量数据的离散程度。标准差使用的更普遍，方差也常使用。之所以介绍方差是因为下一节我们介绍的协方差是基于方差的。</p>
<h5 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h5><p>计算下列数据集的均值、标准差和方差。</p>
<p>[12 23 34 44 59 70 98]</p>
<p>[12 15 25 27 32 88 99]</p>
<p>[15 35 78 82 90 95 97]</p>
<h5 id="2-1-3-协方差"><a href="#2-1-3-协方差" class="headerlink" title="2.1.3 协方差"></a>2.1.3 协方差</h5><p>我们之前介绍的前两种度量方式只针对纯1维情况。1维数据集合可能是这种形式：屋里所有人的身高，或者上学期计算机科目101的成绩等等。但许多数据集是大于1维的情况， 对于这种数据集，统计分析的目标经常是分析不同的维度之间的关系。例如，我们可能有个数据集同时包含了课堂上学生的身高，以及他们论文的分数。接着我们就可以利用统计分析工具来观察学生的身高对他们的成绩是否有影响。</p>
<p>标准差和方差只是对单一维度的计算，因此，你只能对数据的每一个维度单独计算标准差。然而，如果有一种类似的度量能找到各维度相互在偏离均值的变化关系会非常有用。</p>
<p>协方差就是这样一种度量。协方差总是用来度量两个维度，如果你计算一个维度和他自己维度的协方差，这时协方差就退化为这一个维度的方差。因此，如果你有一个3维数据集$(x,y,z)$,协方差的计算公式与方差非常相似。方差的计算公式也可以这样表示：<br>$$var(X)=\frac{\sum_{i=1}^n(X_i-\overline{X_i})(X_i-\overline{X_i})}{(n-1)}$$<br>这里我简单的对平方项进行了展开。有了以上知识，我们现在可以写出协方差的公式了：<br>$$cov(X,Y)=\frac{\sum_{i=1}^n(X_i-\overline{X_i})(Y_i-\overline{Y_i})}{(n-1)}$$</p>
<p>除了第二个括号中的$X$全部被替换成了$Y$以外，协方差和方差的公式完全一样。我们可以这么表述：“对于每个数据项，把每个$x$和$x$均值的差与每个$y$和$y$均值的差相乘，再加和除以$(n-1)$”。协方差是怎样的一种工作机理呢？我们这里用一些数据来举例。想象你通过调查得到一个2维数据。假设我们问了一堆学生他们花在科目COSC241的总小时数，以及他们的学期末成绩。现在我们有了两个维度，第一个维度是$H$，标识学习的小时数，第二个维度是M，标识学生的成绩。<strong>图2.2</strong>展示了我们假设的数据以及两个维度学习小时数和成绩之间的协方差$cov(H,M)$。</p>
<p>这张图告诉我们什么呢？协方差的值没有它的符号重要（正或负）。如果值是正的，比如我们这里，那么意味着两个维度一起增减。即，一般来说，如果学习的小时数增加，那么这个学生最后取得的成绩就会高。</p>
<p>但是如果协方差的值是负的，那么如果其中一个维度增加，另一个维度就会减少。如果我们刚刚计算的协方差的结果是负值。那我们的说法就变成了随着学习小时数的增加，期末成绩会降低。</p>
<p>最后一种情况，如果协方差是$0$，那么说明两个维度是相互独立的。</p>
<p>我们很容易画一张图如图2.1.3，得出结论：学习成绩随着学习的小时数增加而增加。但是，只有两维或三维这种低维的奢侈情况，我们才能通过可视化观察趋势。由于在一个数据集中可以计算任意两个维度的协方差，这种技术经常是高维数据可视化非常困难的情况下寻找维度之间关系的一种方法。</p>
<p>你可能会问，$cov(X, Y)$与$cov(Y,X)$是否相等？简单一看我们就会发现，它们是完全相等的，因为两个式子计算的唯一不同是在$cov(Y,X)$中$(X_i-\overline{X_i})(Y_i-\overline{Y_i})$被替换成了$(Y_i-\overline{Y_i})(X_i-\overline{X_i})$。我们知道乘法满足交换率，也就是说，无论乘数和被乘数的位置怎么变化，结果都是一样，也就是说这两个协方差结果是相同的。</p>
<h5 id="2-1-4"><a href="#2-1-4" class="headerlink" title="2.1.4"></a>2.1.4</h5><p>我们知道，协方差总是用来计算两个维度之间的关系。如果我们有一个超过2维的数据集合，那么我们要计算的协方差的值的数量就不止一个了。比如，一个三维的数据集（$x,y,z三个维度$)。你可以计算的协方差就有$cov(x,y)$、$cov(x,z)$ 和 $cov(x,z)$。事实上，对于一个$n$维的数据集，你可以计算$\frac{n!}{(n-2)! * 2}$不同的值。</p>
<p>数据：<br>\begin{array}{lrr}<br>&amp;小时数(H)&amp;成绩(M)\<br>\hline<br>数据 &amp; 9 &amp; 39\<br>&amp; 15 &amp;56 \<br>&amp; 25 &amp;93 \<br>&amp; 14 &amp;61 \<br>&amp; 10 &amp;50 \<br>&amp; 18 &amp;75 \<br>&amp; 0 &amp;32 \<br>&amp; 16 &amp;85 \<br>&amp; 5 &amp;42 \<br>&amp; 19 &amp;70 \<br>&amp; 16 &amp;66 \<br>&amp; 20 &amp;80 \<br>\hline<br>总数&amp;167&amp;749\<br>\hline<br>平均 &amp; 13.92&amp; 62.42\<br>\hline<br>\end{array}</p>
<p>协方差：<br>\begin{array}{cc|c|c|c}<br>H &amp; M &amp; (H_i - \overline{H}) &amp; (M_i-\overline{M}) &amp; (H_i-\overline{H})(M_i-\overline{M})\<br>\hline<br>9 &amp; 39 &amp; -4.92&amp; -23.42 &amp;115.23\<br>15 &amp; 56 &amp; 1.08&amp; -6.42 &amp;-6.93\<br>25 &amp; 93 &amp; 11.08&amp; -30.58 &amp;338.83\<br>14 &amp; 61 &amp; 0.08&amp; -1.42 &amp;-0.11\<br>10 &amp; 50 &amp; -3.92&amp; -12.42 &amp;48.69\<br>18 &amp; 75 &amp; 4.08&amp; 12.58 &amp;51.33\<br>0 &amp; 32 &amp; -13.92&amp; -30.42 &amp;423.45\<br>16 &amp; 85 &amp; 2.08&amp; -22.58 &amp;46.97\<br>5 &amp; 42 &amp; -8.92&amp; -20.42 &amp;182.15\<br>19 &amp; 70 &amp; 5.08&amp; -7.58 &amp;38.51\<br>16 &amp; 66 &amp; 2.08&amp; -3.58 &amp;7.45\<br>20 &amp; 80 &amp; 6.08&amp; 17.58 &amp;106.89\<br>\hline<br>总数 &amp; &amp; &amp; &amp; 1149.89\<br>\hline<br>平均 &amp; &amp; &amp; &amp; 104.54\<br>\end{array}</p>
<p>想求出所有不同维度的协方差，非常有用的方法是把他们全计算出来然后放入矩阵。我假设你对矩阵比较熟悉，以及矩阵怎样定义。因此，对于一个$n$维的数据集的协方差矩阵：<br>$$C^{n\times n}=(c_{i,j}, c_{i,j}=cov(Dim_i, Dim_j))$$,<br>这里$C^{n\times n}$是一个$n$行$n$列的矩阵，$Dim_x$ 是第$x$维。上面非常不美观的公式说的是，如果你有一个$n$维数据集，那么协方差矩阵就是一个$n$行$n$列的矩阵，矩阵的每一个元素是两个维度之间的协方差计算结果。例如，矩阵的第2行第三列就是维度2和维度3之间的协方差计算结果。</p>
<p>一个例子。我们假设有一个3维的数据集，分别使用$x$,$y$,$z$表示3个维度。那么协方差矩阵是一个3行3列的矩阵，矩阵中的元素就是：<br>\begin{pmatrix}<br>cov(x,x) &amp; cov(x,y) &amp; cov(x,z) \<br>cov(y,x) &amp; cov(y,y) &amp; cov(y,z) \<br>cov(z,x) &amp; cov(z,y) &amp; cov(z,z) \<br>\end{pmatrix}</p>
<p>几个需要注意：主对角线计算的某一维和它自己的协方差，也就是这些维度的方差。剩下的元素，因为$cov(a,b)=cov(b,a)$，所以矩阵关于主对角线对称。</p>
<h5 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h5><ol>
<li>计算以下关于$x$和$y$的2维数据集的协方差，然后描述一下协方差结果可能推导出数据什么方面的结论。<br>\begin{array}{c|c|c|c|c|c}<br>项目id &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\<br>\hline<br>x &amp; 10 &amp; 39 &amp; 19 &amp; 23 &amp; 28\<br>y &amp; 43 &amp; 13 &amp; 32 &amp; 21 &amp; 20\<br>\hline<br>\end{array}</li>
<li>计算下列3维数据的协方差矩阵：<br>\begin{array}{c|c|c|c}<br>项目id &amp; 1 &amp; 2 &amp; 3 \<br>\hline<br>x &amp; 1 &amp; -1 &amp; 4\<br>y &amp; 2 &amp; 1 &amp; 3\<br>z &amp; 1 &amp; 3 &amp; -1\<br>\hline<br>\end{array}</li>
</ol>
<h4 id="2-2-矩阵代数"><a href="#2-2-矩阵代数" class="headerlink" title="2.2 矩阵代数"></a>2.2 矩阵代数</h4><p>本节会介绍PCA所用到的一些矩阵代数的背景知识，我将重点介绍对给定矩阵计算特征向量和特征值的相关知识。这里我假设你了解矩阵的基本知识。<br>\begin{align}\begin{pmatrix}<br>2 &amp; 3\<br>2 &amp; 1\<br>\end{pmatrix}\times<br>\begin{pmatrix}<br>1\\<br>3<br>\end{pmatrix}=<br>\begin{pmatrix}<br>11\\<br>5<br>\end{pmatrix}<br>\end{align}</p>
<p>\begin{align}\begin{pmatrix}<br>2 &amp; 3\<br>2 &amp; 1\<br>\end{pmatrix}\times<br>\begin{pmatrix}<br>3\\<br>2<br>\end{pmatrix}=<br>\begin{pmatrix}<br>12\\<br>8<br>\end{pmatrix}=4\times<br>\begin{pmatrix}<br>3\\<br>2<br>\end{pmatrix}<br>\end{align}<br>$$\bf{图2.2：非特征向量和1个特征向量}$$<br>\begin{align}<br>2\times<br>\begin{pmatrix}<br>3\\<br>2<br>\end{pmatrix}=<br>\begin{pmatrix}<br>6\\<br>4<br>\end{pmatrix}<br>\end{align}</p>
<p>\begin{align}\begin{pmatrix}<br>2 &amp; 3\<br>2 &amp; 1\<br>\end{pmatrix}\times<br>\begin{pmatrix}<br>6\\<br>4<br>\end{pmatrix}=<br>\begin{pmatrix}<br>24\\<br>16<br>\end{pmatrix}=4\times<br>\begin{pmatrix}<br>6\\<br>4<br>\end{pmatrix}<br>\end{align}</p>
<p>$$\bf{图2.3: 缩放特征向量后仍为特征向量}$$</p>
<h5 id="2-2-1-特征向量"><a href="#2-2-1-特征向量" class="headerlink" title="2.2.1 特征向量"></a>2.2.1 特征向量</h5><p>如你所知，只要两个矩阵的大小相容，你就可以将两个矩阵相乘。特征向量是矩阵相乘的的特殊形式。我们现在考虑如图2.2所示的矩阵和向量相乘的情况。</p>
<p>第一个例子中，计算结果不是整数与原始矩阵相乘的形式，但到了第二的例子，计算结果的就是一个整数乘以与左边完全相同的一个向量。为何能产生这样的结果呢？实际上，向量就是2维空间的一个矢量。向量$\begin{pmatrix}3\2\end{pmatrix}$(第二个相乘的例子)代表从原点$(0,0)$一个指向$(3,2)$的一个箭头，另一个矩阵可以被认为是变换矩阵。如果你在向量的左边乘以一个矩阵，结果就是把这个向量从其原始位置进行了变换。</p>
<p>上面说得就是变换就是特这向量的本质。想象一个变换矩阵，以及一个在直线$y=x$上的向量，矩阵左乘这个向量。如果你发现结果仍然位于$y=x$这条直线上，那么这就向是量的自反射。这个向量（所有的乘子，因为我们不关心向量的大小）就是这个变换矩阵的一个特征向量。</p>
<p>这些特征向量有什么性质呢？第一你要知道的就是只有方矩阵才有特征向量。其次是不是所有的方矩阵都有特征向量。最后，如果一个$n\times n$矩阵只要有，那么就一定有$n$个特征向量。如果一个$3\times 3$的矩阵有特征向量，那就有3个。</p>
<p>特征向量的另一个性质是：如果我在相乘之前对其缩放一定量，那么我可以仍然得到相同的乘积形式（如图2.3）。这是因为如果你缩放一个的向量，你做的仅仅是把这个向量变长，而没有改变其方向。最后，一个矩阵的所有的特征向量都是<em>垂直</em>的。也就是说，无论你有多少维的向量，他们都是互相形成直角。另一个更数学化的说法叫<em>正交</em>。这么描述非常重要，原因是我们可以更方便表述这些垂直的正交向量，而不用在$x$轴和$y$轴的坐标系中描述。在PCA介绍部分我们会用到这些。</p>
<p>另一点重要的是，数学家们在寻找特征向量时，他们总喜欢找长度为1的特征向量。原因我们已经知道，向量的长度并不是影响因素，方向才是。所以为了使特征向量有标准形式，我们的一般做法是将其缩放成长度为1的向量。这样，所有的特征向量就都有相同的长度了。下面我们把例子中的向量标准化。<br>$$\begin{pmatrix}<br>3\<br>2<br>\end{pmatrix}<br>$$<br>是一个特征向量，这个向量的长度为：<br>$$\sqrt{(3^2+2^2)}=\sqrt{13}$$<br>所以我们把原始的向量除以这个长度，就得到了长度唯一的特征向量。<br>$$\begin{pmatrix}<br>3\<br>2<br>\end{pmatrix}\div\sqrt{13}=<br>\begin{pmatrix}<br>{3}/{\sqrt{13}}\<br>{2}/{\sqrt{13}}<br>\end{pmatrix}<br>$$</p>
<p>怎么找到这些神秘的特征向量呢？很不幸，只有当矩阵足够小时，特征向量才好找，比如不超过$3\times 3$的矩阵。如果矩阵大小再变大，通常的做法是用复杂的迭代方式求解，这些方法此教程不会讲解。如果你想在程序中使用计算特征向量的方法，很多数学库都有实现，<a href="http://webnz.com/robert/" target="_blank" rel="noopener">一个有用的数学库包</a>。</p>
<p>如果想进一步了解特征向量和特征值以及正交等内容，请参考霍华德.安东著有约翰威立国际出版公司出版的数学课本《Elementary Linear Algebra 5e》，ISBN 0-471-85223-6。</p>
<h5 id="2-2-2-特征值"><a href="#2-2-2-特征值" class="headerlink" title="2.2.2 特征值"></a>2.2.2 特征值</h5><p>特征值和特征向量高度相关，其实我们已经在图2.2看到过特征值。还记得被矩阵缩放以后的特征向量有相同的大小么？在那个例子中，这个值是4。这里4就是特征向量相关的特征值。无论我们对特征向量怎么缩放，我们始终得到的特征值都一直是一样的，如图2.3的例子特征值一直是4。</p>
<p>现在我们发现特征向量和特征值总是成对出现。如果你现在需要某个编程库计算特征向量，通常特征值也被同时计算出来了。</p>
<h4 id="练习-2"><a href="#练习-2" class="headerlink" title="练习"></a>练习</h4><p>对于下面的矩阵<br>$$\begin{pmatrix}<br>3&amp;0&amp;-1\<br>-4&amp;1&amp;2\<br>-6&amp;0&amp;-2<br>\end{pmatrix}<br>$$<br>判断下面是否有此矩阵的特征向量，如果有，请求出对应的特征值。<br>$$<br>\begin{pmatrix}<br>2\<br>2\<br>-1<br>\end{pmatrix}\<br>\begin{pmatrix}<br>-1\<br>0\<br>2<br>\end{pmatrix}\<br>\begin{pmatrix}<br>-1\<br>1\<br>3<br>\end{pmatrix}\<br>\begin{pmatrix}<br>0\<br>1\<br>0<br>\end{pmatrix}\<br>\begin{pmatrix}<br>3\<br>2\<br>1<br>\end{pmatrix}<br>$$</p>
<h3 id="第三章-主成分分析（Principal-Components-Analysis）"><a href="#第三章-主成分分析（Principal-Components-Analysis）" class="headerlink" title="第三章 主成分分析（Principal Components Analysis）"></a>第三章 主成分分析（Principal Components Analysis）</h3><p>终于到了主成分分析（PCA）部分了，PCA可以在数据中识别模式，并通过此种方式突出数据中相似和不同的部分。由于很难用图像表示高维数据，也就意味着在高维数据中寻找模式变得非常困难，这时，PCA就成了极为强大的数据分析工具。</p>
<p>另一个PCA的优点是，如果你通过其找到了数据中的模式，你还可以用来压缩数据，即，在不损失太多信息的前提下降低数据的维度。这个技术被用于图像压缩，我们在稍后的章节中会有涉及。</p>
<p>本章我们将针对一个数据集，一步一步实现PCA计算。这里我不准备描述<em>为什么</em><br>PCA表现出色。我做的是为你提供每一步都发生了什么，这样，将来如果你想使用此技术时，就会有足够多的知识帮助你做决策。</p>
<h4 id="3-1-方法"><a href="#3-1-方法" class="headerlink" title="3.1 方法"></a>3.1 方法</h4><h5 id="第一步：数据集"><a href="#第一步：数据集" class="headerlink" title="第一步：数据集"></a>第一步：数据集</h5><p>在我们这个简单的例子中，我会使用我编造的一个数据集。这个数据集只有两维，之所以选择这份数据是因为我可以通过画出图形来分析PCA的每一步都发生了什么。</p>
<h5 id="第二步：减掉均值"><a href="#第二步：减掉均值" class="headerlink" title="第二步：减掉均值"></a>第二步：减掉均值</h5><p>如果想实现PCA，我们首先要把每一维的数据减掉均值。就是说要对每一维求平均值，接着把每一维的每个数据都减掉均值。我们这里所有的$x$值都要减掉$\overline{x}$($x$维度所有数据的均值)，所有的$y$值都减掉$\overline{y}$。这样我们就构造了一个均值为0的数据集。<br>\begin{align}<br>\bf{数据}=<br>\begin{array}{c|c}<br>x&amp; y\<br>\hline<br>2.5 &amp; 2.4\<br>0.5 &amp; 0.7\<br>2.2 &amp; 2.9\<br>1.9 &amp; 2.2\<br>3.1 &amp; 3.0\<br>2.3 &amp; 2.7\<br>2 &amp; 1.6\<br>1 &amp; 1.1\<br>1.5 &amp; 1.6\<br>1.1 &amp; 0.9\<br>\end{array}\bf{调整后的数据=}<br>\begin{array}{c|c}<br>x&amp; y\<br>\hline<br>0.69 &amp; 0.49\<br>-1.31 &amp; -1.21\<br>0.39 &amp; 0.99\<br>0.09 &amp; 0.29\<br>1.29 &amp; 1.09\<br>0.49 &amp; 0.79\<br>0.19 &amp; -0.31\<br>-0.81 &amp; -0.81\<br>-0.31 &amp; -0.31\<br>-0.71 &amp; -1.01\<br>\end{array}<br>\end{align}<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-062821.jpg" alt><br>$$图3.1：PCA示例数据，左边为原始数据，右边为减掉均值的数据$$</p>
<h5 id="第三步：计算协方差矩阵"><a href="#第三步：计算协方差矩阵" class="headerlink" title="第三步：计算协方差矩阵"></a>第三步：计算协方差矩阵</h5><p>协方差矩阵我们在2.1.4小节已经讨论过。由于我们的数据是2维的，所有协方差矩阵就是$2\times 2$。协方差矩阵计算没特别说明的，我直接给出结果：<br>$$cov=\begin{pmatrix}<br>0.616555556&amp;0.615444444\<br>0.615444444&amp;0.716555556<br>\end{pmatrix}$$<br>由于协方差矩阵非对角线元素都是正值，所以我们可以预期$x$和$y$一起增减。</p>
<h5 id="第四步：计算协方差矩阵的特征向量和特征值"><a href="#第四步：计算协方差矩阵的特征向量和特征值" class="headerlink" title="第四步：计算协方差矩阵的特征向量和特征值"></a>第四步：计算协方差矩阵的特征向量和特征值</h5><p>协方差矩阵是方阵，所以我们可以计算其特征向量和特征值。这极为重要，因为他们可以告诉我们关于数据的有用信息。我一会儿会说明原因，现在我们来看一下特征值和特征向量：<br>\begin{align}<br>\bf{特征值}=\begin{pmatrix}<br>0.0490833989\<br>1.28402771<br>\end{pmatrix}\bf{\bf    特征向量}=\begin{pmatrix}<br>-0.7351178656 &amp; -0.6778873399\<br>0.677873399 &amp; -0.735178656<br>\end{pmatrix}<br>\end{align}<br>一定要注意到两个特征向量都是单位向量。也就是说他们的长度都是1。这个结果对PCA非常重要，幸运的是，大部分数学工具包计算特征向量提供的都是单位向量。</p>
<p>那么，这些计算结果都是什么意思呢？如果你观察图3.2的数据点，你会发现这些数据有非常强的模式。和我们用协方差预期的一致，这些数据确实一起增减。我利用数据同时也画了两个特征向量，这两个特征向量看起来像图3.2的对角线。我们在特征向量小节介绍过，两个特征向量是相互垂直的。但是，更重要的是特征向量为我们提供了数据中的模式信息。可以看出，其中一条线（译者注：大概45度倾角的这条线）看起来像画了拟合这些数据点的一条线。这个特征向量告诉我们两个数据维度沿着线的相关性（译者注：原文是两个数据集，我认为是两个数据维度）。第2个特征向量（译者注：大概135度倾角的这条线）给我们提供了另外一些重要性稍低的数据中的模式，数据点分布在线的两边。</p>
<p>因此，通过从特征矩阵中取出特征向量进行分析，我们已经提取出了刻画数据特点的线。接下来的步骤会包括数据变换以便于用我们这些线来表达数据。</p>
<h5 id="第五步：选择成分及构建特征的向量"><a href="#第五步：选择成分及构建特征的向量" class="headerlink" title="第五步：选择成分及构建特征的向量"></a>第五步：选择成分及构建特征的向量</h5><p>现在我们来讨论数据压缩和降维的概念。如果你学习了前面小节的特征向量和特征值的相关信息，你会注意到特征向量是之间有很大不同。事实上，具有更大特征值对应的特征向量是数据集的主成分(Principal Component)。在我们这个例子中，对应更大特征值特的特征向量是基本拟合数据点的这条线。这维特征向量描述了数据维度之间最重要的关系。<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-081716.jpg" alt><br>$$图3.2:\bf{标准化（减掉均值）后的数据图以及协方差矩阵中特征向量图}$$</p>
<p>一般来说，一旦从协方差矩阵中找出特征向量，下一步我们要做的就是把他们对应的特征值从高到低排列。排序表明了成分（component）的重要性高低。现在，如果你如果愿意，可以忽略那些重要性没那么高的成分，你就会丢失一些信息，但是如果特征值非常小，你丢失的信息并不会太多。如果你扔掉一些成分，最终的数据集的维度会低于原始数据的维度。具体来说，如果你的原始数据有$n$维，因此你可以计算出$n$个特征向量和$n$个特征值，接下来如果你只选取前$p$维特征向量，那么最终的数据就变成了$p$维的数据集。</p>
<p>现在你需要做的是构造一个特征(feature)的向量，其实就是一个向量的矩阵。矩阵是通过挑选你希望留下的特征向量，组成一个每列1个特征向量的矩阵(译者注：最后一维我认为是第p维更好，可以与上面一段对应)。<br>$$\bf{特征的向量}=(eig_1,eig_2,eig_3,…,eig_p)$$</p>
<p>来看我们的例子，现在我们有2个特征向量，我们现在有两个选择，第一选择是两个特征向量都被用于构造特征的向量：<br>$$cov=\begin{pmatrix}<br>-0.77873399&amp;-0.735178656\<br>-0.735178656&amp;0.677873399<br>\end{pmatrix}$$<br>或者，我们可以扔掉不重要的成分，那么特征的向量只有1列：<br>$$cov=\begin{pmatrix}<br>-0.677873399\<br>-0.735178656<br>\end{pmatrix}$$<br>下一节我们将针对上面两种新的数据集进行讨论。</p>
<h5 id="第六步：生成新数据集"><a href="#第六步：生成新数据集" class="headerlink" title="第六步：生成新数据集"></a>第六步：生成新数据集</h5><p>这是PCA的最后同时是最简单的一步。一旦我们选择了我们希望保留到成分（特征向量集），我们只需把特征的矩阵转置，左乘调整后的数据（原始数据减掉均值），然后再转置。<br>$$\bf{最终数据=行特征的向量} \times \bf{行调整后的数据}$$</p>
<p>这里$\bf{行特征的向量}$是特征的向量组成的矩阵进行转置，也就是说现在特征向量现在是以行的形式排列，最重要的特征向量在第一行。$\bf{行调整后的数据}$是经过均值调整后的数据，也进行了转置，也就是说数据项在每一列，而每行是一个独立的维度。抱歉数据转置可能来得有点儿突然，但是如果我们现在对特征向量的矩阵和数据进行转置，后面的公式就会简单很多，而不是一直带着个转置的上标符号$T$。$\bf{最终数据}$是最终的数据集合，其中每一列是一个数据项，每一行是一个维度。</p>
<p>做完这些我们可以得到什么呢？我们可以得到和我们选择向量完全相关的原始数据。我们的原始数据有$x$轴和$y$轴两个坐标的坐标系，所以我们的数据与这两个坐标的坐标系相关。其实你可以用任何你喜欢的两个坐标轴的坐标系来表示你的数据。如果坐标轴互相垂直，这种表示方法是最高效的，这就是为何特征向量间互相垂直这么重要。现在我们已经把我们的数据从跟$x$轴和$y$轴相关改为2个特征向量组成的坐标系相关。如果说我们已经通过降维构造了新的数据集，也就是说我们扔掉了一些特征向量，那么新数据只跟我们留下的特征向量相关。</p>
<p>为了展示我们的数据，我已经把两种可能的特征的向量都对数据做了变换。我已经对每种情况的结果进行的了转置，这样我就把数据恢复成表结构的组织形式。同时，我也把最终的数据点画了出来，这样我们就可以观察这些数据点与这些成分之间的关系。</p>
<p>两个特征向量都保留的情况转换后的结果见图3.3。这个图其实就是原始数据旋转后，这样特征向量就成了坐标轴。这种情况很好理解，因为我们在分解的过程中并没有丢失任何信息。</p>
<p>另外一种变换，我们只保留有最大特征值的特征向量，我们可以从图3.4中看的数据的结果。和预期的一样，这个数据只有一维。如果你用这份数据与两维特征向量都用变换后的数据对比，你会注意到，这个数据就是另一份数据的第一列。所以，如果你画出这个数据的图，这份数据只有一维，那么结果其实就是图3.3数据点$x$的坐标点。我们其实就是高效的抛弃了其他的坐标轴，也就是其他的特征向量。</p>
<p>那么我究竟做了什么呢? 本质上我们把数据进行了变换，使之可以用相关的模式进行表示，这些模式就是一些最适合描述这些数据之间关系的线。这么做非常有用，因为我们现在已经把数据点对每条线的贡献进行分类，然后进行组合。首先，我们仅仅有$x$轴和$y$轴，这还不错，但是每个$x$和$y$的数据点其实并无法告诉我们，每个数据点和其他数据点之间的关系。现在数据点的值可以精确告诉我们数据点处于趋势线的位置（上面或者下面）。如果是两个特征向量都用的情况，我们仅仅是把数据转换以便于我们使这些数据与特征向量相关，而不是$x$轴和$y$轴。但是只留一维特征向量的分解移除了较小特征向量的贡献，是我们的数据只与保留的一维数据相关。</p>
<h5 id="3-1-1-把旧数据找回来"><a href="#3-1-1-把旧数据找回来" class="headerlink" title="3.1.1 把旧数据找回来"></a>3.1.1 把旧数据找回来</h5><p>显然，如果你用PCA对数据进行压缩，你一定想把原始数据恢复回来。（下一章我们看到例子）这些内容来自于<a href="http://www.vision.auc.dk/sig/Teaching/Flerdim/Current/hotelling/hotelling.html" target="_blank" rel="noopener">这里</a>。<br>\begin{align}<br>\bf{转换后的数据}=\begin{array}{c|c}<br>x&amp;y\<br>\hline<br>-0.827970186 &amp; -0.175115307\<br>1.77758033 &amp; 0.142857227\<br>-0.992197494 &amp; 0.384374989\<br>-0.274210416 &amp; 0.1304117207\<br>-1.67580142 &amp; -0.209498461\<br>-0.912949103 &amp; 0.17528282444\<br>0.0991094375 &amp; -0.349824698\<br>1.14457216 &amp; 0.0464172582\<br>0.438046137 &amp; 0.0177646297\<br>1.22382056 &amp; -0.162675287\<br>\end{array}<br>\end{align}<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-29-134547.jpg" alt><br>$$图3.3：应用了PCA分析后并使用了两个特征向量的数据表以及绘制的新数据点$$</p>
<p>转换后的数据(单一特征向量)<br>\begin{array}{c}<br>x\<br>\hline<br>-0.827970186\<br>1.77758033\<br>-0.992197494\<br>-0.274210416\<br>-1.67580142\<br>-0.912949103\<br>1.14457216\<br>0.438046137\<br>1.22382056<br>\end{array}<br>$$图3.4：只用最重要的特征向量数据转换的数据$$</p>
<p>所以，我们怎么把原来数据恢复回来？在我们进行恢复原始数据之前，回忆只有我们将所有特征向量进行转换才能精确的把数据恢复回来。如果我们在最后转换时减少特征向量，那么恢复的数据已经失去很多信息。<br>回想一下，最后的变换是：<br>$$\bf{最终数据=行特征的向量} \times \bf{行调整后的数据}$$<br>我们可以把公式反转过来，进而得到原始数据，<br>$$\bf{行调整后的数据}=\bf{行特征的向量}^{-1}\times\bf{最终数据}$$<br>这里，$\bf{行特征的向量}^{-1}$是的$\bf{行特征的向量}$的逆。由于我们讨论的是特征向量，组成的特征的向量，所以，$\bf{行特征的向量}^{-1}$其实就是$\bf{行特征的向量}$的转置。当然，只有在矩阵中的所有元素是由单位特征向量组成是才成立。这样，恢复原始数据又变得容易了很多，现在公式变成了：<br>$$\bf{行调整后的数据}=\bf{行特征的向量}^{T}\times\bf{最终数据}$$<br>这个公式在我们只保留部分特征向量的情况下仍然成立。也就是说，就算你扔掉了一下特征向量，上面的公式仍然成立。</p>
<p>我不会演示用所有特征向量恢复原始数据，因为这样计算的结果和开始的数据一模一样。但是，我们一起来看一下只保留了一维特征向量的情况下，是怎样损失信息的。图3.5展示了丢失信息的情况。我们把图中的数据点与图3.1对比一下就会发现，沿着主特征向量的变化被保留下来了（见图3.2特征向量及数据）沿着其他成分（另一个特征向量被扔掉了）的变化丢失了。</p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-30-003805.jpg" alt><br>$$图3.5 从单独一维特征向量重新构造的数据$$</p>
<h4 id="练习-3"><a href="#练习-3" class="headerlink" title="练习"></a>练习</h4><ol>
<li>协方差矩阵的特征向量为我们提供了什么呢？</li>
<li>我们在PCA计算的过程中，哪一步可以决定压缩数据，压缩可以起到什么效果呢？</li>
<li>举例说明PCA在图像处理怎样用主成分表示，同时调研一下人脸识别中“特征脸”(Eigenfaces)主题。</li>
</ol>
<h3 id="第四章-计算机视觉应用"><a href="#第四章-计算机视觉应用" class="headerlink" title="第四章 计算机视觉应用"></a>第四章 计算机视觉应用</h3><p>本章我们将简单的PCA在计算机视觉领域的应用，首先我们看一下图像是怎么表示的，然后我们我们看看能怎样用PCA处理这些图像。本章关于人脸识别的的信息主要来自于1997年IEEE 9月Vol 85, No. 9《Face Recognition: Eigenface, Elastic Matching, and Neural Nets》。图像表示来自于爱迪生-韦斯利出版社1987年出版的由Rafael C. Gonzalez 和Paul Wintz合著的《Digital Image Processing》想了解更多信息，KL变换相关知识也是非常好的参考。图像压缩相关知识来自于<a href="http://www.vision.auc.dk/sig/Teaching/Flerdim/Current/hotelling/hotelling.html" target="_blank" rel="noopener">这里</a>，此网站还提供了大量用不同数量特征向量重新构造图像的方法。</p>
<h4 id="4-1-表示"><a href="#4-1-表示" class="headerlink" title="4.1 表示"></a>4.1 表示</h4><p>在我们把一系列矩阵技术应用于计算机视觉上时，我们必须考虑图像的表示方法。一个正方形，$N\times N$的图像可以被表示成$N^2$维的向量。<br>$$X=(x_1,x_2,x_3,…,x_{N^2})$$<br>这里，第一行前$N$个$(x_1–x_n)$一个挨着一个的像素点组成了1维的图像，下$N$个元素是下一行，以此类推。每个像素点的值代表图像三原色的亮度，也可能是只是灰度图像，那么只需要1个单独的值即可表示。</p>
<h4 id="4-2-PCA寻找模式"><a href="#4-2-PCA寻找模式" class="headerlink" title="4.2 PCA寻找模式"></a>4.2 PCA寻找模式</h4><p>假设我们有20个图像。每个图像的像素非常高。对每个图像，我们都建立一个图像向量表示相应图像。接着我们就可以把所有的图像放到一个像这样的大矩阵中：<br>$$图像矩阵=<br>\begin{pmatrix}<br>ImageVec_1\<br>ImageVec_2\<br>\vdots\<br>ImageVec_{20}\<br>\end{pmatrix}$$<br>我们现在就可以开始以这条图像矩阵为基始，应用PCA，先构造协方差矩阵，然后得到原始数据相关的特征向量。为什么用PCA分析有用呢？假设我们要做人脸识别，那我们的原始数据就是很多人脸。接下来的问题是，给一张我新的图片，那么这是原始人脸数据中谁的人脸呢（注意，这新的图片不是我们开始给的20个人脸图片）？计算机视觉的处理方法是衡量新的图片和原始图片的差别，但并不是在原始坐标系进行对比，而是在PCA分析的生成的坐标系下衡量。</p>
<p>在实际应用中，PCA生成的坐标系下识别人脸会好非常多，因为PCA分析已经提供了原始图片中不同和相似等相关性。主成分分析已经识别出了数据中的统计学模式。</p>
<p>因为所有的向量都是$N^2$维的，我们最后会得到$N^2$个特征向量，在实践中，我们可以扔掉其中不重要的一些特征向量，识别效果仍然非常好。</p>
<h4 id="4-3-PCA图像压缩"><a href="#4-3-PCA图像压缩" class="headerlink" title="4.3 PCA图像压缩"></a>4.3 PCA图像压缩</h4><p>使用PCA做图像压缩常常也被称作霍特林变换或者是KL变换（Karhunen-Leove transform）. 如果我们有20个图像，每个图像有$N^2$个像素，所以我们就可以构造$N^2$个向量，每个向量20维。每个向量由每个图片的相同像素点的图片亮度值组成。这与我们之前的例子不同，因为之前我们是有一个图像的向量，向量里的每项都是不同的像素。然而我们现在是有一个每个像素的向量，向量里的每项是都是来自于不同的图片。</p>
<p>如果现在我们在一个数据集上应用PCA，那么，我们将会得到20个特征向量，因为，每个向量都是20维的。如果想要压缩数据，我们可以选择只用其中一部分特征向量变换，假设是15个特征向量。这样我得到的最终数据只有15维，达到了节省空间的目的。但是，当要恢复原始数据是，图像已经丢了一些信息。这种压缩技术叫做有损压缩，因为解压后的图片已经不是和原始图片完全一样的图片了，一般来说会变差。</p>
<h3 id="附录-A"><a href="#附录-A" class="headerlink" title="附录 A"></a>附录 A</h3><h4 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h4><p>这份代码用于可替换Matlab的自由软件Scilab。我用这份代码生成了文章的所有例子。除了第一个宏，剩下的都是我(原文作者)写的。</p>
<figure class="highlight scilab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This macro taken from</span></span><br><span class="line"><span class="comment">// http://www.cs.montana.edu/ ̃harkin/courses/cs530/scilab/macros/cov.sci // No alterations made</span></span><br><span class="line"><span class="comment">// Return the covariance matrix of the data in x, where each column of x</span></span><br><span class="line"><span class="comment">// is one dimension of an n-dimensional data set.  That is, x has x columns</span></span><br><span class="line"><span class="comment">// and m rows, and each row is one sample.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// For example, if x is three dimensional and there are 4 samples.</span></span><br><span class="line"><span class="comment">// x=[123;456;789;101112]</span></span><br><span class="line"><span class="comment">// c=cov(x)</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">c</span>]=<span class="title">cov</span> <span class="params">(x)</span></span></span><br><span class="line"><span class="comment">// Get the size of the array</span></span><br><span class="line">sizex=<span class="built_in">size</span>(x);</span><br><span class="line"><span class="comment">// Get the mean of each column</span></span><br><span class="line">meanx = mean (x, <span class="string">"r"</span>);</span><br><span class="line"><span class="comment">// For each pair of variables, x1, x2, calculate</span></span><br><span class="line"><span class="comment">// sum ((x1 - meanx1)(x2-meanx2))/(m-1)</span></span><br><span class="line"><span class="keyword">for</span> var = <span class="number">1</span>:sizex(<span class="number">2</span>),</span><br><span class="line">        x1 = x(:,var);</span><br><span class="line">        mx1 = meanx (var);</span><br><span class="line">        <span class="keyword">for</span> ct = var:sizex (<span class="number">2</span>),</span><br><span class="line">                x2 = x(:,ct);</span><br><span class="line">                mx2 = meanx (ct);</span><br><span class="line">                v = ((x1 - mx1)’ * (x2 - mx2))/(sizex(<span class="number">1</span>) - <span class="number">1</span>);</span><br><span class="line"><span class="keyword">end</span>, c=cv;</span><br><span class="line"><span class="keyword">end</span>,</span><br><span class="line">cv(var,ct) = v;</span><br><span class="line">cv(ct,var) = v;</span><br><span class="line"><span class="comment">// do the lower part of c also.</span></span><br><span class="line"><span class="comment">// This a simple wrapper function to get just the eigenvectors</span></span><br><span class="line"><span class="comment">// since the system call returns 3 matrices</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">x</span>]=<span class="title">justeigs</span> <span class="params">(x)</span></span></span><br><span class="line"><span class="comment">// This just returns the eigenvectors of the matrix</span></span><br><span class="line">[a, eig, b] = bdiag(x);</span><br><span class="line">x= eig;</span><br><span class="line"><span class="comment">// this function makes the transformation to the eigenspace for PCA</span></span><br><span class="line"><span class="comment">// parameters:</span></span><br><span class="line"><span class="comment">// adjusteddata = mean-adjusted data set</span></span><br><span class="line"><span class="comment">// eigenvectors = SORTED eigenvectors (by eigenvalue)</span></span><br><span class="line"><span class="comment">// dimensions  = how many eigenvectors you wish to keep</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// The first two parameters can come from the result of calling</span></span><br><span class="line"><span class="comment">// PCAprepare on your data.</span></span><br><span class="line"><span class="comment">// The last is up to you.</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">finaldata</span>] = <span class="title">PCAtransform</span><span class="params">(adjusteddata,eigenvectors,dimensions)</span> <span class="title">finaleigs</span> = <span class="title">eigenvectors</span><span class="params">(:,1:dimensions)</span>;</span></span><br><span class="line">prefinaldata = finaleigs’*adjusteddata’;</span><br><span class="line">finaldata = prefinaldata’;</span><br><span class="line"><span class="comment">// This function does the preparation for PCA analysis</span></span><br><span class="line"><span class="comment">// It adjusts the data to subtract the mean, finds the covariance matrix,</span></span><br><span class="line"><span class="comment">// and finds normal eigenvectors of that covariance matrix.</span></span><br><span class="line"><span class="comment">// It returns 4 matrices</span></span><br><span class="line"><span class="comment">// meanadjust = the mean-adjust data set</span></span><br><span class="line"><span class="comment">// covmat = the covariance matrix of the data</span></span><br><span class="line"><span class="comment">// eigvalues = the eigenvalues of the covariance matrix, IN SORTED ORDER</span></span><br><span class="line"><span class="comment">// normaleigs = the normalised eigenvectors of the covariance matrix,</span></span><br><span class="line"><span class="comment">// IN SORTED ORDER WITH RESPECT TO</span></span><br><span class="line"><span class="comment">// THEIR EIGENVALUES, for selection for the feature vector.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span> This function cannot handle data sets that have any eigenvalues</span></span><br><span class="line"><span class="comment">// equal to zero. It’s got something to do with the way that scilab treats</span></span><br><span class="line"><span class="comment">// the empty matrix and zeros.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">meanadjusted</span>,<span class="title">covmat</span>,<span class="title">sorteigvalues</span>,<span class="title">sortnormaleigs</span>] = <span class="title">PCAprepare</span> <span class="params">(data)</span> // <span class="title">Calculates</span> <span class="title">the</span> <span class="title">mean</span> <span class="title">adjusted</span> <span class="title">matrix</span>, <span class="title">only</span> <span class="title">for</span> 2 <span class="title">dimensional</span> <span class="title">data</span></span></span><br><span class="line">means = mean(data,<span class="string">"r"</span>);</span><br><span class="line">meanadjusted = meanadjust(data);</span><br><span class="line">covmat = cov(meanadjusted);</span><br><span class="line">eigvalues = spec(covmat);</span><br><span class="line">normaleigs = justeigs(covmat);</span><br><span class="line">sorteigvalues = sorteigvectors(eigvalues’,eigvalues’);</span><br><span class="line">sortnormaleigs = sorteigvectors(eigvalues’,normaleigs);</span><br><span class="line"><span class="comment">// This removes a specified column from a matrix</span></span><br><span class="line"><span class="comment">// A = the matrix</span></span><br><span class="line"><span class="comment">// n = the column number you wish to remove</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">columnremoved</span>] = <span class="title">removecolumn</span><span class="params">(A,n)</span></span></span><br><span class="line">inputsize = <span class="built_in">size</span>(A);</span><br><span class="line">numcols = inputsize(<span class="number">2</span>);</span><br><span class="line">temp = A(:,<span class="number">1</span>:(n<span class="number">-1</span>));</span><br><span class="line"><span class="keyword">for</span> var = <span class="number">1</span>:(numcols - n)</span><br><span class="line">        temp(:,(n+var)<span class="number">-1</span>) = A(:,(n+var));</span><br><span class="line">columnremoved = temp;</span><br><span class="line"><span class="comment">// This finds the column number that has the</span></span><br><span class="line"><span class="comment">// highest value in it’s first row.</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">column</span>] = <span class="title">highestvalcolumn</span><span class="params">(A)</span></span></span><br><span class="line">inputsize = <span class="built_in">size</span>(A);</span><br><span class="line">numcols = inputsize(<span class="number">2</span>);</span><br><span class="line">maxval = A(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">maxcol = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> var = <span class="number">2</span>:numcols</span><br><span class="line">        <span class="keyword">if</span> A(<span class="number">1</span>,var) &gt; maxval</span><br><span class="line">                maxval = A(<span class="number">1</span>,var);</span><br><span class="line"><span class="keyword">end</span>,</span><br><span class="line">        <span class="keyword">end</span>,</span><br><span class="line">column = maxcol</span><br><span class="line">maxcol = var;</span><br><span class="line"><span class="number">25</span></span><br><span class="line"><span class="keyword">end</span>,</span><br><span class="line"><span class="comment">// This sorts a matrix of vectors, based on the values of</span></span><br><span class="line"><span class="comment">// another matrix</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// values = the list of eigenvalues (1 per column)</span></span><br><span class="line"><span class="comment">// vectors = The list of eigenvectors (1 per column)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span>  The values should correspond to the vectors</span></span><br><span class="line"><span class="comment">// so that the value in column x corresponds to the vector</span></span><br><span class="line"><span class="comment">// in column x.</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">sortedvecs</span>] = <span class="title">sorteigvectors</span><span class="params">(values,vectors)</span></span></span><br><span class="line">inputsize = <span class="built_in">size</span>(values);</span><br><span class="line">numcols  = inputsize(<span class="number">2</span>);</span><br><span class="line">highcol = highestvalcolumn(values);</span><br><span class="line">sorted = vectors(:,highcol);</span><br><span class="line">remainvec = removecolumn(vectors,highcol);</span><br><span class="line">remainval = removecolumn(values,highcol);</span><br><span class="line"><span class="keyword">for</span> var = <span class="number">2</span>:numcols</span><br><span class="line">        highcol = highestvalcolumn(remainval);</span><br><span class="line">        sorted(:,var) = remainvec(:,highcol);</span><br><span class="line">        remainvec = removecolumn(remainvec,highcol);</span><br><span class="line">        remainval = removecolumn(remainval,highcol);</span><br><span class="line"><span class="keyword">end</span>,</span><br><span class="line">sortedvecs = sorted;</span><br><span class="line"><span class="comment">// This takes a set of data, and subtracts</span></span><br><span class="line"><span class="comment">// the column mean from each column.</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">meanadjusted</span>] = <span class="title">meanadjust</span><span class="params">(Data)</span></span></span><br><span class="line">inputsize = <span class="built_in">size</span>(Data);</span><br><span class="line">numcols = inputsize(<span class="number">2</span>);</span><br><span class="line">means = mean(Data,<span class="string">"r"</span>);</span><br><span class="line">tmpmeanadjusted = Data(:,<span class="number">1</span>) - means(:,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> var = <span class="number">2</span>:numcols</span><br><span class="line">        tmpmeanadjusted(:,var) = Data(:,var) - means(:,var);</span><br><span class="line">meanadjusted = tmpmeanadjusted</span><br><span class="line"><span class="keyword">end</span>,</span><br></pre></td></tr></table></figure>
<p><a href="http://cmb.oss-cn-qingdao.aliyuncs.com/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89.pdf" target="_blank" rel="noopener">本文PDF</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/03/18/Naive-Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/18/Naive-Bayes/" itemprop="url">Naive Bayes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-18T21:06:08+01:00">
                2017-03-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>You are sitting in front your sceen, annoyed by a bunch of spam mails. You wonder if there are any appoaches to get rid of so much many offended emails. Last time you doped out a extremely good idea. You set a series of words to identify those emails: every mail invovled by words “coupon” was trown to trash. However, on one hand, there were only about 10% spam including “coupon”, one the other hand, you had trashed two significant emails, due to which, you lost two business valued about two million dollars. The thing was that, your inbox seems being overrun by those spams. Who can rescue you from endless deleting spams everyday?</p>
<h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>Actually, you were close to the answer when you were putting all emails to trash which included the word “coupon”. Today, we learn about an efficient method to solve the problem systematically.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-18-110227.jpg" alt><br>Maybe we can search through internet, find all words which are about advertisment in email. You can restrict that if and only if those which includes more than 4 word in the list of spam words can be put into trash can. Maybe finally you can design a rule system to recoginize those spams without miss important ones. But it seems so boring a job to do this, moreover, it may be not personalized. If some those who are working for saling discount stuffs, he/she may find it doesn’t work through applying your effective rules. How about thinking of probablity of those words emerge in all your inbox. Some words such as “coupon” may contribute more but not entirety, simultaneously, affordable may contribute less but not none. Notice that profit emerge in spam emails and normal emails both sometimes.<br>We let $y=0$ denote an normal email while $y=1$ the opposite. And if a word such as “coupon” emerges in a mail, we set $coupon=1$, otherwise $coupon=0$. Suppose you have an email, we define the probability to :<br>\begin{align}<br>&amp;p1=p(y=0|free=1, discount=1, affordable=1, customer=0, KPI=0, budget=0,…,bias=0)=?\<br>&amp;p2=p(y=1|free=1, discount=1, affordable=1, customer=0, KPI=0, budget=0,…,bias=0)=?<br>\end{align}<br>Our aim is to decide which is bigger $p1$ or $p2$. Let’s describe the problem as followed:</p>
<blockquote>
<p>We want to decide the probality of a mail spam or normal when word “free” is in the mail, “discount” is in the mail, “affordable” is in the mail, customer is not in the mail, …, “bias” is not.</p>
</blockquote>
<p>The description above is only about one email. For some other emails, maybe “free” and “discount” both did not emerge at all. </p>
<h3 id="Definitioin"><a href="#Definitioin" class="headerlink" title="Definitioin"></a>Definitioin</h3><p>To generalize the problem, we let $x_i$ denote a word in emails. Suppose we now know all words in your inbox, say 10 thousand words. Then we have $x_1$ denote if “free” is in a mail, $x_1=0$ denotes negative while $x_1=1$ denotes positive. So we have $x_1,x_2,x_3,…,x_{10000}$, which denotes the status of each word in a mail. Then the problem is transferred as followed:<br>\begin{align}<br>&amp;p1=p(y=0|x_1=1, x_2=1, x_3=1, x_4=0, x_5=0, x_6=0,…,x_{10000}=0)=?\<br>&amp;p2=p(y=1|x_1=1, x_2=1, x_3=1, x_4=0, x_5=0, x_6=0,…,x_{10000}=0)=?<br>\end{align}<br>Suppose we have N words in your inbox, then we want to decide:<br>\begin{align}<br>&amp;p1=p(y=0|x_1, x_2,…,x_{N})=?\<br>&amp;p2=p(y=1|x_1, x_2,…,x_{N})=?<br>\end{align}<br>So how to sovle the probability problem?</p>
<h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><p>Recall Bayes rules:<br>\begin{equation}<br>p(y|x)=\frac{p(x,y)}{p(x)}=\frac{p(x|y)p(y)}{p(x)}<br>\end{equation}<br>we apply the equation to our problem, then we have:<br>\begin{align}<br>&amp;p(y=1|x_1,x_2,…,x_N)\<br>&amp; =\frac{p(x_1,x_2,…,x_N,y=1)}{p(x_1,x_2,…,x_N)}\<br>&amp; =\frac{p(x_1,x_2,…,x_N|y)p(y=1)}{p(x_1,x_2,…,x_N)}\<br> \<br>&amp;p(y=0|x_1,x_2,…,x_N)\<br>&amp; =\frac{p(x_1,x_2,…,x_N,y=0)}{p(x_1,x_2,…,x_N)}\<br>&amp; =\frac{p(x_1,x_2,…,x_N|y)p(y=0)}{p(x_1,x_2,…,x_N)}<br>\end{align}<br>Notice that {p(x_1,x_2,…,x_N) is positive, and a constant as well. so our aim is transferred to:<br>\begin{align}<br>&amp;Max(p(y=1|x_1,x_2,…,x_N), p(y=0|x_1,x_2,…,x_N)\<br>&amp;=Max(p(x_1,x_2,…,x_N|y=1)p(y=1),p(x_1,x_2,…,x_N|y=0)p(y=0)<br>\end{align}</p>
<p>First of all, we talk about how to get $p(y=0)$ and $p(y=1)$. We have $N=10000$, suppose there are $900$ spams and $91000$ normal emails, then:<br>\begin{align}<br>&amp;p(y=0)=\frac{count(spam\ email)}{count(all\ emails)}=\frac{900}{10000}=0.09\<br>&amp;p(y=1)=\frac{count(normal\ email)}{count(all\ emails)}=\frac{9100}{10000}=0.91<br>\end{align}<br>And right now our task left is to compute $p(x_1,x_2,…,x_N|y=0)$ and $p(x_1,x_2,…,x_N|y=1)$, that means we want to know if a mail is a normal one or not, what is the probability of the combination of these $N=10000$ words. In other word, $x_1=0\ or\ 1$,$x_2=0\ or\ 1$ and so on. So we have to compute $2<em>2^{10000}=2</em>1.995*10^{3010}$ probabilities under our circumstance. It seems the scale is so large that we can not handle it. So we have an extremely adventurous assumption:</p>
<blockquote>
<p>Each x in an email only can be decided by y.</p>
</blockquote>
<p>Then we have:<br>\begin{align}<br>p(x_1,x_2,…,x_N|y=0)=p(x_1|y=0)\cdot p(x_2|y=0)\cdots p(x_N|y=0)\<br>p(x_1,x_2,…,x_N|y=1)=p(x_1|y=1)\cdot p(x_2|y=1)\cdots p(x_N|y=1)<br>\end{align}<br>Now, we just need compute 2*10000 probabilities, it is surely a mission possible now. So how to compute $p(x_i=0\ or\ 1|y=0\ or\ 1)$? Take word “free” for example, we want to compute:<br>\begin{align}<br>&amp;p(free=0|y=0)\<br>&amp;=\frac{p(free=0, y=0)}{p(y=0)}\<br>&amp;=\frac{count\ of\ emails\ have\ no\ word\ “free”\ in\ normal\ emails}{count\ of\ normal\ emails}\<br>&amp;p(free=0|y=1)\<br>&amp;=\frac{p(free=0, y=1)}{p(y=1)}\<br>&amp;=\frac{count\ of\ emails\ have\ no\ word\ “free”\ in\ spams}{count\ of\ spam\ emails}<br>\end{align}<br>As long as we have computed all of these values, we can use the equation to decide which email should be trashed. Suppose we have computed the probabilites of $p1$ and $p2$:<br>\begin{align}<br>&amp;p1\triangleq p(x_1,x_2,…,x_N|y=1)p(y=1)\triangleq p(y=0)\cdot \Pi_{i=1}^N p(x_i|y=0)=0.00091\<br>&amp;p2\triangleq p(x_1,x_2,…,x_N|y=0)p(y=0)\triangleq p(y=1)\cdot \Pi_{i=1}^N p(x_i|y=1)=0.00000032<br>\end{align}<br>Then we consider that the email is more of a spam mail.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Today we have talked about how to run a Naive Bayes algorithm to decide if an email is a spam. We suppose each word in a mail is no of business of the other, which is a simple assumption named <strong>conditional independence assumption</strong>  but not the reality(e.g. “coupon” maybe emerges with “save” and “money” due to their inner association). However, the algorthm is very effective. </p>
<h3 id="Future-Thinking"><a href="#Future-Thinking" class="headerlink" title="Future Thinking"></a>Future Thinking</h3><p>Suppose if a word “wooooo” haven’t emerged in inbox, then the probability will reach $p1=p2=0$, how to solve it?<br>Suppose you have a task to differiate oranges from apples and pears using color and shape, how to design the algorithm?<br>suppose x is continuous rather than discrete, Naive Bayes still works or not?</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/03/11/neural-network-ABC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/11/neural-network-ABC/" itemprop="url">Neural Network ABC</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-11T19:46:55+01:00">
                2017-03-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>Deep learning is very popular recently, which is based on Neural Network, an old algorithm that had degraded for years but is resurging right now. We talk about some basic concept about Neural network today, hoping supply a intuitive perspective of it.</p>
<p>Before beginning, I’d like to introduce you an exicting product which help those who are blind see the world. BrianPort, which is invented by Wicab, uses you tougue to see the world. Tongue array contains 400 electrodes and is connected to the glasses. The product transfers from light to electric signal. More than 80% blind persons could pass through the block during the experiments.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-035015.jpg" alt></p>
<p>In fact, Wicab takes advantage the mechanism of neural network of our brain. There are 86 billion neuron in our brain. We can smell, see, hear the world just because of these neurons. They are connect to each other to help us sense the world. Algorithm Neural Network is a way of mimic the mechanism of our brain. </p>
<h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>Let’s start from the easiest model, we get $a_1$ in two steps:<br>step1: $z_1=w_1x_1+w_2x_2+w_3x_3$<br>step2: $a_1=\frac{1}{1+e^{(-z)}}$<br>In addition, we add a bias $w_0$ to the calculate. After letting $x_0=1$, then:<br>$$z=w_0x_0+w_1x_1+w_2x_2+w_3x_3$$  We always add a bias at each layer but the last to Neural Network.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-043241.jpg" alt><br>If we contrast this model with logistic regression model, ww find that right now the to model is just the same: input every $x$ represents a feature. In logistic regression, we want to train a model $h_w(x)=\frac{1}{1+e^{-W^Tx}}$. The simpliest Neural Network, the model is a little complex, but if we do not take hidden layer into account, the model is just logistic regression.</p>
<h3 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h3><p>To approach the authentic Neural Network, we add two more nerons($a_2^{(2)}$ and $a_1^{(3)}$) to logistic regression model. Notice that the model inner green triangle box is just like logistic regression demonstrated above. There are only two layers in Logistic Regression, in contrast, we can add more layers like L2 layer. In Neural Network, we call these layers hidden layers which are neither the input(e.g. layer have $x_1, x_2, x_2$), nor the output $h(x)$. The figure below has only one hidden layer, though we can add many hidden layers to the model.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-122045.jpg" alt><br>Look at the figure above, let’s look at the definition of Neural Network, take $w_{12}^{(1)}$ for example, the subscript $_{12}$ represents the weight from the former layer $2nd$ unit to the current layer $1st$ unit. The superscript $^1$ represents former layer is layer L1. These $w$ are named weights of Neural Network. The sigmoid function $f=\frac{1}{1+e^{-x}}$ is activation function. We can choose other activation function such as symmetrical sigmoid $S(x)=\frac{1-e^{-x}}{1+e^{-x}}$. Now let’s think about how to calculate $h(x)$, for the L2 layer, we have:<br>\begin{align}<br>&amp; z_1^{(2)}=w_{10}^{(1)}x_0 + w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 +  w_{13}^{(1)}x_3\<br>&amp; z_2^{(2)}=w_{20}^{(1)}x_0 + w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 +  w_{23}^{(1)}x_3\<br>&amp; a_1^{(2)} = g(w_{10}^{(1)}x_0 + w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 +  w_{13}^{(1)}x_3)=g(z_1^{2})\<br>&amp; a_2^{(2)} = g(w_{20}^{(1)}x_0 + w_{21}^{(1)}x_1 + w_{22}^{(1)}x_2 +  w_{23}^{(1)}x_3)=g(z_2^{2})<br>\end{align}<br>Here, $g()$ is the activation function. Notice that if we use matrices represent the equation, result will be simpler:<br>$$a^{(2)} = g(z^{(2)}) = g(W^{(1)} a^{(1)})$$<br>Here, we let $a_i^{(1)}=x_i$. We can conclude one more step, for layer k, we have:<br>$$a^{(k)} = g(z^{(k)}) = g(W^{(k-1)} a^{(k-1)})$$<br>Then for the L3 Layer, we have only one neural:<br>\begin{align}<br>h(x) = a_1^{3}=g(w_{10}^{(1)}a_0^{(2)} + w_{11}^{(1)}a_1^{(2)} + w_{12}^{(1)}a_2^{(2)})=g(z_1^{3})<br>\end{align}<br>If we substitute $a_1^{2}$ and $a_2^{2}$ for elme $h(x)$, we have:<br>\begin{align}<br>h(x)=a_1^{3}=g(w_{10}^{(1)}\cdot 1 + w_{11}^{(1)}<br>\cdot g(z_1^{(2)})+ w_{12}^{(1)}\cdot g(z_2^{(2)}))<br>\end{align}<br>The formula show that we use $g()$ function once and once again to nest the input, and compute the output eventaully. It is rather a non-linear classifier than linear classifier such as Linear Regression and Logistic Regression.</p>
<h3 id="More-Complicated-Network"><a href="#More-Complicated-Network" class="headerlink" title="More Complicated Network"></a>More Complicated Network</h3><p>A Neural Network can be very complex, as long as we add more hidden layer into the network, the figure showed below is a neural network which has 20 layers, which means it has 1 input layer, 1 output layer and 18 hidden layers. From the connected weight we can imagine how much many weight we would calculate if we want to train such a big Neural Network. Notice that we add a bias subscript with zero on each layer except the output layer. And in each layer, we can add different amount of nerons. If we want to recognize numer image in zipcode from 0~9, we can design the Neural Network with 10 outputs in the output layer.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-08-135243.jpg" alt></p>
<h3 id="Simple-Applications"><a href="#Simple-Applications" class="headerlink" title="Simple Applications"></a>Simple Applications</h3><p>This section, I’d like to construct a Neural Network to simulate a logic gate. Remember that bias $x_0$ is always $1$. Now let set $w_{10},$$w_{11}$ and $w_{12}$, and find what will h(x) become:<br>$$w_{10}=-30\,,w_{11}=20\,,w_{12}=20\,$$</p>
<table>
<thead>
<tr>
<th>$x_1$</th>
<th>$x_2$</th>
<th>$z_1$</th>
<th>$a_1$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>-30</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>-10</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>-10</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>10</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Here we take advantge the property of sigmoid function, $g(-10)=4.5\times 10^{-5}\approx 0$ and $g(10)=0.99995\approx 1$. From the table we have constructed an $AND$ logic gate. It is easy to construct an $OR$ logic gate. We just set:<br>    $$w_{10}=-30\,,w_{11}=50\,,w_{12}=50\,$$<br>Then we get an $OR$ logic gate. We can construct $NOR$ gate as well, just set:<br>$$w_{10}=10\,,w_{11}=-20\,,w_{12}=-20\,$$<br>Question: can we construct a $XOR$ gate?<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-09-052259.jpg" alt><br>In fact, we can get a more powerful logic gate through adding more hidden layers. Only 2 layers of Neural Network can not construct a $XOR$ gate but 3 layers can. Neural Network shown below can implement function as $XOR$ logic gate.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-09-085822.jpg" alt><br>The weights matrices is as followed, we can testify through table listed.<br>\begin{align}<br>&amp;W^{(1)}=\begin{bmatrix}-30&amp;20&amp;20\<br>10&amp;-20&amp;-20<br>\end{bmatrix}\<br>&amp;W^{(2)}=\begin{bmatrix}10&amp;-20&amp;-20<br>\end{bmatrix}<br>\end{align}</p>
<table>
<thead>
<tr>
<th>$x_1$</th>
<th>$x_2$</th>
<th>$a_1^{(2)}$</th>
<th>$a_2^{(2)}$</th>
<th>$a_1^{(3)}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>From examples we have seen, hope you can gain intuition about Neural Network. We can generate more abstract features through adding hidden layers.</p>
<h3 id="Summerize"><a href="#Summerize" class="headerlink" title="Summerize"></a>Summerize</h3><p>Today we used Logistic Regression adding hidden layers to generate Neural Network. Then we talked about how to represent a Neural Network. In the end, we found that Neural Network can simulate logic gate. We do not talk about how to train a Neural Network here. Usually we use Backpropagation Algorithm to train a Neural Network. </p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning</a></li>
<li>《Neural Networks》by Raul Rojas</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://commanber.com/2017/03/04/linear-regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mingbo Cheng">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/04/linear-regression/" itemprop="url">Linear Regression for Trump</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-04T20:36:45+01:00">
                2017-03-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>In the end of 2016, Trump occupied the presidency. We are always thinking about how he will build the wall between US and Mexico, Today, I’d like to compute how many resources he would cost if he truly began to build the wall using linear regression. </p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>The table below is some basic information about Chinese GreatWall as well as one item about Hadrian’s Wall：</p>
<table>
<thead>
<tr>
<th>Age</th>
<th>people(1000)</th>
<th>Years</th>
<th>Length(KM)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qin Dynasty</td>
<td>300</td>
<td>15</td>
<td>5000</td>
</tr>
<tr>
<td>Han Dynasty</td>
<td>500</td>
<td>100</td>
<td>5000</td>
</tr>
<tr>
<td>North Northern Dynasties</td>
<td>1800</td>
<td>12</td>
<td>2800</td>
</tr>
<tr>
<td>Sui Dynasty</td>
<td>1280</td>
<td>30</td>
<td>350</td>
</tr>
<tr>
<td>Ming Dynasty</td>
<td>3000</td>
<td>40</td>
<td>885</td>
</tr>
<tr>
<td>Hadrian’s Wall</td>
<td>18</td>
<td>14</td>
<td>117</td>
</tr>
</tbody>
</table>
<p>We use matrices represent the resources and the length of these walls. Each row of $x$ represents the quantity of (1000 people and year) men and years, and each row of $y$ denotes the length of walls(KiloMeter) </p>
<p>\begin{align}<br>X=\begin{bmatrix}<br>5000\<br>5000\<br>2800\<br>350\<br>8851\<br>117<br>\end{bmatrix}<br>y=<br>\begin{bmatrix}<br>300*15\<br>500*100\<br>1800*12\<br>1280*30\<br>3000*40\<br>18*14<br>\end{bmatrix}<br>\end{align}</p>
<p>Let’s draw the picture of these data:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">X=[<span class="number">5000</span>;<span class="number">5000</span>;<span class="number">2800</span>;<span class="number">350</span>;<span class="number">8851</span>;<span class="number">117</span>];</span><br><span class="line">y=[<span class="number">300</span>\*<span class="number">15</span>;<span class="number">500</span>\*<span class="number">100</span>; <span class="number">1800</span>\*<span class="number">12</span>;<span class="number">1280</span>\*<span class="number">30</span>;<span class="number">3000</span>\*<span class="number">40</span>; <span class="number">18</span>\*<span class="number">14</span>];</span><br><span class="line">xlabel(<span class="string">"x (km)"</span>);</span><br><span class="line">hold on;</span><br><span class="line">ylabel(<span class="string">"y (k people year)"</span>);</span><br><span class="line">plot(X,y,<span class="string">"ro"</span>, <span class="string">"MarkerFaceColor"</span>, <span class="string">"b"</span>);</span><br></pre></td></tr></table></figure></p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-01-113516.jpg" alt><br>We hope find a mapping $x\rightarrow f(x)$ ($x$ denotes the length of walls, $y$ denotes the cost of resources)drawing a line as the figure above which meets all data best. When we encounter new data(e.g. new length of wall), we hope $f(x)$ will help us find how many resources will we cost. This is the goal of linear regression. </p>
<h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>First of all, we assume that the line have the form as followed:<br>$$h_{\theta}(x)=\theta_0+\theta_1 x_1+\theta_2 x_2 + \cdots + \theta_nx_n$$<br>Specifically, under our circumstance, there is only one $x$(the length of walls), then we will have the form as below:<br>$$h_{\theta}(x)=\theta_0+\theta_1x$$<br>In fact, there are many factor infulence the outcome of the cost of wall besides people and time. Tools we use and the economy as well as terrain where building the wall. if we add these factors, maybe the assumption looks like:<br>$$h_{\theta}(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_2$$<br>$x_1$denotes the length of wall, while $x_2$ and $x_3$ denote economy and terrian condition respectively. For simplicity, we consider only the length of wall. </p>
<p>In general, we add a $x_0=1$ to the equation, the we have:<br>$$h_{\theta}(x)=\theta_0x_0+\theta_1 x_1 + \cdots + \theta_nx_n<br>=\sum_{i=1}^{n}\theta_ix_i$$<br>If we use matrices represent the equation, then<br>\begin{align}<br>\Theta=\begin{bmatrix}<br>\theta_0\<br>\theta_1\<br>\cdots\<br>\theta_n<br>\end{bmatrix}<br>\end{align}<br>thus,<br>\begin{align}<br>h_{\theta}(x)=\sum_{i=1}^{n}\theta_ix_i=<br>\begin{bmatrix}<br>\theta_0\theta_1\cdots\theta_n<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_0\<br>x_1\<br>\cdots\<br>x_n<br>\end{bmatrix}<br>=\Theta^T\overrightarrow{X}<br>\end{align}<br>Notice that $\Theta^T$ is a $1\times n$ matrices while $\overrightarrow{X}$ is a $n\times1$ matrices. We get a real number after multiply two matrices.</p>
<h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>We hope find a way to find the best function $h_\theta(x)$ fit these data. Notice that if $|(h_\theta(x_i)-y_i)|=0$ for each $x$, then the function fit data absolutely. In practice, if we minimize $|(h_\theta(x_i)-y_i)|$, we can find the best fit to original data. An optional cost function is square cost function:<br>$$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)^2$$<br>As long as we can minimize $J(\theta)$ with respect to $\theta$, can we find the best $\theta$ fit original data. The cost function looks like as followed:<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% octave</span></span><br><span class="line">m = <span class="built_in">size</span>(X,<span class="number">1</span>)</span><br><span class="line">XX = [<span class="built_in">ones</span>(m,<span class="number">1</span>),X];</span><br><span class="line">theta0 = <span class="built_in">linspace</span>(<span class="number">-100</span>,<span class="number">6000</span>, <span class="number">100</span>);    </span><br><span class="line">theta1 = <span class="built_in">linspace</span>(<span class="number">-20</span>,<span class="number">36</span>, <span class="number">100</span>);</span><br><span class="line">l0=<span class="built_in">length</span>(theta0);                               </span><br><span class="line">l1=<span class="built_in">length</span>(theta1);                               </span><br><span class="line">J_vs = <span class="built_in">zeros</span>(l0,l1);                             </span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:l0                                             </span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:l1                                         </span><br><span class="line">        t = [theta0(<span class="built_in">i</span>); theta1(<span class="built_in">j</span>)];               </span><br><span class="line">        J_vs(<span class="built_in">i</span>,<span class="built_in">j</span>) = cost(XX, y, t);             </span><br><span class="line">    <span class="keyword">end</span>                                                 </span><br><span class="line"><span class="keyword">end</span>                                                     </span><br><span class="line"><span class="built_in">figure</span>;                                                 </span><br><span class="line">J_vs = J_vs';                                       </span><br><span class="line">surfc(theta0, theta1, J_vs);                 </span><br><span class="line">colorbar;                                             </span><br><span class="line">xlabel(<span class="string">'\theta_0'</span>);                                </span><br><span class="line">ylabel(<span class="string">'\theta_1'</span>);</span><br></pre></td></tr></table></figure></p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-02-28-115912.jpg" alt></p>
<p>The figure has a bowl shape, and we can find the minimum of it both using matrices approach and gradient descent. </p>
<h3 id="Matrics-Solution"><a href="#Matrics-Solution" class="headerlink" title="Matrics Solution"></a>Matrics Solution</h3><p>Remember that the cost function $J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x_i)-y_i)^2$, It is more convinient if we apply $J(\theta)$ matrics form, then:<br>\begin{align}<br>J(\theta)=\frac{1}{2m}(h_\theta(x)-y)^T\cdot(h_\theta(x)-y)\<br>=\frac{1}{2m}(X\Theta-y)^T\cdot(X\Theta-y)<br>\end{align}<br>We can take the derivative of $J(\theta)$ and let it be $\overrightarrow{0}$. It is a little complex if we take the derivative of the last equation about $J(\theta)$. Let’s do the derivation a easy way, but not a total Matric solution. If we do the derivation on the sum, we can get:<br>$$\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^m(h_\theta(x_i)-y_i)x_j$$<br>Then,<br>\begin{align}<br>\frac{\partial}{\partial\Theta}J(\theta)=<br>\begin{bmatrix}<br>\frac{\partial}{\partial\theta_1}J(\theta)\<br>\frac{\partial}{\partial\theta_2}J(\theta)\<br>\frac{\partial}{\partial\theta_3}J(\theta)\<br>\cdots\<br>\frac{\partial}{\partial\theta_n}J(\theta)<br>\end{bmatrix}=\overrightarrow{0}<br>\end{align}<br>Now, we transfer the sum into matrices form:<br>$$\frac{\partial}{\partial\Theta}J(\theta)=X^T(X\Theta-y)=X^TX\Theta-X^Ty=\overrightarrow{0}$$<br>thus,<br>we can get the solution of $\Theta$:<br>$$\Theta=(X^TX)^{(-1)}X^Ty$$<br>Let’s look at the solution of Matrices:<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">% octave</span><br><span class="line">xlabel(<span class="string">"x (km)"</span>)<span class="comment">;</span></span><br><span class="line">hold on<span class="comment">;</span></span><br><span class="line">ylabel(<span class="string">"y (k people year)"</span>)<span class="comment">;</span></span><br><span class="line">plot(X,y,<span class="string">"ro"</span>, <span class="string">"MarkerFaceColor"</span>, <span class="string">"b"</span>)<span class="comment">;</span></span><br><span class="line">a=linspace(<span class="number">0</span>,<span class="number">10000</span>,<span class="number">100</span>)<span class="comment">;</span></span><br><span class="line">t = pinv(XX<span class="string">'*XX)*XX'</span>*y<span class="comment">;</span></span><br><span class="line"><span class="keyword">b=t'*[ones(1, </span>length(a))<span class="comment">; a];</span></span><br><span class="line">plot(a,<span class="keyword">b);</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-01-114724.jpg" alt></p>
<p>And $\theta_0$ and $\theta_1$ is as followed:<br>\begin{align}<br>\Theta=\begin{bmatrix}<br>2573\<br>9.9\<br>\end{bmatrix}<br>\end{align}<br>That means $h_\theta(x)=2573+9.9x$, Right Now we subsitute 3169(km) for $x$:<br>$$h_\theta(3169)=2573+9.9*3169=33946(k\  people\ year)$$<br>In other word, Donald Trump need 3,394,600 people continously build 10 years in order to finish the GreatWall between US and Mexico. Surely he can stimulate 33,946,000 people, it only take him one year to finished the wall.</p>
<h3 id="Gradient-Descent-Solution"><a href="#Gradient-Descent-Solution" class="headerlink" title="Gradient Descent Solution"></a>Gradient Descent Solution</h3><p>Let’s look at the cost function one more time. If we first choose a random $\Theta$, say we have $\theta_0\&amp;\theta_1$ located in point 1. Assume you stand at ponit 1, you want to find a way to go down the vally. Surely you can not see the land-form completely. But you can look around at point 1 and find a steepest direction, then you go that way a baby step. Right now, you are at point 2 after the first step. You do the same find a steepest direction and make another baby step. After several steps, you are now standing at point 5. When you are looking around you find that where you are standing is almost flat. Now we can stop the iteration, and we have found a reasonable $\Theta$ which makes the $J(\theta)$ has a minmum value.<br><img src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-03-02-134554.jpg" alt><br>What I described is a method named Gradient Descent which is very easy way to find minima of $J(\theta)$. The step is as followed:</p>
<ol>
<li>Find a reasonable cost function J(\theta).</li>
<li>Take the partial derivative of $J(\theta)$ for each $\theta_j$</li>
<li>Choose a moderate $\alpha$ to constrain the step size.</li>
<li>Take a baby step, the direction is from the derivative and the size is determined by derivative and parameter $alpha$</li>
<li>update the value of $\Theta$, check if we find the minimum. If not, return to the step 4, otherwise, stop the algo</li>
</ol>
<p>Concretely, the algorithm using octave is listed below:<br><figure class="highlight scilab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% octave: The derivative <span class="function"><span class="keyword">function</span></span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">g</span> = <span class="title">gradient</span><span class="params">(X, y, theta)</span></span></span><br><span class="line">    m = <span class="built_in">size</span>(X,<span class="number">1</span>);</span><br><span class="line">    hx = X*theta;</span><br><span class="line">    g = (<span class="number">1</span>/m)*X'*(hx-y);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% octave: I just take 100 steps without checking when <span class="keyword">to</span> stop.</span><br><span class="line">theta=[0;0];</span><br><span class="line"><span class="attribute">alpha</span>=0.00000003</span><br><span class="line"><span class="keyword">for</span> <span class="attribute">i</span>=1:100</span><br><span class="line">    g = gradient(XX,y,theta);</span><br><span class="line">    theta = theta - alpha*g</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>Under the circumstance above, $\theta_0=0.0048$ and $\theta_1=10.33$. If we substitute the parameters for $h_\theta(x)$, the answer that Trump would take to build the wall is 32745.56$\times$1000 people year.</p>
<h3 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h3><p>Today we have talk about how to use linear regression to model a problem of building Great Wall. We have also talk about how to resovle the problem through minimize the cost function both using Matrices solution and Gradient Descent.</p>
<h3 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h3><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">% octave --cost function:cost.m</span><br><span class="line">function J=cost(X, y, theta)</span><br><span class="line">m = size(X,1);</span><br><span class="line">J=0;</span><br><span class="line">hx=X*theta;</span><br><span class="line">J = 1/(2*m)*(hx-y)'*(hx-y);</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Mingbo Cheng</p>
              <p class="site-description motion-element" itemprop="description">Mingbo</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mingbo Cheng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
