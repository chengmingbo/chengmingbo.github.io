<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chengmingbo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.21.0","exturl":false,"sidebar":{"position":"right","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Mingbo">
<meta property="og:type" content="website">
<meta property="og:title" content="Mingbo">
<meta property="og:url" content="http://chengmingbo.github.io/index.html">
<meta property="og:site_name" content="Mingbo">
<meta property="og:description" content="Mingbo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Mingbo Cheng">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://chengmingbo.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Mingbo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mingbo</h1>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="/%5Bobject%20Object%5D" alt="Mingbo">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-slides"><a href="/slides/" rel="section"><i class="area-chart fa-fw"></i>slides</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mingbo Cheng</p>
  <div class="site-description" itemprop="description">Mingbo</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/chengmingbo" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;chengmingbo" rel="noopener me" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="http://www.flickering.cn/" title="http:&#x2F;&#x2F;www.flickering.cn&#x2F;" rel="noopener" target="_blank">flickering</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.zybuluo.com/codeep/note/163962" title="https:&#x2F;&#x2F;www.zybuluo.com&#x2F;codeep&#x2F;note&#x2F;163962" rel="noopener" target="_blank">mathjax grammar</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vividfree.github.io/" title="http:&#x2F;&#x2F;vividfree.github.io&#x2F;" rel="noopener" target="_blank">vividfree</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://colah.github.io/" title="http:&#x2F;&#x2F;colah.github.io&#x2F;" rel="noopener" target="_blank">colah</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.autonlab.org/tutorials" title="https:&#x2F;&#x2F;www.autonlab.org&#x2F;tutorials" rel="noopener" target="_blank">Andrew Moore</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://plot.ly/matlab/plot/" title="https:&#x2F;&#x2F;plot.ly&#x2F;matlab&#x2F;plot&#x2F;" rel="noopener" target="_blank">matlabplot</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.ryanzhang.info/blog/" title="http:&#x2F;&#x2F;www.ryanzhang.info&#x2F;blog&#x2F;" rel="noopener" target="_blank">Ryan’s Cabinet</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.cnblogs.com/jerrylead/tag/Machine%20Learning/" title="http:&#x2F;&#x2F;www.cnblogs.com&#x2F;jerrylead&#x2F;tag&#x2F;Machine%20Learning&#x2F;" rel="noopener" target="_blank">JerryLead</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://yxzf.github.io/" title="https:&#x2F;&#x2F;yxzf.github.io&#x2F;" rel="noopener" target="_blank">YXZF'S BLOG</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://vonng.com/" title="http:&#x2F;&#x2F;vonng.com" rel="noopener" target="_blank">VONNG</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2020/12/07/spectral_2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/07/spectral_2/" class="post-title-link" itemprop="url">Spectral analysis (2)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-12-07 21:10:01" itemprop="dateCreated datePublished" datetime="2020-12-07T21:10:01+01:00">2020-12-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="diffussion-map">Diffussion map</h2>
<p>Eigenvalue decomposition of weighted graph laplacian is used to
obtain diffusion map. The asymmetric matrix or transition matrix <span
class="math inline">\(P=D^{-1}W\)</span>. The decomposition is: <span
class="math display">\[\begin{equation}
P = D^{-1/2}S\Lambda S^\top D^{1/2}
\end{equation}\]</span> In fact, the random walk on the graph would give
rise to the transition graph such that: <span
class="math display">\[\begin{equation}
p^{t+1}= p^t D^{-1}W = p^t P
\end{equation}\]</span> where <span class="math inline">\(p\)</span> is
a initial state of the graph (vertices weight). That is, after a certain
step of random walk on the graph, we would reach a steady state when any
more random walk would not change the weight. Let <span
class="math inline">\(Q=D^{-1/2}S\)</span>, then <span
class="math inline">\(Q^{-1}=S^{\top} D^{1/2}\)</span>, we can derive
<span class="math inline">\(P = Q\Lambda Q^{-1}\)</span>. The random
walk above mentioned can then be represent: <span
class="math display">\[\begin{equation}
\begin{aligned}
P^t &amp;= Q\Lambda Q^{-1}Q\Lambda Q^{-1}\cdots Q\Lambda Q^{-1}
    &amp;= Q\Lambda^t Q
\end{aligned}
\end{equation}\]</span> Since <span
class="math inline">\(\Lambda\)</span> is diagnoal, the random walk on a
graph can be easily cacluted by using the eigenvalue decomposition
outcome. The column of <span class="math inline">\(Q\)</span> is the
diffusion map dimensions.</p>
<h2 id="diffusion-map-example">Diffusion map example</h2>
<p>To understand diffusion map, we here introduce an example data and
run diffusion map on it. The data is a single cell fibroblasts to neuron
data with 392 cells. We first download the data from NCBI and loaded the
data to memory:</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">url <span class="operator">=</span> <span class="string">&#x27;https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE67310&amp;format=file&amp;file=GSE67310%5FiN%5Fdata%5Flog2FPKM%5Fannotated.txt.gz&#x27;</span></span><br><span class="line">r <span class="operator">=</span> requests.get<span class="punctuation">(</span>url<span class="punctuation">,</span> allow_redirects<span class="operator">=</span>True<span class="punctuation">)</span></span><br><span class="line">open<span class="punctuation">(</span><span class="string">&#x27;GSE67310.txt.gz&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;wb&#x27;</span><span class="punctuation">)</span>.write<span class="punctuation">(</span>r.content<span class="punctuation">)</span></span><br><span class="line">data <span class="operator">=</span> pd.read_csv<span class="punctuation">(</span><span class="string">&#x27;GSE67310.txt.gz&#x27;</span><span class="punctuation">,</span> sep<span class="operator">=</span><span class="string">&#x27;\t&#x27;</span><span class="punctuation">,</span> index_col<span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">data <span class="operator">=</span> data.loc<span class="punctuation">[</span>data<span class="punctuation">[</span><span class="string">&#x27;assignment&#x27;</span><span class="punctuation">]</span> <span class="operator">!=</span><span class="string">&#x27;Fibroblast&#x27;</span><span class="punctuation">]</span></span><br><span class="line">group <span class="operator">=</span> data<span class="punctuation">[</span><span class="string">&#x27;assignment&#x27;</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<p>From the code as we can see that, we extract the cell types from the
data and stores it into the variable <code>group</code>. The rest part
of the data is the normalized gene expression count matrix.</p>
<p>To do the pre-processing, we first get the log-normalized count
matrix <span class="math inline">\(X\)</span> and revert it to the
counts and apply log geomatric scaling to the count matrix as the
pre-processing then store it to matrix <span
class="math inline">\(Y\)</span>. <figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X <span class="operator">=</span> np.array<span class="punctuation">(</span>data.iloc<span class="punctuation">[</span><span class="operator">:</span><span class="punctuation">,</span> <span class="number">5</span><span class="operator">:</span><span class="punctuation">]</span><span class="punctuation">)</span>.T</span><br><span class="line">X <span class="operator">=</span> np.power<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">,</span> X<span class="punctuation">[</span>np.apply_along_axis<span class="punctuation">(</span>np.var<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> X<span class="punctuation">)</span><span class="operator">&gt;</span><span class="number">0</span><span class="punctuation">,</span> <span class="operator">:</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span></span><br><span class="line">Y <span class="operator">=</span> np.log<span class="punctuation">(</span>gscale<span class="punctuation">(</span>X<span class="operator">+</span><span class="number">0.5</span><span class="punctuation">)</span><span class="punctuation">)</span>.T</span><br></pre></td></tr></table></figure> The geomatric scaling
implementation is: <figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def gscale<span class="punctuation">(</span>X<span class="operator">:</span>np.ndarray<span class="punctuation">)</span> <span class="operator">-&gt;</span> np.ndarray<span class="operator">:</span></span><br><span class="line">    assert<span class="punctuation">(</span>X.all<span class="punctuation">(</span><span class="punctuation">)</span><span class="operator">&gt;=</span><span class="number">0</span><span class="punctuation">)</span></span><br><span class="line">    div_ <span class="operator">=</span> np.divide<span class="punctuation">(</span>X.T<span class="punctuation">,</span> np.apply_along_axis<span class="punctuation">(</span>lambda x<span class="operator">:</span>np.exp<span class="punctuation">(</span>np.mean<span class="punctuation">(</span>np.log<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    X<span class="punctuation">)</span><span class="punctuation">)</span>.T</span><br><span class="line">    scale_ <span class="operator">=</span> np.apply_along_axis<span class="punctuation">(</span>np.median<span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span> div_<span class="punctuation">)</span></span><br><span class="line">    sc <span class="operator">=</span> StandardScaler<span class="punctuation">(</span>with_mean<span class="operator">=</span>False<span class="punctuation">)</span></span><br><span class="line">    sc.fit<span class="punctuation">(</span>X<span class="punctuation">)</span></span><br><span class="line">    sc.scale_ <span class="operator">=</span> scale_</span><br><span class="line">    <span class="built_in">return</span> sc.transform<span class="punctuation">(</span>X<span class="punctuation">)</span></span><br></pre></td></tr></table></figure> After the pre-processing, we we next
run PCA to perform dimensionality reduction and use which to calculate
euclidean distances between cells, and then run diffusion map.</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pc <span class="operator">=</span> run_pca<span class="punctuation">(</span>Y<span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">)</span></span><br><span class="line">R <span class="operator">=</span> distance_matrix<span class="punctuation">(</span>pc<span class="punctuation">,</span> pc<span class="punctuation">)</span></span><br><span class="line">d <span class="operator">=</span> diffusionMaps<span class="punctuation">(</span>R<span class="punctuation">,</span><span class="number">7</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<p>We first take a look at the PCA figure which shows the first 2 PCs of
the dataset. PCA is a linear methods which cannot reflect cell fate
differentation process. <img
src="https://cmb.oss-cn-qingdao.aliyuncs.com/fib2neuron_pca.png" />
Since the diffusion map applied the Gaussian kernal during the distances
calculation, it usually a better way to capture the cell differentation
events. Here we show diffusion dimension 1,2 and 1,3. It's clear that
1,2 captures the cell differentation from fibroblasts to neuron, and 1,3
captures the differentiation from firoblasts to myocytes. <img
src="https://cmb.oss-cn-qingdao.aliyuncs.com/fib2neuron_diffusions.png" /></p>
<p>We can change the bandwidth from 7 to 100, which use the 100th
neareast neighbor as bandwidth instead of 7th. The following shows the
diffusion map that bandwidth=100. <img
src="https://cmb.oss-cn-qingdao.aliyuncs.com/fib2neuron_bandwidth100.png" /></p>
<h2 id="pseudo-time-by-random-walk">pseudo time by random walk</h2>
<p>The eigenvalue decomposition of graph laplacian can also be used to
infer the pseudo time simulating the pseudo time of cell differentation
events. We here set top <span class="math inline">\(m\)</span> cells
<span class="math inline">\(1/m\)</span> of the progenitor cells (MEF)
and set other cell 0 as the initial state. Next, we perform random walk
to get new pesudotime <span class="math inline">\(u\)</span> such that:
<span class="math display">\[\begin{equation}
u = [\frac{1}{m}, \frac{1}{m}, \cdots, 0, \cdots 0](D^{-1}W)^t
\end{equation}\]</span> By testing different number of random walk steps
we can check the new pseudo time <span class="math inline">\(u\)</span>.
Here we show time step equals to 1, 10, 100 and 200. From the figure we
can notice that after certain step, the pseudo time will not change
anymore. That means the random walk reaches the steady state. <img
src="https://cmb.oss-cn-qingdao.aliyuncs.com/fib2neuron_pseudotime.png" /></p>
<p>To test when can we reach the steady state, I use the graph we
mentioned in my last post <a
href="https://chengmingbo.github.io/2020/06/07/spectral/">Spectral
analysis (1)</a>:
<img src="https://cmb.oss-cn-qingdao.aliyuncs.com/a_graph.png" height="250px"></p>
<p>Here we random generate two initial states (<span
class="math inline">\(v_1\)</span>, <span
class="math inline">\(v_2\)</span>) to do the random walk such that:
<span class="math display">\[\begin{equation}
\begin{aligned}
v_1 &amp;= [0.15,0.41,0.54,0.9,0.62,0.93,0.1,0.46,0.01,0.88]\\
v_2 &amp;= [0.89,0.93,0.07,0.41,0.52,0.88,0.43,0.09,0.1,0.2]
\end{aligned}
\end{equation}\]</span></p>
<p>From the the figure below we can see that <span
class="math inline">\(v_1\)</span> reaches the steady state in 10 steps
whereas <span class="math inline">\(v_2\)</span> reaches steady state in
20 steps. In total, all these two initial state will reach the steady
state that would not change any longer. <img
src="https://cmb.oss-cn-qingdao.aliyuncs.com/fib2neuron_steady.png" /></p>
<h2 id="spectral-clustering">spectral clustering</h2>
<p>The eigenvectors of graph Laplacian can also be used to do the
clustering. From the figure we can find clear cut using the 1st, 2nd and
the 3rd diffusion dimensions. In practice, we can use kmeans, DBSCAN,
leiden, louvain algorithm to perform clustering using the diffusion
dimensions. <img
src="https://cmb.oss-cn-qingdao.aliyuncs.com/fib2neuron_clustering.png" /></p>
<h2 id="appendix">Appendix</h2>
<h5 id="pca-function">PCA function</h5>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def run_pca<span class="punctuation">(</span>mtx<span class="punctuation">,</span> n_components<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span> random_state<span class="operator">=</span><span class="number">2022</span><span class="punctuation">)</span><span class="operator">:</span></span><br><span class="line">    dm <span class="operator">=</span> None</span><br><span class="line">        <span class="keyword">if</span> scipy.sparse.issparse<span class="punctuation">(</span>mtx<span class="punctuation">)</span><span class="operator">:</span></span><br><span class="line">        clf <span class="operator">=</span> TruncatedSVD<span class="punctuation">(</span>n_components<span class="punctuation">,</span> random_state<span class="operator">=</span>random_state<span class="punctuation">)</span></span><br><span class="line">        dm <span class="operator">=</span> clf.fit_transform<span class="punctuation">(</span>mtx<span class="punctuation">)</span></span><br><span class="line">    <span class="keyword">else</span><span class="operator">:</span></span><br><span class="line">        pca <span class="operator">=</span> PCA<span class="punctuation">(</span>n_components<span class="operator">=</span>n_components<span class="punctuation">,</span> random_state<span class="operator">=</span>random_state<span class="punctuation">)</span></span><br><span class="line">        dm <span class="operator">=</span> pca.fit_transform<span class="punctuation">(</span>mtx<span class="punctuation">)</span></span><br><span class="line">        <span class="built_in">return</span> dm</span><br></pre></td></tr></table></figure>
<h5 id="affinity-matrix-function">Affinity matrix function</h5>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def affinity<span class="punctuation">(</span>R<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">7</span><span class="punctuation">,</span> sigma<span class="operator">=</span>None<span class="punctuation">,</span> <span class="built_in">log</span><span class="operator">=</span>False<span class="punctuation">)</span><span class="operator">:</span></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    Gaussian affinity matrix constructor</span></span><br><span class="line"><span class="string">    W = exp(-r_&#123;ij&#125;^2/sigma)</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    def top_k<span class="punctuation">(</span>lst<span class="punctuation">,</span> k<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span><span class="operator">:</span></span><br><span class="line">        assert<span class="punctuation">(</span>len<span class="punctuation">(</span>lst<span class="punctuation">)</span> <span class="operator">&gt;</span>k<span class="punctuation">)</span></span><br><span class="line">        <span class="built_in">return</span> np.partition<span class="punctuation">(</span>lst<span class="punctuation">,</span> k<span class="punctuation">)</span><span class="punctuation">[</span>k<span class="punctuation">]</span></span><br><span class="line"></span><br><span class="line">    R <span class="operator">=</span> np.array<span class="punctuation">(</span>R<span class="punctuation">)</span></span><br><span class="line">    <span class="keyword">if</span> not sigma<span class="operator">:</span></span><br><span class="line">        s <span class="operator">=</span> <span class="punctuation">[</span>top_k<span class="punctuation">(</span>R<span class="punctuation">[</span><span class="operator">:</span><span class="punctuation">,</span> i<span class="punctuation">]</span><span class="punctuation">,</span> k<span class="operator">=</span>k<span class="punctuation">)</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span><span class="punctuation">(</span>R.shape<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">]</span></span><br><span class="line">        S <span class="operator">=</span> np.sqrt<span class="punctuation">(</span>np.outer<span class="punctuation">(</span>s<span class="punctuation">,</span> s<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">    <span class="keyword">else</span><span class="operator">:</span></span><br><span class="line">        S <span class="operator">=</span> sigma</span><br><span class="line">        logW <span class="operator">=</span> <span class="operator">-</span>np.power<span class="punctuation">(</span>np.divide<span class="punctuation">(</span>R<span class="punctuation">,</span> S<span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">log</span><span class="operator">:</span></span><br><span class="line">        <span class="built_in">return</span> logW</span><br><span class="line">    <span class="built_in">return</span> np.exp<span class="punctuation">(</span>logW<span class="punctuation">)</span></span><br></pre></td></tr></table></figure>
<h5 id="diffusion-map-function">Diffusion map function</h5>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def diffusionMaps<span class="punctuation">(</span>R<span class="punctuation">,</span>k<span class="operator">=</span><span class="number">7</span><span class="punctuation">,</span>sigma<span class="operator">=</span>None<span class="punctuation">)</span><span class="operator">:</span></span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">    Diffusion map(Coifman, 2005)</span></span><br><span class="line"><span class="string">    https://en.wikipedia.org/wiki/Diffusion_map</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    dic:</span></span><br><span class="line"><span class="string">        psi: right eigvector of P = D^&#123;-1/2&#125; * evec</span></span><br><span class="line"><span class="string">        phi: left eigvector of P = D^&#123;1/2&#125; * evec</span></span><br><span class="line"><span class="string">        eig: eigenvalues</span></span><br><span class="line"><span class="string">    &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    k<span class="operator">=</span>k<span class="operator">-</span><span class="number">1</span> <span class="comment">## k is R version minus 1 for the index</span></span><br><span class="line">    logW <span class="operator">=</span> affinity<span class="punctuation">(</span>R<span class="punctuation">,</span>k<span class="punctuation">,</span>sigma<span class="punctuation">,</span><span class="built_in">log</span><span class="operator">=</span>True<span class="punctuation">)</span></span><br><span class="line">    rs <span class="operator">=</span> np.exp<span class="punctuation">(</span><span class="punctuation">[</span>logsumexp<span class="punctuation">(</span>logW<span class="punctuation">[</span>i<span class="punctuation">,</span><span class="operator">:</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span><span class="punctuation">(</span>logW.shape<span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment">## dii=\sum_j</span></span><br><span class="line">    w_<span class="punctuation">&#123;</span>i<span class="punctuation">,</span>j<span class="punctuation">&#125;</span></span><br><span class="line">    D <span class="operator">=</span> np.diag<span class="punctuation">(</span>np.sqrt<span class="punctuation">(</span>rs<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">    <span class="comment">## D^&#123;1/2&#125;</span></span><br><span class="line">    Dinv <span class="operator">=</span> np.diag<span class="punctuation">(</span><span class="number">1</span><span class="operator">/</span>np.sqrt<span class="punctuation">(</span>rs<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment">##D^&#123;-1/2&#125;</span></span><br><span class="line">    Ms <span class="operator">=</span> Dinv <span class="operator">@</span> np.exp<span class="punctuation">(</span>logW<span class="punctuation">)</span> <span class="operator">@</span> Dinv <span class="comment">## D^&#123;-1/2&#125; W D^&#123;-1/2&#125;</span></span><br><span class="line">    e <span class="operator">=</span> np.linalg.eigh<span class="punctuation">(</span>Ms<span class="punctuation">)</span> <span class="comment">## eigen decomposition of P&#x27;</span></span><br><span class="line">    evalue<span class="operator">=</span> e<span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">[</span><span class="operator">::</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">]</span></span><br><span class="line">    evec <span class="operator">=</span> np.flip<span class="punctuation">(</span>e<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> axis<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">    s <span class="operator">=</span> np.sum<span class="punctuation">(</span>np.sqrt<span class="punctuation">(</span>rs<span class="punctuation">)</span> <span class="operator">*</span> evec<span class="punctuation">[</span><span class="operator">:</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># scaling</span></span><br><span class="line">    <span class="comment"># Phi is orthonormal under the weighted inner product</span></span><br><span class="line">    <span class="comment">#0:Psi, 1:Phi, 2:eig</span></span><br><span class="line">    dic <span class="operator">=</span> <span class="punctuation">&#123;</span><span class="string">&#x27;psi&#x27;</span><span class="operator">:</span>s <span class="operator">*</span> Dinv<span class="operator">@</span>evec<span class="punctuation">,</span> <span class="string">&#x27;phi&#x27;</span><span class="operator">:</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">/</span>s<span class="punctuation">)</span><span class="operator">*</span>D<span class="operator">@</span>evec<span class="punctuation">,</span> <span class="string">&quot;eig&quot;</span><span class="operator">:</span> evalue<span class="punctuation">&#125;</span></span><br><span class="line">    <span class="built_in">return</span> dic</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2020/06/07/spectral/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/06/07/spectral/" class="post-title-link" itemprop="url">Spectral analysis (1)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-06-07 18:30:22" itemprop="dateCreated datePublished" datetime="2020-06-07T18:30:22+02:00">2020-06-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="introduction">Introduction</h2>
<p>Newton used glass separted the white sunlight into red, orange,
yellow, green, blue, indigo and violet single colors. The light spectrum
analysis can help scientists to interpret the world. For example, we can
detect the elements of our solar system as well as far stars in the
universe. The spectrum analysis is also a field in mathmatjics. In the
graph thoery field, Laplacian matrix is used to represented a graph. We
can obtian features from the undirected graph below (<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Laplacian_matrix">wikipedia</a>).
<img src="https://cmb.oss-cn-qingdao.aliyuncs.com/a_graph.png" height="300px"></p>
<p>For example, we can check the degree of each vertex, which forms our
Degree matrix <span class="math inline">\(D\)</span> such that: <span
class="math display">\[\begin{equation}
D = \begin{pmatrix}
  2 &amp; 0 &amp;  0 &amp;  0 &amp; 0 &amp; 0\\
  0 &amp;  3 &amp; 0 &amp;  0 &amp; 0 &amp; 0\\
   0 &amp; 0 &amp;  2 &amp; 0 &amp;  0 &amp; 0\\
   0 &amp;  0 &amp; 0 &amp;  3 &amp; 0 &amp;0\\
  0 &amp; 0 &amp;  0 &amp; 0 &amp;  3 &amp; 0\\
   0 &amp;  0 &amp;  0 &amp; 0 &amp;  0 &amp; 1
\end{pmatrix}
\end{equation}\]</span></p>
<p>By checking the connection between all pairs nodes, we can create a
Adjacency matrix:</p>
<p><span class="math display">\[\begin{equation}
A = \begin{pmatrix}
  0 &amp; 1 &amp;  0 &amp;  0 &amp; 1 &amp; 0\\
  1 &amp;  0 &amp; 1 &amp;  0 &amp; 1 &amp; 0\\
   0 &amp; 1 &amp;  0 &amp; 1 &amp;  0 &amp; 0\\
   0 &amp;  0 &amp; 1 &amp;  0 &amp; 1 &amp;1\\
  1 &amp; 1 &amp;  0 &amp; 1 &amp;  0 &amp; 0\\
   0 &amp;  0 &amp;  0 &amp; 1 &amp;  0 &amp; 0
\end{pmatrix}
\end{equation}\]</span></p>
<h2 id="graph-laplacian">graph laplacian</h2>
<p>The graph Laplacian <span class="math inline">\(L\)</span> extracts
all useful information from a graph which is: <span
class="math display">\[\begin{equation}
L=D-A =\begin{pmatrix}
  2 &amp; -1 &amp;  0 &amp;  0 &amp; -1 &amp; 0\\
  -1 &amp;  3 &amp; -1 &amp;  0 &amp; -1 &amp; 0\\
   0 &amp; -1 &amp;  2 &amp; -1 &amp;  0 &amp; 0\\
   0 &amp;  0 &amp; -1 &amp;  3 &amp; -1 &amp; -1\\
  -1 &amp; -1 &amp;  0 &amp; -1 &amp;  3 &amp; 0\\
   0 &amp;  0 &amp;  0 &amp; -1 &amp;  0 &amp; 1
\end{pmatrix}
\end{equation}\]</span></p>
<p>In fact, the graph Laplaican matrix is symmetric and also positive
semidefinite (PSD), which means if we perform eigenvalue decomposition,
the eigen values are all real and nonnegative. We can normlize the graph
Laplaican by left multiplying the <span
class="math inline">\(D^{-1}\)</span> which will give rise to all <span
class="math inline">\(1\)</span>s of the diagnal entries of the matrix,
namely: <span class="math display">\[\begin{equation}
\text{norm}({L}) = D^{-1}L = \begin{pmatrix}1 &amp; -0.50 &amp;  0
&amp;  0 &amp; -0.50 &amp; 0\\
  -0.33 &amp;  1 &amp; -0.33 &amp;  0 &amp; -0.33 &amp; 0\\
   0 &amp; -0.50 &amp;  1 &amp; -0.50 &amp;  0 &amp; 0\\
   0 &amp;  0 &amp; -0.33 &amp;  1 &amp; -0.33 &amp;-0.33\\
  -0.33 &amp; -0.33 &amp;  0 &amp; -0.33 &amp;  1 &amp; 0\\
   0 &amp;  0 &amp;  0 &amp; -1 &amp;  0 &amp; 1
\end{pmatrix}
\end{equation}\]</span> This matrix is called transition matrix.
However, the matrix would not keep the symmetric and the PSD property
after the normalization. We will come back to discuss the spectral for
the normalized graph Laplacian.</p>
<h2 id="weighted-graph">Weighted graph</h2>
<p>In practice, graph are usually weighted. The weight between vertices
can be euclidean distance or other measures. The figure blow shows the
weights of the graph. Here we apply gussian kernel to the euclidean
distances between vertices such that: <img
src="https://cmb.oss-cn-qingdao.aliyuncs.com/a_graph_weighted.png"
alt="weighted graph" /></p>
<p><span class="math display">\[\begin{equation}
w_{ij} =\exp (-r_{ij}^2/\sigma) = \exp \big(\frac{-\|x_i -
x_j\|^2}{\sigma_i\sigma_j}\big)
\end{equation}\]</span> where <span
class="math inline">\(r_{ij}\)</span> is the euclidean distance between
vetex <span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span>, and <span
class="math inline">\(sigma\)</span> controls the bandwidth. We call the
matrix <span class="math inline">\(W=(w_{ij})\)</span> gaussian affinity
matrix such that: <span class="math display">\[\begin{equation}
\begin{pmatrix}
0 &amp; w_{12} &amp; w_{13} &amp; w_{14} &amp; w_{15}&amp; w_{16}\\
w_{21} &amp; 0 &amp; w_{23} &amp; w_{24} &amp; w_{25}&amp; w_{26}\\
w_{31} &amp; w_{32} &amp; 0 &amp; w_{34} &amp; w_{35}&amp; w_{36}\\
w_{41} &amp; w_{42} &amp; w_{43} &amp; w_{44} &amp; w_{45}&amp; w_{46}\\
w_{51} &amp; w_{52} &amp; w_{53} &amp; w_{54} &amp; 0&amp; w_{56}\\
w_{61} &amp; w_{62} &amp; w_{63} &amp; w_{64} &amp; w_{65}&amp; 0
\end{pmatrix}
\end{equation}\]</span> The guassian kernel would enlarge the distance
between too far vertices. Similar to unweighted matrix, we can also
construct graph Laplaican matrix using the gaussian affinity matrix.
First, we need find the weighted degree based on <span
class="math inline">\(W\)</span> such that: <span
class="math display">\[\begin{equation}
d_{ii} = \sum_j{w_{ij}}
\end{equation}\]</span> With the diagnal degree matrix and affinity
matrix, we now can have the weighted laplacian that: <span
class="math display">\[\begin{equation}
L = D - W
\end{equation}\]</span> Likewise, we next give the normalized form of
Laplacian such that: <span class="math display">\[\begin{equation}
\text{norm}{(L)}= D^{-1}L = I - D^{-1}W
\end{equation}\]</span> To facilitate the eigenvalue decomposition, we
need apply trick to the asymmetric matrix <span
class="math inline">\(D^{-1}L\)</span>. Since the eigenvectors of <span
class="math inline">\(D^{-1}L\)</span> and <span
class="math inline">\(D^{-1}W\)</span> are the same, we apply some trick
to <span class="math inline">\(P = D^{-1}W\)</span> to simplify the
problem. Lets construct <span class="math inline">\(P&#39;\)</span> such
that: <span class="math display">\[\begin{equation}
P&#39; = D^{1/2} P D^{-1/2} = D^{-1/2}WD^{-1/2}
\end{equation}\]</span> It obvious <span
class="math inline">\(P&#39;\)</span> is symmetric due to the
symmetrisation of <span class="math inline">\(W\)</span>. We can perform
eigenvalue decomposition on <span class="math inline">\(P&#39;\)</span>
such that: <span class="math display">\[\begin{equation}
P&#39; = S\Lambda S^\top
\end{equation}\]</span> Where S stores the eigenvectors of <span
class="math inline">\(P&#39;\)</span> and the diagonals of <span
class="math inline">\(\Lambda\)</span> records the eigenvalues of <span
class="math inline">\(P&#39;\)</span>. We can also get the decompostion
to <span class="math inline">\(P\)</span> such that: <span
class="math display">\[\begin{equation}
P = D^{-1/2}S\Lambda S^\top D^{1/2}
\end{equation}\]</span> Let <span
class="math inline">\(Q=D^{-1/2}\)</span>, then <span
class="math inline">\(Q^{-1}=S^{\top}D^{1/2}\)</span>. We therefore find
the right and left eigenvector of <span class="math inline">\(P\)</span>
such that: <span class="math display">\[\begin{equation}
\psi = D^{-1/2}
\phi = S^{\top}D^{1/2}
\end{equation}\]</span> In fact, columns of <span
class="math inline">\(\psi\)</span> stores the spectral of the graph
which also call diffusion map dimensions.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2019/05/10/CCA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/05/10/CCA/" class="post-title-link" itemprop="url">A tutorial on Canonical Correlation Analysis(CCA)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-05-10 19:54:43" itemprop="dateCreated datePublished" datetime="2019-05-10T19:54:43+02:00">2019-05-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="introduction">Introduction</h2>
<p>Suppose we have two sets of variable corresponding to two aspects
such as height and weight, we want to analysis the relationship between
this two sets. There are several ways to measure the relationship
between them. However, sometime the it is hard to handle datasets with
different dimensions, meaning, if <span class="math inline">\(X\in
\mathbb{R}^m\)</span> and <span class="math inline">\(Y\in
\mathbb{R}^n\)</span>, how to resolve the relationship?</p>
<h2 id="yxsic-of-cca">yxsic of CCA</h2>
<p>Assume there are two sets of data <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>, the size of <span
class="math inline">\(X\)</span> is <span class="math inline">\(n \times
p\)</span>, whereas size of <span class="math inline">\(Y\)</span> is
<span class="math inline">\(n\times q\)</span>. That is, <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> share the same row numbers but are
differnt in columns number. The idea of CCA is simple: find the best
match of <span class="math inline">\(X w_x\)</span> and <span
class="math inline">\(Y w_y\)</span>. Let's just set: <span
class="math display">\[X w_x = z_x\qquad\text{and}\qquad Y w_y =
z_y\]</span></p>
<p>Where <span class="math inline">\(X\in \mathbb{R}^{n\times
p}\)</span>, <span class="math inline">\(w_x \in
\mathbb{R}^{p}\)</span>, <span class="math inline">\(z_x\in
\mathbb{R}^n\)</span>, <span class="math inline">\(Y\in
\mathbb{R}^{n\times q}\)</span>, <span class="math inline">\(w_y \in
\mathbb{R}^{q}\)</span>, <span class="math inline">\(z_y\in
\mathbb{R}^n\)</span>. <span class="math inline">\(w_x\)</span> and
<span class="math inline">\(w_y\)</span> are often refered as canonical
weight vectors, <span class="math inline">\(z_x\)</span> and <span
class="math inline">\(z_y\)</span> are named images as well as canonical
variates or canonical scores. To simplify the problem, we assume <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> are standardized to zero mean and unit
variance. Our task is to maximize the angle of <span
class="math inline">\(z_x\)</span> and <span
class="math inline">\(z_y\)</span>, meaning:</p>
<p><span class="math display">\[\max_{z_x, z_y \in \mathbf{R^n}}
&lt;z_x, z_y&gt;=\max \cos(z_x, z_y)=\max\frac{&lt;z_x,
z_y&gt;}{\|z_x\|\|z_y\|}\]</span></p>
<p>with respect to: <span class="math inline">\(\|z_x\|_{2}=1\quad
\|z_y\|_{2}=1\)</span>.</p>
<p>In fact, our task is just project <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> to a new coordinate system after the
linear transformation to <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span>.</p>
<h2 id="resolve-cca">Resolve CCA</h2>
<p>There are many solutions to this problems. Before start, We need make
some assumptions: 1. the each column vector of <span
class="math inline">\(X\)</span> is perpendicular to the others. Which
means <span class="math inline">\(X^T X= I\)</span>. The assumption is
the same with <span class="math inline">\(Y\)</span> and <span
class="math inline">\(w_x, w_y\)</span>. We can find <span
class="math inline">\(\min(p,q)\)</span> canonical components, and the
<span class="math inline">\(r\)</span>th component is orthogonal to all
the <span class="math inline">\(r-1\)</span> components.</p>
<h4 id="resolve-cca-through-svd">Resolve CCA through SVD</h4>
<p>To solve the CCA problem using SVD, we first introduce the joint
covariance matrix <span class="math inline">\(C\)</span> such such that:
<span class="math display">\[\begin{equation}
    C = \begin{pmatrix}
        C_{xx} &amp; C_{xy}\\
        C_{yx} &amp; C_{yy}\\
    \end{pmatrix}
\end{equation}\]</span> Where <span
class="math inline">\(C_{xx}=\frac{1}{n-1}X^\top X\)</span> and <span
class="math inline">\(C_{yy}=\frac{1}{n-1}Y^\top Y\)</span> are the
empirical variance matrices between <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> respectively. The <span
class="math inline">\(C_{xy}=\frac{1}{n-1} X^\top Y\)</span> is the
covariance matrix between <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span>.</p>
<p>We next can reform CCA problem with two linear transformations <span
class="math inline">\(w_x\)</span> and <span
class="math inline">\(w_y\)</span> such that:</p>
<p><span class="math display">\[\begin{equation}
w_x^\top C_{xx} w_x = I_p, \quad w_y^\top C_{yy} w_y = I_q, \quad
w_x^\top C_{xy} w_y = D
\end{equation}\]</span> Where I_p and I_q are th p-dimensional and
q-dimensional identity meatrics respectively. The diagonal matrix <span
class="math inline">\(D = \text{diag}(\gamma_i)\)</span> so that:</p>
<p><span class="math display">\[\begin{equation}
    \begin{pmatrix}
        {w}_x^\top &amp; { 0}\\
        { 0} &amp;  {w}_y^\top
        \end{pmatrix}
        \begin{pmatrix}
        C_{xx} &amp; C_{xy}\\
        C_{yx} &amp; C_{yy}
        \end{pmatrix}
        \begin{pmatrix}
         {w}_x &amp; { 0}\\
        { 0} &amp;  {w}_y
        \end{pmatrix}
        =
        \begin{pmatrix}
        I_p &amp; D\\
        D^\top &amp; I_q
    \end{pmatrix},
\end{equation}\]</span></p>
<p>The canoical variable: <span class="math display">\[\begin{equation}
Z_x = Xw_x, \quad Z_y = Y w_y
\end{equation}\]</span> The diagonal elements <span
class="math inline">\(\gamma_i\)</span> of D denote the canonical
correlations. Thus we find the linear compounds <span
class="math inline">\({Z}_x\)</span> and <span
class="math inline">\({Z}_y\)</span> to maximize the cross-correlations.
Since both <span class="math inline">\(C_{xx}\)</span> and <span
class="math inline">\(C_{yy}\)</span> are symmetric positive definite,
we can perform Cholesky Decomposition on them to get: <span
class="math display">\[\begin{equation}
    C_{xx} = C_{xx}^{\top/2} C_{xx}^{1/2}, \quad C_{yy} =
C_{yy}^{\top/2} C_{yy}^{1/2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(C_{xx}^{\top/2}\)</span> is the
transpose of <span class="math inline">\(C_{xx}^{1/2}\)</span>. Applying
the inverses of the square root factors symmetrically on the joint
covariance matrix <span class="math inline">\(C\)</span>, the matrix is
transformed into: <span class="math display">\[\begin{equation}
\begin{pmatrix}
    C_{xx}^{-\top/2} &amp; {\mathbf 0}\\
    {\mathbf 0} &amp; C_{yy}^{-\top/2}
    \end{pmatrix}
    \begin{pmatrix}
    C_{xx} &amp; C_{ab}\\
    C_{yx} &amp; C_{yy}
    \end{pmatrix}
    \begin{pmatrix}
    C_{xx}^{-1/2} &amp; {\mathbf 0}\\
    {\mathbf 0} &amp; C_{yy}^{-1/2}
    \end{pmatrix}
    =
    \begin{pmatrix}
    I_p &amp; C_{xx}^{-1/2}C_{ab}C_{yy}^{-1/2}\\
    C_{yy}^{-1/2}C_{yx}C_{xx}^{-1/2} &amp; I_q
\end{pmatrix}.
\end{equation}\]</span></p>
<p>The canonical correlation problem is reduced to that of finding an
SVD of a triple product: <span class="math display">\[\begin{equation}
    U^{\top} (C_{xx}^{-1/2}C_{ab}C_{yy}^{-1/2}) V = D.
\end{equation}\]</span> The matrix <span
class="math inline">\(C\)</span> is thus reduced to the joint covariance
matrix by applying a two-sided Jacobi method such that: <span
class="math display">\[\begin{equation}
    \begin{pmatrix}
        U^\top &amp; {\mathbf 0}\\
        {\mathbf 0} &amp; V^\top
    \end{pmatrix}
    \begin{pmatrix}
        I_p &amp; C_{xx}^{-1/2}C_{ab}C_{yy}^{-1/2}\\
        C_{yy}^{-1/2}C_{_y}C_{xx}^{-1/2} &amp; I_q
    \end{pmatrix}
    \begin{pmatrix}
        U &amp; {\mathbf 0}\\
        {\mathbf 0} &amp; V
    \end{pmatrix} =
    \begin{pmatrix}
    I_p &amp; D\\
    D^\top &amp; I_q
    \end{pmatrix}
\end{equation}\]</span></p>
<p>with the desired transformation <span
class="math inline">\({w}_x\)</span> and <span
class="math inline">\({w}_y\)</span>: <span
class="math display">\[\begin{equation}
    {w}_x = C_{xx}^{-1/2} U, \quad {w}_y = C_{yy}^{-1/2}V
\end{equation}\]</span> where the singular values <span
class="math inline">\(\gamma_i\)</span> are in descending order such
that: <span class="math display">\[\begin{equation}
    \gamma_1 \geq \gamma_2 \geq \cdots \geq 0.
\end{equation}\]</span></p>
<h4 id="resolve-cca-through-standard-eigenvalue-problem">Resolve CCA
through Standard EigenValue Problem</h4>
<p>The Problem can be reformed to solve the problem: <span
class="math display">\[\begin{equation}
\underset{w_x \in \mathbb{R}^p, w_y\in \mathbb{R}^q}{\arg \max} w_x^\top
C_{xy} w_y
\end{equation}\]</span> With respect to <span
class="math inline">\(\|\|w_x^\top C_{xx} w_x\|\|_2 = \sqrt{w_x^\top
C_{xx} w_x}=1\)</span> and <span class="math inline">\(\|\|w_y^\top
C_{yy} w_y\|\|_2 = \sqrt{w_y^\top C_{yy} w_y}=1\)</span>. The problem
can apparently sovled by Lagrange multiplier technique. Let construct
the Lagrange multiplier <span class="math inline">\(L\)</span> such
that: <span class="math display">\[\begin{equation}
    L = w_x^\top C_{xy} w_y - \frac{\rho_1}{2} w_x^\top C_{xx} w_x -
\frac{\rho_2}{2} w_y^\top C_{yy} w_y
    \end{equation}\]</span></p>
<p>The differentiation of L to <span class="math inline">\(w_x\)</span>
and <span class="math inline">\(w_y\)</span> is: <span
class="math display">\[\begin{equation}
\begin{aligned}
\frac{\partial L}{\partial w_x} = C_{xy} w_y - \rho_1 C_{xx}w_x =
\mathbf{0}\\
\frac{\partial L}{\partial w_y} = C_{yx} w_x - \rho_2 C_{yy}w_y =
\mathbf{0}
\end{aligned}
\end{equation}\]</span></p>
<p>By left multipling <span class="math inline">\(w_x\)</span> and <span
class="math inline">\(w_y\)</span> the above equation, we have:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
w_x^\top C_xy w_y -\rho_1 w_x^\top C_xx w_x = \mathbf{0}\\
w_y^\top C_yx w_x -\rho_2 w_y^\top C_yy w_y = \mathbf{0}
\end{aligned}
\end{equation}\]</span> Since w_x^C_xx w_x = 1 and w_y^C_yy w_y = 1, we
can obtain that <span class="math inline">\(\rho_1 = \rho_2 =
\rho\)</span>. By substituting <span class="math inline">\(\rho\)</span>
to the formula. We can get: <span
class="math display">\[\begin{equation}
w_x = \frac{C_{xx}^{-1}C_{xy}w_y}{rho}
\end{equation}\]</span> Evantually we have the equation: <span
class="math display">\[\begin{equation}
C_{yx} C_{xx}^{-1} C_{xy} w_y = \rho^2 C_yy w_y
\end{equation}\]</span> Obviously, this is the form of eigenvalue
decompostion problem where all eigen values are greater or equal to
zero. By solving the eigenvalue decomposition we can find <span
class="math inline">\(w_x\)</span> and <span
class="math inline">\(w_y\)</span>.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2019/03/10/add-comments/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/03/10/add-comments/" class="post-title-link" itemprop="url">简单增加博客评论</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-03-10 20:55:48" itemprop="dateCreated datePublished" datetime="2019-03-10T20:55:48+01:00">2019-03-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="博客的评论系统">博客的评论系统</h2>
<p>希望把博客的评论系统建立起来，之前使用的是disqus，重新部署的时候，页面大部分都无法显示。不想再用disqus。看到有人创造性的利用github作为载体建立评论系统，也就是Gitment了。
按照教程在github上设置了Gitment，惊闻Gitment需要请求服务，是作者搭的，作者已经不维护了。按照以下操作：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i --save gitment</span><br></pre></td></tr></table></figure></p>
<p>修改自己js，连接自己搭建的服务器，WTF？ <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/node_modules/gitment/dist/gitment.browser.js</span><br></pre></td></tr></table></figure>
详细修改过程可参照：
https://sherry0429.github.io/2019/02/12/gitment%E4%BF%AE%E5%A4%8D/</p>
<p>后继续寻觅其他可以评论系统，找到这篇文章：
https://wangjiezhe.com/posts/2018-10-29-Hexo-NexT-3/
根据此文章的教程安装了utterances。目前发现还是比较不错。知识现在看到的效果是全局评论。
issue-term不太了解具体，目前不想深入探究，仅仅设置pathname。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2019/03/10/Change-theme-to-Next/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/03/10/Change-theme-to-Next/" class="post-title-link" itemprop="url">Change theme to Next</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-03-10 14:07:25" itemprop="dateCreated datePublished" datetime="2019-03-10T14:07:25+01:00">2019-03-10</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="不再折腾主题乖乖的切到next">不再折腾主题，乖乖的切到Next</h2>
<p>又一次，今年年初的又一次，博客系统hexo下的maupassant主题又罢工了。由于年初的各种事情繁琐而多，我就放弃治疗博客系统了，也就是说，有新的博文也无法发出来，先不管那些报错了。</p>
<p>现在稍微腾出一点儿时间，准备把博客系统好好弄一下。其实最简单的办法，也是屡试不爽的方法就是把所有的环境重新安装一遍，显示hexo，再是maupassant。这次不灵了，hexo
generate之后一堆报错。我甚至觉得maupassant已经无法搞定了，搜索错误的关键词，发现没有人遇到与我相同的问题。最后是怀疑我文章里有公式的特殊字符，影响markdown
parse。修改了hexo-renderer-marked的js，仍然有问题。最后决定换其他主题了，然而只要把所有的文章迁移过来一定会报错。报错如下：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INFO  Start processing</span><br><span class="line">FATAL Something&#x27;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html</span><br><span class="line">Template render error: (unknown path) [Line 65, Column 565]</span><br><span class="line">  expected variable end</span><br><span class="line">    at Object._prettifyError (/Users/chengmingbo/blog_deploy/blog/node_modules/nunjucks/src/lib.js:36:11)</span><br><span class="line">    at Template.render (/Users/chengmingbo/blog_deploy/blog/node_modules/nunjucks/src/environment.js:542:21)</span><br><span class="line">    at Environment.renderString (/Users/chengmingbo/blog_deploy/blog/node_modules/nunjucks/src/environment.js:380:17</span><br><span class="line">    ... ...</span><br></pre></td></tr></table></figure></p>
<p>好吧，一切从头来，一个文件一个文件的添加，每次hexo
generate一下。终于找到了一个有问题的文件。先注释掉再说，后面慢慢查是什么特殊字符引起的问题。好在可以更新博客了。</p>
<h3 id="反复">反复</h3>
<p>选定了Next主题，又出现了反复，加上评论系统disqus发现博客白屏了，只有一个文件头显示。可是加功能一时爽，调试火葬场。当时实在记不起来到底是加了什么使得博客又不工作了。只能重头再来。在找主题的过程中发现star排名第四的hexo-theme-apollo已经停止开发，作者一句话让我决定不再折腾什么主题了：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">专注文章内容的创作胜过博客样式的美观，祝各位玩的开心:</span><br></pre></td></tr></table></figure></p>
<h3 id="后续工作">后续工作</h3>
<ol type="1">
<li>追查出什么特殊字符引起了hexo generate出现问题</li>
<li>看是否能复原评论系统，如果不能先这样吧，只要不耽误写博文。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/08/07/Expectation-and-variance-of-poisson-distribution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/08/07/Expectation-and-variance-of-poisson-distribution/" class="post-title-link" itemprop="url">Expectation and Variance of Poisson Distribution</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-08-07 21:49:00" itemprop="dateCreated datePublished" datetime="2017-08-07T21:49:00+02:00">2017-08-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Pmf of Poisson Distribution is as follows:</p>
<p><span class="math display">\[f(X=k;\lambda)=\frac{\lambda^k
e^{-\lambda}}{k!}\]</span></p>
<p>Our aim is to derive the the expectation of <span
class="math inline">\(E(X)\)</span> and the variance <span
class="math inline">\(Var(X)\)</span>. Given that the formula of
expectation: <span class="math display">\[
E(X)=\sum_{k=0}^{\infty} k \frac{\lambda^k e^{-\lambda }}{k!}
\]</span></p>
<p>Notice that when <span class="math inline">\(k=0\)</span>, the
formula is equal to 0, that is:</p>
<p><span class="math display">\[\sum_{k=0}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}\Large|_{k=0}=0\]</span></p>
<p>Then, the formula become as followed:</p>
<p><span class="math display">\[E(X)=\sum_{k=1}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}\]</span></p>
<p><span
class="math display">\[\begin{aligned}E(X)&amp;=\sum_{k=0}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}=\sum_{k=0}^{\infty}
\frac{\lambda^ke^{-\lambda}}{(k-1)!}\\&amp;=\sum_{k=0}^{\infty}  \frac{\lambda^{k-1}\lambda
e^{-\lambda}}{(k-1)!}\\&amp;=\lambda
e^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}\end{aligned}\]</span></p>
<p>Now we need take advantage of Taylor Expansion, recall that:</p>
<p><span
class="math display">\[e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\cdots+\frac{x^{k-1}}{(k-1)!}=\sum_{k=1}^{\infty}\frac{x^{k-1}}{(k-1)!}\]</span></p>
<p>Compare <span class="math inline">\(E(X)\)</span>, we can get:</p>
<p><span class="math display">\[E(X)=\lambda
e^{-\lambda}e^\lambda=\lambda\]</span></p>
<p>As known that <span
class="math inline">\(Var(X)=E(X^2)-(E(x))^2\)</span>, we just get <span
class="math inline">\(E(X^2)\)</span>. Given that:</p>
<p><span class="math display">\[E(X)=\sum_{k=1}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}=\lambda\]</span></p>
<p>we can use this formula to derive the <span
class="math inline">\(E(X^2)\)</span>,</p>
<p><span
class="math display">\[\begin{aligned}E(X)=&amp;\sum_{k=1}^{\infty} k
\frac{\lambda^ke^{-\lambda}}{k!}=\lambda\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}
k \frac{\lambda^k}{k!}=\lambda
e^{\lambda}\\\Leftrightarrow&amp;\frac{\partial\sum_{k=1}^{\infty} k
\frac{\lambda^k}{k!}}{\partial \lambda}=\frac{\partial \lambda
e^{\lambda}}{\partial
\lambda}\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k-1}}{k!}=e^\lambda+\lambda
e^\lambda\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k-1}e^{-\lambda}}{k!}=1+\lambda
\\\Leftrightarrow&amp;\sum_{k=1}^{\infty}k^2\frac{\lambda^{k}e^{-\lambda}}{k!}=\lambda+\lambda^2=E(X^2)\end{aligned}\]</span></p>
<p>then,</p>
<p><span
class="math display">\[Var(X)=E(X^2)-(E(X))^2=\lambda+\lambda^2-(\lambda)^2=\lambda\]</span></p>
<p>Thus, we have proved that the Expectation and the Variance of Poisson
Distribution are both <span class="math inline">\(\lambda\)</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/07/09/Gaussian-Disriminant-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/07/09/Gaussian-Disriminant-Analysis/" class="post-title-link" itemprop="url">Gaussian Discriminant Analysis</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-07-09 18:54:33" itemprop="dateCreated datePublished" datetime="2017-07-09T18:54:33+02:00">2017-07-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="preface">Preface</h3>
<p>There are many classification algorithm such as Logistic Regression,
SVM and Decision Tree etc. Today we'll talk about Gaussian Discriminant
Analysis(GDA) Algorithm, which is not so popular. Actually, Logistic
Regression performance better than GDA because it can fit any
distributions from exponential family. However, we can learn more
knowledge about gaussian distribution from the algorithm which is the
most import distribution in statistics. Furthermore, if you want to
understand Gaussian Mixture Model or Factor Analysis, GDA is a good
start.</p>
<p>We, firstly, talk about Gaussian Distribution and Multivariate
Gaussian Distribution, in which section, you'll see plots about Gaussian
distributions with different parameters. Then we will learn GDA
classification algorithm. We'll apply GDA to a dataset and see the
consequnce of it.</p>
<h3 id="multivariate-gaussian-distribution">Multivariate Gaussian
Distribution</h3>
<h4 id="gaussian-distribution">Gaussian Distribution</h4>
<p>As we known that the pdf(Probability Distribution Function) of
gaussian distribution is a bell-curve, which is decided by two
parameters <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\sigma^2\)</span>. The figure below shows us a
gaussian distribution with <span class="math inline">\(\mu=0\)</span>
and <span class="math inline">\(\sigma^2=1\)</span>, which is often
referred to <span class="math inline">\(\mathcal{N}(\mu,
\sigma^2)\)</span>. Thus, Figure1 is distributed normally with <span
class="math inline">\(\mathcal{N}(0,1)\)</span>. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-normal_1.jpg" />
Figure 1. Gaussian Distribution with <span
class="math inline">\(\mu=0\)</span> and <span
class="math inline">\(\sigma^2=1\)</span>.</p>
<p>Actually, parameter <span class="math inline">\(\mu\)</span> and
<span class="math inline">\(\sigma^2\)</span> are exactly the mean and
the variance of the distribution. Therefore, <span
class="math inline">\(\sigma\)</span> is the stand deviation of normal
distribution. Let's take a look at area between red lines and magenta
lines, which are respectively range from <span
class="math inline">\(\mu\pm\sigma\)</span> and from <span
class="math inline">\(\mu\pm2\sigma\)</span>. The area between redlines
accounts for 68.3% of the total area under the curve. That is, there are
68.3% samples are between <span
class="math inline">\(\mu-\sigma\)</span> and <span
class="math inline">\(\mu+\sigma\)</span> . Likely, there are 95.4%
samples are between <span class="math inline">\(\mu-2\sigma\)</span> and
<span class="math inline">\(\mu+2\sigma\)</span>.</p>
<p>You must want to know how these two parameter influence the shape of
PDF of gaussian distribution. First of all, when we change <span
class="math inline">\(\mu\)</span> with fixed <span
class="math inline">\(\sigma^2\)</span>, the curve is the same as before
but move along the random variable axis.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-normal_2.jpg" /></p>
<p>Figure 2. Probability Density Function curver with <span
class="math inline">\(\mu=\pm2\)</span> and <span
class="math inline">\(\sigma=1\)</span>.</p>
<p>So, what if when we change <span
class="math inline">\(\sigma\)</span> then? Figure3. illustrates that
smaller <span class="math inline">\(\sigma\)</span> lead to sharper
shape of pdf. Conversely, larger <span
class="math inline">\(\sigma\)</span> brings us broader curves.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-normal_3.jpg" /></p>
<p>Figure 3. Probability Density Function curver with <span
class="math inline">\(\mu=0\)</span> and change <span
class="math inline">\(\sigma\)</span>.</p>
<p>Some may wonder what is the form of <span
class="math inline">\(p(x)\)</span> of a gaussian distribution, I just
demonstrate here, you can compare Normal distribution with Multivariate
Gaussian.</p>
<p><span class="math display">\[\mathcal{N(x|\mu,
\sigma^2)}=\frac{1}{\sqrt{2\pi}\sigma}
e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2 } } \]</span></p>
<h4 id="multivariate-gaussian">Multivariate Gaussian</h4>
<p>For convenience, we first see what is form of Multivariate Guassian
Distribution:</p>
<p><span class="math display">\[\mathcal{N(x|\mu, \Sigma)}=\frac{1}{ {
(2\pi)}^{\frac{d}{2 } } |\Sigma|^{\frac{1}{2 } } }
e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)}\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean, <span
class="math inline">\(\Sigma\)</span> is the covariance matrices, <span
class="math inline">\(d\)</span> is the dimension of random variable
<span class="math inline">\(x\)</span>, specfically, 2-dimensional
gaussian distribution, we have:</p>
<p><span class="math display">\[\mathcal{N(x|\mu,
\Sigma)}=\frac{1}{\sqrt{2\pi}|\Sigma|^{\frac{1}{2 } } }
e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)}\]</span></p>
<p>In order to get an intuition of Multivariate Guassian Distribution,
We first take a look at a distribution with <span
class="math inline">\(\mu=\begin{pmatrix}0\\0\end{pmatrix}\)</span> and
<span
class="math inline">\(\Sigma=\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}\)</span>.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-mvn_1.jpg" /></p>
<p>Figure 4. 2-dimensional gaussian distribution with <span
class="math inline">\(\mu=\begin{pmatrix}0\\0\end{pmatrix}\)</span> and
<span
class="math inline">\(\Sigma=\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}\)</span>.</p>
<p>Notice that the the figure is rather than a curve but a 3-dimensional
diagram. Just like normal distribution pdf, <span
class="math inline">\(\sigma\)</span> determines the shape of the
figure. However, there are 4 entries of <span
class="math inline">\(\Sigma\)</span> can be changed in this example.
Given that we need compute <span class="math inline">\(|\Sigma|\)</span>
as denominator and <span class="math inline">\(\Sigma^{-1}\)</span>
which demands non-zero determinant of <span
class="math inline">\(\Sigma\)</span>, we must keep in mind that <span
class="math inline">\(|\Sigma|\)</span> is positive.</p>
<h5 id="change-mu">1. change <span
class="math inline">\(\mu\)</span></h5>
<p>Rather than change <span class="math inline">\(\Sigma\)</span>, we
firstly take a look at how the contour looks like when changing <span
class="math inline">\(\mu\)</span>. Figure 5. illustrates the contour
variation when changing <span class="math inline">\(\mu\)</span>. As we
can see, we only move the center of the contour during the variation of
<span class="math inline">\(\mu\)</span>. i.e. <span
class="math inline">\(\mu\)</span> detemines the position of pdf rather
than the shape. Next, we will see how entries in <span
class="math inline">\(\Sigma\)</span> influence the shape of pdf.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-contour.jpg" /></p>
<p>Figure 5. Contours when change <span
class="math inline">\(\mu\)</span> with <span
class="math inline">\(\Sigma=\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}\)</span>.</p>
<h5 id="change-diagonal-entries-of-sigma">2. change diagonal entries of
<span class="math inline">\(\Sigma\)</span></h5>
<p>If scaling diagonal entries, we can see from figure 6. samples are
concentrated to a smaller range when change <span
class="math inline">\(\Sigma\)</span> from <span
class="math inline">\(\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}\)</span>
to <span
class="math inline">\(\begin{pmatrix}0.3&amp;0\\0&amp;0.3\end{pmatrix}\)</span>.
Similarly, if we alter <span class="math inline">\(\Sigma\)</span> to
<span
class="math inline">\(\begin{pmatrix}3&amp;0\\0&amp;3\end{pmatrix}\)</span>,
then figure will spread out.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-0.3000.3.jpg" /></p>
<p>Figure 6. Density when scaling diagonal entries to 0.3.</p>
<p>What if we change only one entry of the diagonal? Figure 7. shows the
variation of the density when change <span
class="math inline">\(\Sigma\)</span> to <span
class="math inline">\(\begin{pmatrix}1&amp;0\\0&amp;5\end{pmatrix}\)</span>.
Notice the parameter spuashes and stretches the figure along coordinate
axis.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-1005.jpg" /></p>
<p>Figure 7. Density when scaling one of the diagonal entries.</p>
<h5 id="change-secondary-diagonal-entries-of-sigma">3. change secondary
diagonal entries of <span class="math inline">\(\Sigma\)</span></h5>
<p>We now try to change entries along secondary diagonal. Figure 8.
demonstrates that the variation of density is no longer parallel to
<span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> axis, where <span
class="math inline">\(\Sigma=\begin{pmatrix}1
&amp;0.5\\0.5&amp;1\end{pmatrix}\)</span>.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-10.50.51.jpg" /></p>
<p>Figure 8. Density when scaling secondary diagonal entries to 0.5</p>
<p>When we alter secondary entries to negative 0.5, the direction of
contour presents a mirror to contour when positive.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-1-0.5-0.51.jpg" /></p>
<p>Figure 9. Density when scaling secondary diagonal entries to -0.5</p>
<p>In light of the importance of determinant of <span
class="math inline">\(\Sigma\)</span>, what will happen if the
determinant is close to zero. Actually, we can, informally, take
determinant of a matrice as the volume of which. Similarly, when
determinant is smaller, the volume under density curve become smaller.
Figure 10. illustrates the circumstance we talked above where <span
class="math inline">\(\Sigma=\begin{pmatrix}1&amp;0.99\\0.99&amp;1\end{pmatrix}\)</span>.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-01-10.990.991.jpg" /></p>
<p>Figure 10. Density when determinant is close to zero.</p>
<h3 id="gaussian-discriminant-analysis">Gaussian Discriminant
Analysis</h3>
<h4 id="intuition">Intuition</h4>
<p>When input features <span class="math inline">\(x\)</span> are
continuous variables, we can use GDA classify data. Firstly, let's take
a look at how GDA to do the job. Figure 11. show us two gaussian
distributions, they share the same covariance <span
class="math inline">\(\Sigma=\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}\)</span>
, and repectively with parameter <span
class="math inline">\(\mu_0=\begin{pmatrix}1\\1\end{pmatrix}\)</span>
and <span
class="math inline">\(\mu_1=\begin{pmatrix}-1\\-1\end{pmatrix}\)</span>.
Imagine you have some data which fall into the cover of the first and
second Gaussian Distribution. If we can find such distributions to fit
the data, then we'll have the capcity to decide which is new data coming
from, the first or the second one.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-08-gda1.jpg" /></p>
<p>Figure 11. Two gaussian distributions with respect to <span
class="math inline">\(\mu_0=\begin{pmatrix}1\\1\end{pmatrix}\)</span>
and <span
class="math inline">\(\mu_1=\begin{pmatrix}-1\\-1\end{pmatrix}\)</span>
, and <span
class="math inline">\(\Sigma=\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}\)</span></p>
<p>Specifically, let's look at a concrete example, Figure 12 are samples
drawn from two Gaussian distribution. There are 100 blue '+'s and 100
red 'o's. Assume that we have such data to be classified. We can apply
GDA to solve the problem.</p>
<p>CODE:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%% octave</span><br><span class="line">pkg load statistics</span><br><span class="line"></span><br><span class="line">m=200;</span><br><span class="line">n=2;</span><br><span class="line">rp=mvnrnd([1 1],[1 0;0 1],m/2);%生成正样本1</span><br><span class="line">rn=mvnrnd([4 4],[1 0;0 1],m/2);%生成负样本0</span><br><span class="line">y=[ones(m/2,1);zeros(m/2,1)];</span><br><span class="line"></span><br><span class="line">figure;hold on;</span><br><span class="line"></span><br><span class="line">plot3(rp(:,1),rp(:,2),y(1:m/2,1),&#x27;b+&#x27;);</span><br><span class="line">plot3(rn(:,1),rn(:,2),y(m/2+1:m,1),&#x27;ro&#x27;);</span><br><span class="line">axis([-3 8 -3 8]);</span><br></pre></td></tr></table></figure>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-08-samples.jpg" /></p>
<p>Figure 12. 200 samples drawn from two Gaussian Distribution with
parameters <span
class="math inline">\(\mu_0=\begin{bmatrix}1\\1\end{bmatrix},\mu_1=\begin{bmatrix}4\\4\end{bmatrix},\Sigma=\begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix}\)</span>.</p>
<h4 id="definition">Definition</h4>
<p>Now, let's define the algorithm. Firstly we assume discrete random
variable classes <span class="math inline">\(y\)</span> are distributed
Bernoulli and parameterized by <span
class="math inline">\(\phi\)</span>, then we have:</p>
<p><span class="math display">\[y\sim {\rm Bernoulli}(\phi)\]</span></p>
<p>Concretely, the probablity of <span
class="math inline">\(y=1\)</span> is <span
class="math inline">\(\phi\)</span>, and <span
class="math inline">\(1-\phi\)</span> when <span
class="math inline">\(y=0\)</span>. We can simplify two equations to
one:</p>
<p><span
class="math inline">\(p(y|\phi)=\phi^y(1-\phi)^{1-y}\)</span></p>
<p>Apparently, <span class="math inline">\(p(y=1|\phi)=\phi\)</span> and
<span class="math inline">\(p(y=0|\phi)=1-\phi\)</span> given that y can
only be <span class="math inline">\(0\)</span> or <span
class="math inline">\(1\)</span>.</p>
<p>Another assumption is that we consider <span
class="math inline">\(x\)</span> are subject to different Gaussian
Distributions given different <span class="math inline">\(y\)</span>. We
assume the two Gaussian distributions share the same covariance and
different <span class="math inline">\(\mu\)</span>. Based on above all,
then</p>
<p><span class="math display">\[p(x|y=0)=\frac{1}{(2\pi)^{\frac{d}{2 } }
|\Sigma|^{\frac{1}{2 } }
}e^{-\frac{1}{2}(x-\mu_0)^{T}\Sigma^{-1}(x-\mu_0)}\]</span></p>
<p><span class="math display">\[p(x|y=1)=\frac{1}{(2\pi)^{\frac{d}{2 } }
|\Sigma|^{\frac{1}{2 } }
}e^{-\frac{1}{2}(x-\mu_1)^{T}\Sigma^{-1}(x-\mu_1)}\]</span></p>
<p>i.e. <span class="math inline">\(x|y=0 \sim
\mathcal{N}(\mu_0,\Sigma)\)</span> and <span class="math inline">\(x|y=1
\sim \mathcal{N}(\mu_1,\Sigma)\)</span>. suppose we have <span
class="math inline">\(m\)</span> samples, it is hard to compute <span
class="math inline">\(p(x^{(1)}, x^{(2)},
x^{(3)},\cdots,x^{(m)}|y=0)\)</span> or <span
class="math inline">\(p(x^{(1)}, x^{(2)},
x^{(3)},\cdots,x^{(m)}|y=1)\)</span> . In general, we assume the
probabilty of <span class="math inline">\(x^{(i)}\)</span> <span
class="math inline">\(p(x^{(i)}|y=0)\)</span> is independent to any
<span class="math inline">\(p(x^{(j)}|y=0)\)</span>, then we have:</p>
<p><span
class="math display">\[p(X|y=0)=\prod_{i=1\,y^{(i)}\neq1}^{m}\frac{1}{(2\pi)^{\frac{d}{2
} } |\Sigma|^{\frac{1}{2 } }
}e^{-\frac{1}{2}(x^{(i)}-\mu_0)^{T}\Sigma^{-1}(x^{(i)}-\mu_0)}\]</span></p>
<p>Vice versa,</p>
<p><span class="math display">\[p(X|y=1)=\prod_{i=1\,y^{(i)}\neq
0}^{m}\frac{1}{(2\pi)^{\frac{d}{2 } } |\Sigma|^{\frac{1}{2 } }
}e^{-\frac{1}{2}(x^{(i)}-\mu_1)^{T}\Sigma^{-1}(x^{(i)}-\mu_1)}\]</span></p>
<p>Here <span class="math inline">\(X=(x^{(1)}, x^{(2)},
x^{(3)},\cdots,x^{(m)})\)</span>. Now, we want to maximize <span
class="math inline">\(p(X|y=0)\)</span> and <span
class="math inline">\(p(X|y=1)\)</span>. Why is that, because we hope
find parameters that let <span
class="math inline">\(p(X|y=0)p(X|y=1)\)</span> largest, based on that
the samples are from the two Gaussian Distributions. These samples we
have are more likely emerging. Thus, our task is to maximize <span
class="math inline">\(p(X|y=0)p(X|y=1)\)</span> , we let</p>
<p><span
class="math display">\[\mathcal{L}(\phi,\mu_0,\mu_1,\Sigma)=\arg\max
p(X|y=0)p(X|y=1)=\arg\max\prod_{i=1}^{m}p(x^{(i)},
y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\]</span></p>
<p>It's tough for us to maximize <span
class="math inline">\(\mathcal{L}(\phi,\mu_0,\mu_1,\Sigma)\)</span>.
Notice function <span class="math inline">\(\log\)</span> is monotonic
increasing. Thus, we can maximize <span
class="math inline">\(\log\mathcal{L}(\phi,\mu_0,\mu_1,\Sigma)\)</span>
instead of <span
class="math inline">\(\mathcal{L}(\phi,\mu_0,\mu_1,\phi)\)</span>,
then:</p>
<p><span
class="math display">\[\begin{aligned}\ell(\phi,\mu_0,\mu_1,\Sigma)&amp;=\log\mathcal{L}(\phi,\mu_0,\mu_1,\Sigma)\\&amp;=\arg\max\log\prod_{i=1}^{m}p(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\&amp;=\arg\max\log\prod_{i=1}^{m}p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)\\&amp;=\arg\max\sum_{i=1}^{m}p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)+p(y^{(i)};\phi)\\&amp;=\arg\max\sum_{i=1}^{m}p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)+\sum_{i=1}^{m}p(y^{(i)};\phi)\end{aligned}\]</span></p>
<p>By now, we have found a convex function with respect parameters <span
class="math inline">\(\mu_0, mu_1,\Sigma\)</span> and <span
class="math inline">\(\phi\)</span>. Next section, we'll obtain these
parameter through partial derivative.</p>
<h4 id="solution">Solution</h4>
<p>To estimate these four parameters, we just apply partial derivative
to <span class="math inline">\(\ell\)</span>. Now we estimate <span
class="math inline">\(\phi\)</span> in the first place. We let <span
class="math inline">\(\frac{\partial \ell}{\partial \phi}=0\)</span>,
then</p>
<p><span class="math display">\[\begin{aligned}\frac{\partial
\ell(\phi,\mu_0,\mu_1,\Sigma)}{\partial
\phi}=0&amp;\Rightarrow\frac{\partial
\arg\max\sum_{i=1}^{m}p(x_i|y;\mu_0,\mu_1,\Sigma)+\sum_{i=1}^{m}p(y_i;\phi)}{\partial
\phi}=0\\&amp;\Rightarrow\frac{\partial\sum_{i=1}^{m}\log
p(y^{(i)};\phi)}{\partial
\phi}=0\\&amp;\Rightarrow\frac{\partial\sum_{i=1}^{m}\log \phi^{y^{(i) }
} (1-\phi)^{(1-y^{(i)}) } } {\partial
\phi}=0\\&amp;\Rightarrow\frac{\partial\sum_{i=1}^{m}{y^{(i) } } \log
\phi+{(1-y^{(i)})}\log(1-\phi)}{\partial
\phi}=0\\&amp;\Rightarrow\frac{\partial\sum_{i=1}^{m}{ {
1}{\{y^{(i)}=1\} } }\log \phi+{1}{\{y^{(i)}=0\} } \log(1-\phi)}{\partial
\phi}=0\\&amp;\Rightarrow\phi=\frac{1}{m}\sum_{i=1}^{m}1\{y^{(i)}=1\}\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(\mu_0\)</span> and <span
class="math inline">\(\mu_1\)</span> is symmetry in the equation, thus,
we need only obtain one of them. Here we take the derivative to <span
class="math inline">\(\mu_0\)</span></p>
<p><span class="math display">\[\begin{aligned}\frac{\partial
\ell(\phi,\mu_0,\mu_1,\Sigma)}{\partial
\mu_0}=0&amp;\Rightarrow\frac{\partial
\arg\max\sum_{i=1}^{m}p(x_i|y;\mu_0,\mu_1,\Sigma)+\sum_{i=1}^{m}p(y_i;\phi)}{\partial
\mu_0}=0\\&amp;\Rightarrow\frac{\partial\sum_{i=1}^{m} \log
p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)}{\partial
\mu_0}=0\\&amp;\Rightarrow\frac{\partial
\sum_{i=1}^{m}\log\frac{1}{(2\pi)^{\frac{d}{2 } } |\Sigma|^{\frac{1}{2 }
} }e^{-\frac{1}{2}(x^{(i)}-\mu_0)^T\Sigma^{-1}(x^{(i)}-\mu_0) } }
{\partial \mu_0}=0\\&amp;\Rightarrow0+\frac{\partial
\sum_{i=1}^{m}{-\frac{1}{2}(x^{(i)}-\mu_0)^T\Sigma^{-1}(x^{(i)}-\mu_0) }
} {\partial \mu_0}=0\end{aligned}\]</span></p>
<p>We have <span class="math inline">\(\frac{\partial X^TAX}{\partial
X}=(A+A^T)X\)</span>，let <span
class="math inline">\((x^{(i)}-\mu_0)=X\)</span>, then,</p>
<p><span class="math display">\[\begin{aligned}\frac{\partial
\ell(\phi,\mu_0,\mu_1,\Sigma)}{\partial \mu_0}=0&amp;\Rightarrow
0+\frac{\partial
\sum_{i=1}^{m}{-\frac{1}{2}(x^{(i)}-\mu_0)^T\Sigma^{-1}(x^{(i)}-\mu_0) }
} {\partial
\mu_0}=0\\&amp;\Rightarrow{\sum_{i=1}^{m}-\frac{1}{2}((\Sigma^{-1})^T+\Sigma^{-1})(x^{(i)}-\mu_0)\cdot(-1)}=0\\&amp;\Rightarrow
\sum_{i=1}^{m}1\{y^{(i)}=0\}x^{(i)}=\sum_{i=1}^{m}1\{y^{(i)}=0\}\mu_0\\&amp;\Rightarrow\mu_0=\frac{\sum_{i=1}^{m}1\{y^{(i)}=0\}x^{(i)
} } {\sum_{i=1}^{m}1\{y^{(i)}=0\} } \end{aligned}\]</span></p>
<p>Simlarly,</p>
<p><span
class="math display">\[\mu_1=\frac{\sum_{i=1}^{m}1\{y^{(i)}=1\}x^{(i) }
} {\sum_{i=1}^{m}1\{y^{(i)}=1\} } \]</span></p>
<p>Before calculate <span class="math inline">\(\Sigma\)</span>, I first
illustrate the truth that <span
class="math inline">\(\frac{\partial|\Sigma|}{\partial\Sigma}=|\Sigma|\Sigma^{-1},\quad
\frac{\partial\Sigma^{-1 } } {\partial\Sigma}=-\Sigma^{-2}\)</span>,
then</p>
<p><span class="math display">\[\begin{aligned}\frac{\partial
\ell(\phi,\mu_0,\mu_1,\Sigma)}{\partial
\Sigma}=0&amp;\Rightarrow\frac{\partial\sum_{i=1}^{m} \log
p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)+\sum_{i=1}^{m} \log
p(y^{(i)};\phi)}{\partial \Sigma}=0\\&amp;\Rightarrow\frac{\partial
\sum_{i=1}^{m}\log\frac{1}{(2\pi)^{\frac{d}{2 } } |\Sigma|^{\frac{1}{2 }
} }e^{-\frac{1}{2}(x^{(i)}-\mu_{y^{(i) } }
)^T\Sigma^{-1}(x^{(i)}-\mu_{y^{(i) } } ) } } {\partial
\Sigma}=0\\&amp;\Rightarrow\frac{\partial
\sum_{i=1}^{m}-\frac{d}{2}\log2\pi}{\partial \Sigma}+\frac{\partial
\sum_{i=1}^{m}-\frac{1}{2}\log|\Sigma|}{\partial \Sigma}+\frac{\partial
\sum_{i=1}^{m}{-\frac{1}{2}(x^{(i)}-\mu_{y^{(i) } }
)^T\Sigma^{-1}(x^{(i)}-\mu_{y^{(i) } } ) } } {\partial
\Sigma}=0\\&amp;\Rightarrow\frac{\partial
\sum_{i=1}^{m}-\frac{1}{2}\log|\Sigma|}{\partial \Sigma}+\frac{\partial
\sum_{i=1}^{m}{-\frac{1}{2}(x^{(i)}-\mu_{y^{(i) } }
)^T\Sigma^{-1}(x^{(i)}-\mu_{y^{(i) } } ) } } {\partial
\Sigma}=0\\&amp;\Rightarrow
m\frac{1}{|\Sigma|}|\Sigma|\Sigma^{-1}+\sum_{i=1}^m(x^{(i)}-\mu_{y^{(i)
} } )^T(x^{(i)}-\mu_{y^{(i) } }
)(-\Sigma^{-2}))=0\\&amp;\Rightarrow\Sigma=\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu_{y^{(i)
} } )(x^{(i)}-\mu_{y^{(i) } } )^T\end{aligned}\]</span></p>
<p>In spite of the harshness of the deducing, the outcome are pretty
beautiful. Next, we will apply these parameters and see how the
estimation performance.</p>
<h3 id="apply-gda">Apply GDA</h3>
<p>Notice the data drawn from two Gaussian Distribution is random, thus,
if you run the code, the outcome may be different. However, in most
cases, distributions drawn by estimated parameters are roughly the same
as the original distributions.</p>
<p><span
class="math display">\[\begin{aligned}&amp;\phi=0.5\\&amp;\mu_0=\begin{bmatrix}4.0551\\4.1008\end{bmatrix}\\&amp;\mu_1=\begin{bmatrix}0.85439\\1.03622\end{bmatrix}\\&amp;\Sigma=\begin{bmatrix}1.118822&amp;-0.058976\\-0.058976&amp;1.023049\end{bmatrix}\end{aligned}\]</span></p>
<p>From Figure 13, We can see contours of two Gaussian distribution, and
most of samples are correctly classified.</p>
<p>CODE:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%% octave </span><br><span class="line">phi=length(find(y==1))/m;</span><br><span class="line">mu_0 = sum(rn)/length(find(y==0))</span><br><span class="line">mu_1 = sum(rp)/length(find(y==1))</span><br><span class="line">X = [rp;rn];</span><br><span class="line">X_mu1 = X(find(y==1),:)-mu_1;</span><br><span class="line">X_mu0 = X(find(y==0),:)-mu_0;</span><br><span class="line">X_mu = [X_mu1; X_mu1];</span><br><span class="line">sigma = (X_mu&#x27;*X_mu)/m</span><br><span class="line"></span><br><span class="line">[x1 y1]=meshgrid(linspace(-3,8,100)&#x27;,linspace(-3,8,100)&#x27;);</span><br><span class="line">X1=[x1(:) y1(:)];</span><br><span class="line">z1=mvnpdf(X1,mu_1,sigma);</span><br><span class="line">contour(x1,y1,reshape(z1,100,100),8);</span><br><span class="line">hold on;</span><br><span class="line">z2=mvnpdf(X1,mu_0,sigma);</span><br><span class="line">contour(x1,y1,reshape(z2,100,100),8);</span><br></pre></td></tr></table></figure>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-08-samples_fit.jpg" /></p>
<p>Figure 13. Contours drawn from parameters estimated.</p>
<p>In fact, we can compute the probability of each data point to predict
which distribution it is more likely belongs, for example, if we want to
predict <span
class="math inline">\(x=\begin{pmatrix}0.88007\\3.9501\end{pmatrix}\)</span>
is more of the left distribution or the right, we apply <span
class="math inline">\(x\)</span> to these two distribution:</p>
<p><span
class="math display">\[\begin{aligned}p\left(x=\begin{bmatrix}0.88\\3.95\end{bmatrix}\Bigg|y=0\right)=&amp;\frac{1}{2\pi|\Sigma|^{\frac{1}{2
} }
}e^{-\frac{1}{2}{\begin{bmatrix}x_1-\mu_1\\x_2-\mu_2\end{bmatrix}^T\Sigma^{-1}\begin{bmatrix}x_1-\mu_1\\x_2-\mu_2\end{bmatrix}
} }\\=&amp;\frac{1}{ {
2\pi}\left|\begin{matrix}1.1188&amp;-0.059\\-0.059&amp;1.023\end{matrix}\right|^{\frac{1}{2
} }
}e^{-\frac{1}{2}{\begin{bmatrix}-3.175\\-0.151\end{bmatrix}^T\begin{bmatrix}0.896&amp;-0.052\\-0.0520&amp;0.98\end{bmatrix}\begin{bmatrix}-3.175\\-0.151\end{bmatrix}
} }\\=&amp;\frac{1}{2\pi\sqrt{(1.141) } } e^{-\frac{1}{2}\times
9.11}=0.149\times 0.01=0.0015\end{aligned}\]</span></p>
<p>and</p>
<p><span
class="math display">\[\begin{aligned}p\left(x=\begin{bmatrix}0.88\\3.95\end{bmatrix}\Bigg|
y=1\right)&amp;\frac{1}{2\pi|\Sigma|^{\frac{1}{2 } }
}e^{-\frac{1}{2}{\begin{bmatrix}x_1-\mu_1\\x_2-\mu_2\end{bmatrix}^T\Sigma^{-1}\begin{bmatrix}x_1-\mu_1\\x_2-\mu_2\end{bmatrix}
} }\\=&amp;\frac{1}{ {
2\pi}\left|\begin{matrix}1.1188&amp;-0.059\\-0.059&amp;1.023\end{matrix}\right|^{\frac{1}{2
} }
}e^{-\frac{1}{2}{\begin{bmatrix}0.03\\2.91\end{bmatrix}^T\begin{bmatrix}0.896&amp;-0.052\\-0.0520&amp;0.98\end{bmatrix}\begin{bmatrix}0.03\\2.91\end{bmatrix}
} }\\=&amp;\frac{1}{2\pi\sqrt{(1.141) } } e^{-\frac{1}{2}\times
8.336}=0.149\times 0.015=0.0022\end{aligned}\]</span></p>
<p>In light of the equivalency of <span
class="math inline">\(p(y=1)\)</span> and <span
class="math inline">\(p(y=0)\)</span> (both are <span
class="math inline">\(0.5\)</span>), we just compare<span
class="math inline">\(p\left(x=\begin{bmatrix}0.88\\3.95\end{bmatrix}\Bigg|
y=1\right)\)</span> to <span
class="math inline">\(p\left(x=\begin{bmatrix}0.88\\3.95\end{bmatrix}\Bigg|
y=0\right)\)</span>. Apparently, this data point is predicted from the
left distribution, which is a wrong assertion. Actually, in this
example, we have only this data pointed classified incorrectly.</p>
<p>You may wonder why there is a blue line. It turns out that all the
data point below the blue line will be considered as blue class.
Otherwise, data points above the line is classified as the red class.
How it work?</p>
<p>The blue line is decision boundary, if we know the expression of this
line, the decision will be made easier. In fact GDA is a linear
classifier, we will prove it later. Still, we see the data point above,
if we just divide one probability to another, we just need find if the
ratio larger or less than 1. For our example, the ratio is roughly 0.68,
so the data point is classified to be the blue class.</p>
<p><span
class="math display">\[\frac{p\left(x=\begin{bmatrix}0.88\\3.95\end{bmatrix}\Bigg|
y=0\right)p(y=0)}{p\left(x=\begin{bmatrix}0.88\\3.95\end{bmatrix}\Bigg|y=1\right)p(y=1)}=\frac{0.0015}{0.0022}=0.68182&lt;1\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-07-08-decison_boundry.jpg" /></p>
<p>Figure 14. Decision Boundary</p>
<p>If we can obtain the expression of the ratio, that should be good. So
given a new <span class="math inline">\(x\)</span>, we predict problem
is tranformed as followed:</p>
<p><span class="math display">\[x\in \text{red
class}\propto\mathcal{R}=\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)} &gt;
1\]</span></p>
<p>which is equal to</p>
<p><span
class="math display">\[\mathcal{R}=\log\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)}
=\log\frac{\phi}{1-\phi}+\log\frac{\mathcal{N}(x;\mu_1,\Sigma)}{\mathcal{N}(x;\mu_0,\Sigma)}&gt;
0\]</span></p>
<p>Then,</p>
<p><span
class="math display">\[\begin{aligned}\mathcal{R}&amp;=\log\frac{\frac{1}{(2\pi)^{\frac{d}{2
} } |\Sigma|^{\frac{1}{2 } }
}\exp(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))}{\frac{1}{(2\pi)^{\frac{d}{2
} } |\Sigma|^{\frac{1}{2 } }
}\exp(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))}+\log\frac{\phi}{1-\phi}\\&amp;=-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1))+\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0))+\log\frac{\phi}{1-\phi}\\&amp;=-\frac{1}{2}x^T\Sigma^{-1}x+\mu_1^T\Sigma^{-1}x-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+-\frac{1}{2}x^T\Sigma^{-1}x-\mu_0^T\Sigma^{-1}x+\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0+\log\frac{\phi}{1-\phi}\\&amp;=(\mu_0-\mu_1)^T\Sigma^{-1}x-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0+\log\frac{\phi}{1-\phi}\end{aligned}\]</span></p>
<p>Here, <span
class="math inline">\(\mu_1^T\Sigma^{-1}x=x^T\Sigma^{-1}\mu_1\)</span>
because it is a real number. For a real number <span
class="math inline">\(a=a^T\)</span>, moreover, <span
class="math inline">\(\Sigma^{-1}\)</span> is symmetric, so <span
class="math inline">\(\Sigma^{-T}=\Sigma^{-1}\)</span>. Let's set <span
class="math inline">\(w^T=(\mu_1-\mu_0)^T\Sigma^{-1}\)</span> and <span
class="math inline">\(w_0=-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+\frac{1}{2}\mu_0^T\Sigma^{-1}\mu_0+\log\frac{\phi}{1-\phi}\)</span>,
then we have:</p>
<p><span
class="math display">\[\mathcal{R}=\log\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)}
=w^Tx+w_0\]</span></p>
<p>If you plug parameters in the formula, you will find:</p>
<p><span
class="math inline">\(\mathcal{R}=-3.0279x_1-3.1701x_2+15.575=0\)</span></p>
<p>It is the decision boundary(Figure 14.). Since you have got the
decision boundary formula, it is convenient to use the decision boundary
function predict if a data point <span class="math inline">\(x\)</span>
belongs to the blue or red class. If <span
class="math inline">\(\mathcal{R}&gt;0\)</span>, <span
class="math inline">\(x\in \text{red class}\)</span>, otherwise, <span
class="math inline">\(x\in \text{blue class}\)</span>.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Today, we have talked about Guassian Distribution and its
Multivariate form. Then, we assume two groups of data drawn from
Gaussian Distributions. We apply Gaussian Discriminant Analysis to the
data. There are 200 data point, only one is misclassified. In fact we
can deduce GDA to Logistic regression Algorithm(LR). But LR can not
deduce GDA, i.e. LR is a better classifier, especially when we do not
know the distribution of the data. However, if you have known that data
is drawn from Gaussian Distribution, GDA is the better choice.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>Andrew Ng http://cs229.stanford.edu/notes/cs229-notes2.pdf</li>
<li>https://en.wikipedia.org/wiki/Normal_distribution</li>
<li>https://en.wikipedia.org/wiki/Multivariate_normal_distribution</li>
<li>http://www.cnblogs.com/emituofo/archive/2011/12/02/2272584.html</li>
<li>http://m.blog.csdn.net/article/details?id=52190572</li>
<li>张贤达《矩阵分析与应用》:156-158</li>
<li>http://www.tk4479.net/hujingshuang/article/details/46357543</li>
<li>http://www.chinacloud.cn/show.aspx?id=24927&amp;cid=22</li>
<li>http://www.cnblogs.com/jcchen1987/p/4424436.html</li>
<li>http://www.xlgps.com/article/139591.html</li>
<li>http://www.matlabsky.com/thread-10308-1-1.html</li>
<li>http://classes.engr.oregonstate.edu/eecs/fall2015/cs534/notes/GaussianDiscriminantAnalysis.pdf</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/06/17/sample-variance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/06/17/sample-variance/" class="post-title-link" itemprop="url">样本方差为什么除以N-1?（翻译）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-06-17 17:52:38" itemprop="dateCreated datePublished" datetime="2017-06-17T17:52:38+02:00">2017-06-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>原文作者：<a target="_blank" rel="noopener" href="http://www.visiondummy.com/">Vincent Spruy</a></p>
<p>译者：程明波</p>
<p><a
target="_blank" rel="noopener" href="http://www.visiondummy.com/2014/03/divide-variance-n-1/">英文文章地址</a></p>
<p><a
href="http://chengmingbo.github.io/2017/06/17/sample-variance/">译文地址</a></p>
<p>译者注：由于历史原因，高斯分布(Gaussian
Distribution)，正态分布(Normal Distribution)皆指概率密度函数形如<span
class="math inline">\(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)</span>的分布。文中我会采用正态分布的提法。</p>
<h3 id="简介">简介</h3>
<p>本文，呼应标题，我将推导著名正态分布数据均值和方差的计算公式。如果一些读者对于这个问题的“为什么”并不感兴趣，仅仅是对“什么时候使用”感兴趣，那答案就非常简单了：</p>
<p>如果你想预估一份数据的均值和方差(典型情况)，那么方差公式除的是<span
class="math inline">\(N-1\)</span>，即：</p>
<p><span class="math display">\[\sigma^2 = \frac{1}{N-1}\sum_{i=1}^N
(x_i - \mu)^2\]</span></p>
<p>另一种情况，如果整体的真实均值已知，那么方差公式除的就是<span
class="math inline">\(N\)</span>，即：</p>
<p><span class="math display">\[\sigma^2 = \frac{1}{N}\sum_{i=1}^N (x_i
- \mu)^2\]</span></p>
<p>然而，前一种情况，会是你遇到更典型的情形。一会儿，我会举一个预估高斯白噪音的离散程度例子。例子中高斯白噪音的均值是已知的0，这种情况下，我们只需要估计方差。</p>
<p>如果数据是正态分布，我们可以完全用均值<span
class="math inline">\(\mu\)</span>和方差<span
class="math inline">\(\sigma^2\)</span>刻画这个分布。其中，方差是标准差<span
class="math inline">\(\sigma\)</span>的平方，标准差代表了每个数据点偏离均值点的平均距离，也就是说，方差表示了数据离散程度。对于正态分布，68.3%的数据的值会介于<span
class="math inline">\(\mu-\sigma\)</span>和<span
class="math inline">\(\mu+\sigma\)</span>之间。下面图片展示是一个正态分布的概率密度函数，他的均值是<span
class="math inline">\(\mu=10\)</span>,方差是<span
class="math inline">\(\sigma^2=3^2=9\)</span>：</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-06-20-110159.jpg" /></p>
<p><strong>图1.</strong> 正态分布概率密度函数.
对于正态分布数据，68%的样本落在均值<span
class="math inline">\(\pm\)</span>方差。</p>
<p>通常，我们拿不到全部的全体数据。上面的例子中，典型的情况是我们有一些观察数据，但是，我们没有上图中x轴上所有可能的观察数据。例如我们可能有下面一些观察数据：</p>
<p>表1</p>
<table>
<thead>
<tr class="header">
<th>观察数据ID</th>
<th>观察值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>观察数据 1</td>
<td>10</td>
</tr>
<tr class="even">
<td>观察数据 2</td>
<td>12</td>
</tr>
<tr class="odd">
<td>观察数据 3</td>
<td>7</td>
</tr>
<tr class="even">
<td>观察数据 4</td>
<td>5</td>
</tr>
<tr class="odd">
<td>观察数据 5</td>
<td>11</td>
</tr>
</tbody>
</table>
<p>现在如果我们通过把所有值相加并除以观察的次数，得到经验均值：</p>
<p><span
class="math display">\[\mu=\frac{10+12+7+5+11}{5}=9\tag{1}\]</span>.</p>
<p>通常，我们会假设经验均值接近分布的未知的真实均值，因此，我们可以假设观测数据来自于均值为<span
class="math inline">\(\mu=9\)</span>的正态分布。在这个例子中，分布真实均值是10，
也就是说，经验均值实际上接近于真实均值。</p>
<p>数据的方差计算如下：</p>
<p><span class="math display">\[\begin{aligned}\sigma^2&amp;=
\frac{1}{N-1}\sum_{i=1}^N (x_i - \mu)^2\\&amp;=
\frac{(10-9)^2+(12-9)^2+(7-9)^2+(5-9)^2+(11-9)^2}{4})\\&amp;=
8.5.\end{aligned}\tag{2}\]</span></p>
<p>同样，我们一般假设经验方差接近于基于分布真实未知方差。在此例中，真实方差是9，所以，经验方差也是接近于真实方差。</p>
<p>那么我们手上的问题现在就是为什么我们用于计算经验均值和经验方差的公式是正确的。事实上，另一个我们经常用于计算方差的公式是这样定义的：</p>
<p><span class="math display">\[\begin{aligned}\sigma^2 &amp;=
\frac{1}{N}\sum_{i=1}^N (x_i - \mu)^2 \\&amp;=
\frac{(10-9)^2+(12-9)^2+(7-9)^2+(5-9)^2+(11-9)^2}{4}) \\&amp;=
6.8.\end{aligned}\tag{3}\]</span></p>
<p>公式(2)和公式(3)的唯一不同是前一个公式除的是<span
class="math inline">\(N-1\)</span>，而后一个除的是<span
class="math inline">\(N\)</span>。两个公式都是对的，只是根据不同的场景使用不同的公式。</p>
<p>接下来的部分，我们针对给定一个正态分布的样本集，完成对其未知方差和均值最好估计的完整推导。我们将会看到，一些情况下，方差除的是<span
class="math inline">\(N\)</span>，另一些情况除的是<span
class="math inline">\(N-1\)</span>。</p>
<p>用一个公式近似一个参数(均值或方差)叫做估计量。下面，我们定义一个分布的真实但未知的参数为<span
class="math inline">\(\hat{\mu}\)</span>和<span
class="math inline">\(\hat{\sigma}^2\)</span>。而估计量，例如，经验的平均和经验方差，定义为<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma^2\)</span>。</p>
<p>为了找到最优的估计量，首先，一个整体均值为<span
class="math inline">\(\mu\)</span>标准差为<span
class="math inline">\(\sigma\)</span>的正态分布，对于特定的观察点<span
class="math inline">\(x_i\)</span>，我们需要一个分析相似的表达式。对于一个已知参数的正态分布一般定义为<span
class="math inline">\(N(\mu,\sigma^2)\)</span>。似然函数为：</p>
<p><span class="math display">\[x_i \sim N(\mu,\sigma^2) \Rightarrow
P(x_i;
\mu,\sigma)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}.\tag{4}\]</span></p>
<p>为了计算均值和方差，显然，我们需要这个分布一个以上的样本。接下来，设<span
class="math inline">\(\vec{x}=(x_1,x_2,\cdots,x_N)\)</span>为包含所有的可用样本的向量（例如：表一中所有的值）。如果所有这些样本统计独立，我们可以写出联合似然函数为所有似然函数的乘积：</p>
<p><span
class="math display">\[\begin{aligned}P(\vec{x};\mu,\sigma^2)&amp;=P(x_1,x_2,\cdots,x_n;\mu,\sigma^2)\\&amp;=P(x_1;\mu,\sigma^2)P(x_2;\mu,\sigma^2)\cdots
P(x_N;\mu,\sigma^2)\\&amp;=\prod_{i=1}^{N}P(x_i;\mu,\sigma^2)\end{aligned}.\tag{5}\]</span></p>
<p>把公式(4)代入公式(5)，可得出联合概率密度函数的分析表达式：</p>
<p><span
class="math display">\[\begin{aligned}P({\vec{x};\mu,\sigma})&amp;=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2\sigma^2}(x_i-\mu)^2}\\&amp;=\frac{1}{(2\pi\sigma^2)^{\frac{N}{2}}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(x_i-\mu)^2}\end{aligned}.\tag{6}\]</span></p>
<p>公式(6)在接下来的部分将非常重要。我们会用它推导关于正态分布著名的估计量均值和方差。</p>
<h3 id="最小方差无偏估计量">最小方差，无偏估计量</h3>
<p>决定一个估计量是不是“好”估计量，首先我们需要定义什么是真正的“好”
估计量。说一个估计量好，依赖于两个度量，叫做其偏差(bias)和方差(variance)(是的，我们要讨论均值估计量的方差，以及方差估计量的方差)。本节将简单的讨论这两个度量。</p>
<h4 id="参数偏差">参数偏差</h4>
<p>想象一下，如果我们能拿到全体不同的(互斥)数据子集。类比之前的的例子，假设，除了【表1】中的数据，我们还有完全不同观察结果表2及表3。那么，一个关于均值好的估计量，应该使得这个估计量平均下来等于真实的均值。我们可以接受其中一个自己的经验均值不等于真实均值，但是，一个好的估计量应该保证：对于所有子集均值估计的平均值等于真实均值。这个限制条件用数学化的表示，就是估计量的期望值(Expected
Value)应该等于参数值：</p>
<p><span class="math display">\[E(\mu)=\hat{\mu}\qquad
E(\sigma^2)=\hat{\sigma}^2.\tag{7}\]</span></p>
<p>如果满足上面的条件，那么这些估计量就被称之为“无偏估计”。反之，如果上面的条件不满足，这些估计量叫做“有偏的”，也就是说平均来看，他们或者低估或者高估了参数的真实值。</p>
<h4 id="参数方差">参数方差</h4>
<p>无偏估计量保证平均来看，它们估计的值等于真是参数。但是，这并不意味着每次估计是一个好的估计。比如，如果真实均值为10，一个无偏估计量可以估计全体的其中一个子集的均值为50，而另一个均值为-30。期望的估计的值确实是10，也等于真是的参数值，但是，估计量的质量明显依赖每次估计的离散程度。对于全体5个不同子集，一个估计量产生的估计值(10,15,5,12,8)是无偏的和另一个估计量产生的估计值（50，-30，100，-90，20）（译者注：原文作者最后一个是10，我计算换成20，这样均值才是10）。但是第一个估计量的所有估计值明显比第二个估计量的估计值更接近真实值。</p>
<p>因此，一个好的估计量不仅需要有低偏差，同时也需要低方差。这个方差表示为平均平方误差的估计量：</p>
<p><span
class="math display">\[Var(\mu)=E[(\hat{\mu}-\mu)^2]\]</span></p>
<p><span
class="math display">\[Var(\sigma^2)=E[(\hat{\sigma}-\sigma)^2]\]</span></p>
<p>因此一个好的估计量是低偏差，低方差的。如果存在最优的估计量，那么这个估计应该是无偏的，而且方差比所有的其他可能估计量都要低。这样的一个估计量被称之为最小方差，无偏（MVU）估计量。下一节，我们将会针对一个正态分布推导均值和方差估计量的数学表达式。我们将会看到，一个正态分布的方差MVU估计量在一些假设下需要除以<span
class="math inline">\(N\)</span>，而在另一些假设下需要除以<span
class="math inline">\(N-1\)</span>。</p>
<h3 id="最大似然估计">最大似然估计</h3>
<p>基于整体的一个子集，尽管有大量的获取一个参数估计量的技术，所有这些技术中最简单的可能就数最大似然估计了。</p>
<p>观察值<span
class="math inline">\(\vec{x}\)</span>的概率在公式(6)定义为<span
class="math inline">\(P(\vec{x};\mu,\sigma^2)\)</span>.
如果我们在此函数中固定<span class="math inline">\(x\)</span>和<span
class="math inline">\(\sigma^2\)</span>，当使<span
class="math inline">\(\vec{x}\)</span>变化时，我们就可以获得图(1)的正态分布。但是，我们也可以固定<span
class="math inline">\(\vec{x}\)</span>，使<span
class="math inline">\(\mu\)</span>和（或）<span
class="math inline">\(\sigma^2\)</span>变化。比如，我们可以选择类似前面例子中的<span
class="math inline">\(\vec{x}=(10,12,7,5,11)\)</span>。我们选择固定<span
class="math inline">\(\mu=10\)</span>，同时使<span
class="math inline">\(\sigma^2\)</span>变化。图(2)展示了当<span
class="math inline">\(x\)</span>和<span
class="math inline">\(\mu\)</span>固定时，<span
class="math inline">\(\sigma^2\)</span>对于这个分布取不同值的变化曲线：</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-06-20-110329.jpg" /></p>
<p>图 2. 此图表示了似然函数在特定观察数据<span
class="math inline">\(\vec{x}\)</span>，下固定<span
class="math inline">\(\mu=10\)</span>，<span
class="math inline">\(\sigma^2\)</span>变化曲线。</p>
<p>上图，我们通过固定<span
class="math inline">\(\mu=10\)</span>，令<span
class="math inline">\(\sigma^2\)</span>变化计算了<span
class="math inline">\(P(\vec{x};\sigma^2)\)</span>的似然函数。在结果曲线的每一个数据点代表了似然度，观察值<span
class="math inline">\(\vec{x}\)</span>是一个正态分布在参数<span
class="math inline">\(\sigma^2\)</span>下的样本。那么对应最大似然度的参数值最有可能是从我们定义的分布中产生数据的参数。因此，我们能通过找到似然度曲线的最大值决定最优的<span
class="math inline">\(\sigma^2\)</span>。在此例中，最大值在<span
class="math inline">\(\sigma^2=7.8\)</span>，这样标准差就是<span
class="math inline">\(\sqrt{(\sigma^2)=2.8}\)</span>。事实上，如果给定<span
class="math inline">\(\mu=10\)</span>，通过传统的方法计算，我们会发明方差就是7.8：</p>
<p><span
class="math display">\[\frac{(10-10)^2+(12-10)^2+(7-10)^2+(5-10)^2+(11-10)^2}{5}=7.8\]</span></p>
<p>因此，基于样本数据的方差计算公式只需要简单的通过找到最大的似然函数的最高点。此外，除了固定<span
class="math inline">\(\mu\)</span>，我们可以使<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma^2\)</span>同时变化。然后找到两个估计量对应在两个维度的似然函数的最大值。</p>
<p>要找一个函数的最大值，也很简单，只需要求导使其等于0。如果想找一个有两个变量函数的最大值，我们需要计算每个变量的偏导，再把两个偏导全部设置为0。接下来，设<span
class="math inline">\(\hat{\mu}_{ML}\)</span>为通过极大似然方法得到的总体均值的最优估计量，设<span
class="math inline">\(\hat{\sigma}^2_ML\)</span>为方差的最优估计量。要最大化似然函数，我们可以简单的计算它的(偏)导数，然后赋值为0，如下：</p>
<p><span class="math display">\[\begin{aligned} &amp;\hat{\mu}_{ML} =
\arg\max_\mu P(\vec{x}; \mu, \sigma^2)\\ &amp;\Rightarrow \frac{\partial
P(\vec{x}; \mu, \sigma^2)}{\partial \mu} = 0 \end{aligned}\]</span></p>
<p>及</p>
<p><span class="math display">\[\begin{aligned} &amp;\hat{\sigma}^2_{ML}
= \arg\max_{\sigma^2} P(\vec{x}; \mu, \sigma^2)\\ &amp;\Rightarrow
\frac{\partial P(\vec{x}; \mu, \sigma^2)}{\partial \sigma^2} = 0
\end{aligned}\]</span></p>
<p>下一节，我们将利用这个技术得到<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma^2\)</span>的MVU估计量。我们考虑两种情形：</p>
<p>第一种情形，我们假设分布的真正的均值<span
class="math inline">\(\hat{\mu}\)</span>是已知的，因此，我们只需要估计方差，那么问题就变成在参数为<span
class="math inline">\(\sigma^2\)</span>的一维的极大似然函数中对应找其最大值。这种情况不经常出现，但是，在实际应用中确实存在。例如，如果我们知道一个信号(比如：一幅图中一个像素的颜色值)本来应该有特定的值，但是，信号被白噪音污染了（均值为0的高斯噪音），这时分布的均值是已知的，我们只需要估计方差。</p>
<p>第二种情形就是处理均值和方差的真实值都不知道的情况。这种情况最常见，这时，我们需要基于样本数据估计均值和方差。</p>
<p>后面我们将看到，每种情形产生不同的MVU估计量。具体来说，第一种情形方差估计量需要除以<span
class="math inline">\(N\)</span>来标准化MVU。而第二种除的是<span
class="math inline">\(N-1\)</span>。</p>
<h3 id="均值已知的方差估计">均值已知的方差估计</h3>
<h4 id="参数估计">参数估计</h4>
<p>如果分布的均值真实值已知，那么似然函数只有一个参数<span
class="math inline">\(\sigma^2\)</span>。求最大似然估计量也就是解决：</p>
<p><span class="math display">\[\hat{\sigma^2}_{ML}=\arg\max_{\sigma^2}
P(\vec{x};\sigma^2).\tag{8}\]</span></p>
<p>但是，根据公式(6)的定义，如果计算<span
class="math inline">\(P(\vec{x};\sigma^2)\)</span>涉及到计算函数中指数的偏导。事实上，计算对数似然函数比计算似然函数本身的导数要简单的多。因为对数函数是单调递增函数，其最大值取值位置与原似然函数是一样的。因此我们用下面的式子替换：</p>
<p><span
class="math display">\[\hat{\sigma}^2_{ML}=\arg\max_{\sigma^2}\log(P(\vec{x};\sigma^2)).\tag{9}\]</span></p>
<p>下面，我令<span
class="math inline">\(s=\sigma^2\)</span>简化式子。我们通过计算公式(6)的对数的导数赋值为0来最大化对数似然函数：</p>
<p><span class="math display">\[\begin{aligned}&amp;\frac{\partial
\log(P(\vec{x};\sigma^2))}{\partial
\sigma^2}=0\\&amp;\Leftrightarrow\frac{\partial\log(P(\vec{x};s))}{\partial
s}=0\\&amp;\Leftrightarrow\frac{\partial}{\partial
s}\log\left(\frac{1}{(2\pi
s)^{\frac{N}{2}}}e^{-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2}
\right)=0\\&amp;\Leftrightarrow\frac{\partial}{\partial
s}\log\left(\frac{1}{(2\pi)^{\frac{N}{2}}}\right)+\frac{\partial}{\partial
s}\log\left(\frac{1}{\sqrt{s}^\frac{N}{2}}\right)+\frac{\partial}{\partial
s} \log\left(e^{-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2}\right
)=0\\&amp;\Leftrightarrow0+\frac{\partial}{\partial
s}\log\left((s)^{-\frac{N}{2}}\right)+\frac{\partial}{\partial
s}\left(-\frac{1}{2s}\sum_{i=1}^{N}(x_i-\mu)^2\right)=0\\&amp;\Leftrightarrow
-\frac{N}{2}\log (s)+\frac{1}{2
s^2}\sum_{i=1}^{N}(x_i-\mu)^2=0\\&amp;\Leftrightarrow
-\frac{N}{2s}+\frac{1}{2s^2}\sum_{i=1}^{N}(x_i-\mu)^2=0\\&amp;\Leftrightarrow
\frac{N}{2s^2}\left(-s+\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2\right)=0\\&amp;\Leftrightarrow\frac{N}{2s^2}\left(\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2-s\right)=0\end{aligned}\]</span></p>
<p>很明显，如果<span
class="math inline">\(N&gt;0\)</span>，那么上面等式唯一的解就是：</p>
<p><span
class="math display">\[s=\sigma^2=\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2.\tag{10}\]</span></p>
<p>注意到，实际上<span
class="math inline">\(\hat{\sigma}^2\)</span>的极大似然估计估计量就是传统上一般计算方差的公式。这里标准化因子是<span
class="math inline">\(\frac{1}{N}\)</span>.</p>
<p>但是，极大似然估计并不保证得出的是一个无偏估计量。另外，就算得到的估计量是无偏的，极大似然估计也不能保证估计是最小方差，即MVU。因此，我们需要检查公式(10)的的估计量是否是无偏的。</p>
<h4 id="表现评价">表现评价</h4>
<p>我们需要检查公式(7)的等式是否成立，来确定是否公式(10)中的估计量是无偏的。即判断：</p>
<p><span class="math display">\[E(s)=\hat{s}.\]</span></p>
<p>我们把公式(10)代入到<span
class="math inline">\(E(s)\)</span>，计算：</p>
<p><span class="math display">\[\begin{aligned}E[s] &amp;= E
\left[\frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2 \right] = \frac{1}{N}
\sum_{i=1}^N E \left[(x_i - \mu)^2 \right] = \frac{1}{N} \sum_{i=1}^N E
\left[x_i^2 - 2x_i \mu + \mu^2 \right]\\&amp;= \frac{1}{N} \left( N
E[x_i^2] -2N \mu E[x_i] + N \mu^2 \right)\\&amp;= \frac{1}{N} \left( N
E[x_i^2] -2N \mu^2 + N \mu^2 \right)\\&amp;= \frac{1}{N} \left( N
E[x_i^2] -N \mu^2 \right)\end{aligned}\]</span></p>
<p>另外，真实方差<span class="math inline">\(\hat{s}\)</span>有一个<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variance#Definition">非常重要的性质</a>为<span
class="math inline">\(\hat{s}=E[x_i^2]-E[x_i]^2\)</span>，可变换公式为<span
class="math inline">\(E[x_i^2]=\hat{s}+E[x_i]^2=\hat{s}+\mu^2\)</span>。使用此性质我们可能从上面的公式推出：</p>
<p><span class="math display">\[\begin{aligned}E[s]&amp;=\frac{1}{N}(N
E[x_i^2]-N\mu^2)\\&amp;=\frac{1}{N}(N\hat{s}+N\mu^2-N\mu^2)\\&amp;=\frac{1}{N}(N\hat{s})\\&amp;=\hat{s}\end{aligned}\]</span></p>
<p>满足了公式(7)的条件<span class="math inline">\(E[s]=\hat
s\)</span>，因此，我们得到的数据方差<span class="math inline">\(\hat
s\)</span>的统计量是无偏的。此外，因为极大似然估计的如果是一个无偏的估计量，那么也是最小方差(MVU)，也就是说，我们得到的估计量比任何一个其他的估计量都大。</p>
<p>因此，在分布真实均值已知的情况下，我们不用除以<span
class="math inline">\(N-1\)</span>，而是用除<span
class="math inline">\(N\)</span>计算正态分布的方差。</p>
<h3 id="均值未知的方差估计">均值未知的方差估计</h3>
<h4 id="参数估计-1">参数估计</h4>
<p>上一节，分布的真实均值已知，因此，我们只需要估计数据的方差。但是，如果真实的均值未知，我们均值的估计量就也需要计算了。</p>
<p>此外，方差的估计量需要使用均值的估计量。我们会看到，这时，之前我们得到的方差的估计量就不再无偏了。我们一会儿会通过除以N-1，而不是N来稍微的增加方差估计量的值，从而使方差估计无偏。</p>
<p>与之前一样，基于log似然函数，我们用极大似然估计计算两个估计量。首先我们先计算<span
class="math inline">\(\hat\mu\)</span>的极大似然估计量：</p>
<p><span class="math display">\[\begin{aligned}&amp;\frac{\partial
\log(P(\vec{x}; s, \mu))}{\partial \mu} = 0\\&amp;\Leftrightarrow
\frac{\partial}{\partial \mu} \log \left( \frac{1}{(2 \pi
s)^{\frac{N}{2}}} e^{-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2} \right) =
0\\&amp;\Leftrightarrow \frac{\partial}{\partial \mu} \log \left(
\frac{1}{(2 \pi)^{\frac{N}{2}}} \right) + \frac{\partial}{\partial \mu}
\log \left(e^{-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2} \right) =
0\\&amp;\Leftrightarrow \frac{\partial}{\partial \mu}
\left(-\frac{1}{2s}\sum_{i=1}^N(x_i - \mu)^2 \right) =
0\\&amp;\Leftrightarrow -\frac{1}{2s}\frac{\partial}{\partial \mu}
\left(\sum_{i=1}^N(x_i - \mu)^2 \right) = 0\\&amp;\Leftrightarrow
-\frac{1}{2s} \left(\sum_{i=1}^N -2(x_i - \mu) \right) =
0\\&amp;\Leftrightarrow \frac{1}{s} \left(\sum_{i=1}^N (x_i - \mu)
\right) = 0 \\&amp;\Leftrightarrow \frac{N}{s} \left( \frac{1}{N}
\sum_{i=1}^N (x_i) - \mu \right) = 0 \end{aligned}\]</span></p>
<p>显然，如果<span
class="math inline">\(N&gt;0\)</span>，那么上面的等式只有一种解：</p>
<p><span
class="math display">\[\mu=\frac{1}{N}\sum_{i=1}^{N}x_i.\tag{11}\]</span></p>
<p>注意到，实际的这是计算一个分布均值的著名公式。虽然我们知道这个公式，但我们现在证明了极大似然估计量估计了一个正态分布未知均值的真实值。现在我们先假定我们之前公式(10)计算的方差<span
class="math inline">\(\hat
s\)</span>的估计量仍然是MVU方差估计量。但下一节我们会证明这个估计量已经是有偏的了。</p>
<h4 id="表现评价-1">表现评价</h4>
<p>我们需要通过检查估计量<span
class="math inline">\(\mu\)</span>对真实<span class="math inline">\(\hat
\mu\)</span>的估计是否无偏来确定公式(7)的条件能否成立：</p>
<p><span
class="math display">\[E[\mu]=E\left[\frac{1}{N}\sum_{i=1}^{N}x_i\right]=\frac{1}{N}\sum_{i=1}^N
E[x_i]=\frac{1}{N}N E[x_i]=\frac{1}{N} N \hat\mu=\hat\mu.\]</span></p>
<p>既然<span
class="math inline">\(E[\mu]=\hat\mu\)</span>，那么也就是说我们对分布均值的估计量是无偏的。因为极大似然估计可以保证在估计是无偏的情况下得到的是最小方差估计量，所以我们就已经是证明了<span
class="math inline">\(\mu\)</span>是均值的MVU估计量。</p>
<p>现在我们检查基于经验均值<span
class="math inline">\(\mu\)</span>，而不是真实均值<span
class="math inline">\(\hat\mu\)</span>的方差估计量<span
class="math inline">\(s\)</span>对真实方差<span
class="math inline">\(\hat
s\)</span>的估计身上仍然是无偏的。我们只需要把得到的估计量<span
class="math inline">\(\mu\)</span>带入到之前在公式(10)推导出的公式：</p>
<p><span class="math display">\[\begin{aligned} s &amp;= \sigma^2 =
\frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\\&amp;=\frac{1}{N}\sum_{i=1}^N
\left(x_i - \frac{1}{N} \sum_{i=1}^N (x_i)
\right)^2\\&amp;=\frac{1}{N}\sum_{i=1}^N \left[x_i^2 - 2 x_i \frac{1}{N}
\sum_{i=1}^N (x_i) + \left(\frac{1}{N} \sum_{i=1}^N (x_i) \right)^2
\right]\\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \frac{2\sum_{i=1}^N x_i
\sum_{i=1}^N x_i}{N^2} + \left(\frac{\sum_{i=1}^N x_i}{N}
\right)^2\\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \frac{2\sum_{i=1}^N x_i
\sum_{i=1}^N x_i}{N^2} + \left(\frac{\sum_{i=1}^N x_i}{N}
\right)^2\\&amp;=\frac{\sum_{i=1}^N x_i^2}{N} - \left(\frac{\sum_{i=1}^N
x_i}{N} \right)^2\end{aligned}\]</span></p>
<p>现在我们需要再次检查公式(7)的条件是否成立，来决定估计量是否无偏：</p>
<p><span class="math display">\[\begin{aligned} E[s]&amp;= E \left[
\frac{\sum_{i=1}^N x_i^2}{N} - \left(\frac{\sum_{i=1}^N x_i}{N}
\right)^2 \right ]\\&amp;= \frac{\sum_{i=1}^N E[x_i^2]}{N} -
\frac{E[(\sum_{i=1}^N x_i)^2]}{N^2} \end{aligned}\]</span></p>
<p>记得我们在之前用过方差一个非常重要的性质，真实方差<span
class="math inline">\(\hat s\)</span>可以写成<span
class="math inline">\(\hat s = E[x_i^2]-E[x_i]^2\)</span>，即，<span
class="math inline">\(E[x_i^2]=\hat s + E[x_i]^2=\hat s
+\mu^2\)</span>。利用这个性质我们可以推出：</p>
<p><span class="math display">\[\begin{aligned} E[s] &amp;=
\frac{\sum_{i=1}^N E[x_i^2]}{N} - \frac{E[(\sum_{i=1}^N
x_i)^2]}{N^2}\\&amp;= s + \mu^2 - \frac{E[(\sum_{i=1}^N
x_i)^2]}{N^2}\\&amp;= s + \mu^2 - \frac{E[\sum_{i=1}^N x_i^2 + \sum_i^N
\sum_{j\neq i}^N x_i x_j]}{N^2}\\&amp;= s + \mu^2 - \frac{E[N(s+\mu^2) +
\sum_i^N \sum_{j\neq i}^N x_i x_j]}{N^2}\\&amp;= s + \mu^2 -
\frac{N(s+\mu^2) + \sum_i^N \sum_{j\neq i}^N E[x_i] E[x_j]}{N^2}\\&amp;=
s + \mu^2 - \frac{N(s+\mu^2) + N(N-1)\mu^2}{N^2}\\&amp;= s + \mu^2 -
\frac{N(s+\mu^2) + N^2\mu^2 -N\mu^2}{N^2}\\&amp;= s + \mu^2 -
\frac{s+\mu^2 + N\mu^2 -\mu^2}{N}\\&amp;= s + \mu^2 - \frac{s}{N} -
\frac{\mu^2}{N} - \mu^2 + \frac{\mu^2}{N}\\&amp;= s -
\frac{s}{N}\\&amp;= s \left( 1 - \frac{1}{N} \right)\\&amp;= s
\left(\frac{N-1}{N} \right) \end{aligned}\]</span></p>
<p>显然<span class="math inline">\(E[s]\neq\hat
s\)</span>，上面公式可知分布的方差估计量不再是无偏的了。事实上，平均来看，这个估计量低估了真实方差，比例为<span
class="math inline">\(\frac{N-1}{N}\)</span>。当样本的数量趋于无穷时(<span
class="math inline">\(N\rightarrow\infty\)</span>)，这个偏差趋近于0。但是对于小的样本集，这个偏差就意义了，需要被消除。</p>
<h4 id="修正偏差">修正偏差</h4>
<p>因为偏差不过是一个因子，我们只需通过对公式(10)的估计量乘以偏差的倒数。这样我们就可以定义一个如下的无偏的估计量<span
class="math inline">\(s\prime\)</span>：</p>
<p><span class="math display">\[\begin{aligned} s\prime &amp;= \left (
\frac{N-1}{N} \right )^{-1} s\\s\prime &amp;= \left ( \frac{N-1}{N}
\right )^{-1} \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\\s\prime &amp;=\left
( \frac{N}{N-1} \right ) \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2\\s\prime
&amp;= \frac{1}{N-1}\sum_{i=1}^N(x_i - \mu)^2\end{aligned}\]</span></p>
<p>这个估计量现在就是无偏的了，事实上，这个公式与传统计算方差的公式非常像，不同的是除的是<span
class="math inline">\(N-1\)</span>而不是<span
class="math inline">\(N\)</span>。然而，你可能注意到这个估计量不再是最小方差估计量，但是这个估计量是所有无偏估计量中最小方差的一个。如果我们除以<span
class="math inline">\(N\)</span>，那么估计量就是有偏的了，如果我们除以<span
class="math inline">\(N-1\)</span>，估计量就不是最小方差估计量。但大体来说，一个有偏的估计量要比一个稍高一点方差的估计量要糟糕的多。因此，如果当总体的均值是未知的情况下，方差除的是<span
class="math inline">\(N-1\)</span>，而不是<span
class="math inline">\(N\)</span>。</p>
<h3 id="总结">总结</h3>
<p>本文，我们推导了如果从分布数据中计算常见的方差和均值公式。此外，我们还证明了在方差估计中，标准化因子在总体均值已知时是<span
class="math inline">\(\frac{1}{N}\)</span>，在均值也需要估计时是<span
class="math inline">\(\frac{1}{N-1}\)</span>。</p>
<p><a
target="_blank" rel="noopener" href="http://cmb.oss-cn-qingdao.aliyuncs.com/%E6%A0%B7%E6%9C%AC%E6%96%B9%E5%B7%AE%E4%B8%BA%E4%BB%80%E4%B9%88%E9%99%A4%E4%BB%A5N-1%3F%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89.pdf">本文PDF</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/05/21/decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/21/decision-tree/" class="post-title-link" itemprop="url">Decision Tree (ID3)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-21 15:10:50" itemprop="dateCreated datePublished" datetime="2017-05-21T15:10:50+02:00">2017-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Alogorithm/" itemprop="url" rel="index"><span itemprop="name">Alogorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="preface">Preface</h2>
<p>In April 9th, 2017, incident occurred in United Airlines where crew
of UA beat up a passenger and dragged him out of the plane before which
was about to take off attracted attention all around the world. Many
would gave out doubt: why a company being so rude to passengers can
exist in this world? Actually, UA is going well is just because they
have an extremely precise emergency situation procedure which is
calculate by compute depending on big-data analysis. Computer can help
us make decisions though, it has no emotions, which is effective in most
cases, but can not be approved by our human beings. Let's take a look at
how algorithm make a decision: <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-United%20Airlines.png" />
It is a decision tree, which simply represents the procedure of how UA
algorithm make the decision. First of all, before taking off, four
employees of UA need fly from Chicago to Kentucky. Then the algorithm
check if there is any seats left, if so, passengers were safe for the
moment. But UA3411 was full, the algorithm began assessing the
importance of employees or passengers. Obviously, the algorithm think
crew is more important due to business consideration. Then how to choose
who should be evicted from the plane. The algorithm was more complicated
than the tree I drew, however, Asian or not was one of the criterion.
But why? Because Asian are pushovers. The passenger agreed at first,
however, when he heard that he had to wait for one day, he realized that
he could not treat his patient, then he refused. Then he was beat up and
dragged off the plane.</p>
<p>As you have seen, it is a decision tree, which is similar to human
decision-making process. Decision tree is a simple but powerful
algorithm in machine learning. In fact, you are often using decision
tree theory when making decision, for example <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-07-homework.png" /></p>
<h2 id="introduction">Introduction</h2>
<p>Decision tree is a classification and regression algorithm, we build
a tree through statistics. Today we only talk about how to classify
dataset using Decision Tree. First we will introduce some information
theory background knowledge, then we use iris data build a decision tree
using IDC3 algorithm.</p>
<h2 id="iris-data">Iris data</h2>
<p><a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Iris">Iris
dataset</a> is a very famous dataset deposited on UCI machine learning
repository, which described three kinds of iris. there are four columns
corresponding for features as followed： * sepal length in cm * sepal
width in cm * petal length in cm * petal width in cm</p>
<p>The last column represents iris categories:</p>
<ul>
<li>Iris-setosa (n=50)</li>
<li>Iris-versicolor (n=50)</li>
<li>Iris-virginica (n=50)</li>
</ul>
<p>Here, our task is to use the dataset to train a model and generate a
decision tree. During the process we need calculate some statistics
values to decide how to generate a better one.</p>
<p>The dataset is very small so that you can easily download it and take
a look.</p>
<h2 id="entropy-and-information-gain">Entropy and Information Gain</h2>
<h4 id="entropy">Entropy</h4>
<p>Before Decision Tree, I'd like to talk about some concept in
Information Theory. Entropy is a concept from thermodynamics at first,
C.E.Shannon introduced which into information theory which represent
redundancy in 1948. It sounds a very strange concept. In fact, it is
very easy to understand. For example, during the knockout stages in
world Cup Games, there are 16 teams. Now I let you guess which team will
win the champion which assume I know the answer, how many times do you
need to get the outcome? First of all, you cut 16 teams to 8-8 parts,
you asked me if the team in first 8 teams or the other. I told you that
the team was in the other 8 teams. Then you cut the the 8 teams again,
you ask me if the team is in the first 4 teams or the other, I told you
that the champion would be in the first 4 teams, and so forth and so on.
And how many times is the entropy of who wining the champion.</p>
<p><span class="math display">\[ Entropy(champion) = {\rm log}_2^{16}=4
\]</span></p>
<p>That is, we can use 4 bits to represents which team will win the
game. Clever you may ask why we divide team to two parts other than
three or four parts. That is because we use binary represents the world
in computer world. $ 2^4=16 $ means we can use 4 bit represents 16
conditions. We can use entropy represent all information in this world.
And if you have known that which team will win the campion, the entropy
is 0, because, you do not need any more information to deduce the
outcome.</p>
<p>Entropy represents uncertainty indeed. Ancient China, we have to
record history on bamboo slips, which demanded us decrease words. That
means entropy of every single ancient Chinese character is higher than
words we are saying today. That is, if we lost just some of these words,
we would lose lots of stories. There are many songs starts with:"Yoo,
yoo, check now", which barely offer us information, which means we can
drop those words and interpret the these songs precisely as well. The
entropy of these sentence is low.</p>
<p>Assume <span class="math inline">\(X\)</span> is discrete random
variable, the distribution is: <span
class="math display">\[P(X=x_i)=p_i\]</span> then the entropy of X is:
<span class="math display">\[H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2
p_i\]</span> where if p_0=0, we define 0log0 = 0.</p>
<p>It seems that the equation has nothing to do with the entropy we have
calculated in the champion example. Now let's calculate the example.
First of all <span class="math inline">\(X\)</span> represents the
probability of each team which would win the game. we assume all teams
were at the same level, so we have <span
class="math display">\[p(X=x_1)=p(X=x_2)=p(X=x_3)=\cdots =
p(X=x_{16})=\frac{1}{16}\]</span> the entropy is <span
class="math display">\[H(X)=-\sum_{i=1}^{16}\frac{1}{16}{\rm log}_2
\frac{1}{16}=-16\times\frac{1}{16}\times {\rm log}_2
{2^{-4}}=4\]</span></p>
<p>Bingo, the the answer is same. In fact, if we know some more
information, the entropy is lower than 4. for example, the probability
of Germany is higher than some Asian teams. #### Entropy and Iris Data
Now we calculate entropy of Iris Data which will be used to fit a
decision tree in following sections. We concern about the
categories(setosa, versicolor and virginica). Remember the equation of
how to calculate entropy: <span
class="math display">\[H(X)=-\sum_{i=1}^{n}p_i {\rm log}_2
p_i\]</span></p>
<p>Three kinds of flowers are all 50s, so the probability of each
category is the same: <span
class="math display">\[p_1=p_2=p_3=\frac{50}{50+50+50}=\frac{1}{3}\]</span>
Then, the entropy is pretty easy to calculate <span
class="math display">\[H(X)=-1\times (\frac{1}{3}{\rm
log}_2\frac{1}{3}+\frac{1}{3}{\rm log}_2\frac{1}{3}+\frac{1}{3}{\rm
log}_2\frac{1}{3})=1.5850\]</span> #### Conditional Entropy The meaning
of Conditional Entropy is as its name. With respect with random
variable<span class="math inline">\((X, Y)\)</span>, the joint
distribution is <span class="math display">\[P(X=x_i, Y=y_j)=p_{ij},
i=1,2,3\cdots m; j=1,2,3,\cdots n\]</span> Conditional Entropy H(Y|X)
represents that given we have known random variable <span
class="math inline">\(X\)</span> , the disorder or uncertainty of <span
class="math inline">\(Y\)</span>. The definition is as followed: <span
class="math display">\[H(Y|X)=\sum_{i=1}^m p_i H(Y|X=x_i)\]</span> Here,
<span class="math inline">\(p_i=P(X=x_i)\)</span>.</p>
<h4 id="conditional-entropy-and-iris-data">Conditional Entropy and Iris
Data</h4>
<p>We calculate some Conditional Entropy as examples. First of all, I
random choose 15 columns of sepal length with respect to their
categories. the result is as followed：</p>
<table>
<thead>
<tr class="header">
<th>No.</th>
<th>sepal length in cm</th>
<th>categories</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>2</td>
<td>7.20</td>
<td>Iris-virginica</td>
</tr>
<tr class="odd">
<td>3</td>
<td>5.00</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>4</td>
<td>5.00</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>5</td>
<td>5.90</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>6</td>
<td>5.70</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>7</td>
<td>5.20</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.50</td>
<td>Iris-versicolor</td>
</tr>
<tr class="odd">
<td>9</td>
<td>4.80</td>
<td>Iris-setosa</td>
</tr>
<tr class="even">
<td>10</td>
<td>4.60</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>11</td>
<td>6.50</td>
<td>Iris-versicolor</td>
</tr>
<tr class="even">
<td>12</td>
<td>5.20</td>
<td>Iris-setosa</td>
</tr>
<tr class="odd">
<td>13</td>
<td>7.70</td>
<td>Iris-virginica</td>
</tr>
<tr class="even">
<td>14</td>
<td>6.40</td>
<td>Iris-virginica</td>
</tr>
<tr class="odd">
<td>15</td>
<td>6.00</td>
<td>Iris-versicolor</td>
</tr>
</tbody>
</table>
<p>The octave code is <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%% octave</span><br><span class="line">[a,b,c,d, cate] = textread(&quot;iris.data&quot;, &quot;%f%f%f%f%s&quot;,&quot;delimiter&quot;, &quot;,&quot;);</span><br><span class="line"></span><br><span class="line">for i=1:15</span><br><span class="line">  x = floor(rand()*150);</span><br><span class="line">  fprintf(&#x27;%f %s\n&#x27;, a(x), cate&#123;x&#125; );</span><br><span class="line">end</span><br></pre></td></tr></table></figure> We just take this 15 items for
examples, I assume that we divide sepal length into two parts: greater
than mean and less than mean. The mean is <span
class="math display">\[mean = (5.90+7.2+\cdots+6.00)/15 =
5.7733\]</span> There are 8 elements less then 5.7733 and 7 bigger ones.
That is</p>
<table>
<thead>
<tr class="header">
<th>mean</th>
<th>idx of greater than mean</th>
<th>idx of less than mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>5.7733</td>
<td>1,2,5,11,13,14,15</td>
<td>3,4,6,7,8,9,10,12</td>
</tr>
</tbody>
</table>
<p>We let <span
class="math inline">\(x_1=greater\)</span>(1,2,5,11,13,14,15), <span
class="math inline">\(x_2=less\)</span>(3,4,6,7,8,9,10,12) then <span
class="math display">\[H(Y|X=x_1)=-(p_1 {\rm log}_2 p_1 + p_2 {\rm
log}_2 p_2 + p_3 {\rm log}_2 p_3)=\frac{4}{7}{\rm
log}_2\frac{4}{7}+\frac{3}{7}{\rm log}_2\frac{3}{7}+0{\rm log}_2
0=0.98523\]</span> <span class="math display">\[H(Y|X=x_2)=-(p_1 {\rm
log}_2 p_1 + p_2 {\rm log}_2 p_2+p_3 {\rm log}_2 p_3)=\frac{3}{8}{\rm
log}_2\frac{3}{8}+0{\rm log}_2 0+\frac{5}{8}{\rm
log}_2\frac{5}{8}=0.95443\]</span></p>
<p>The Conditional Entropy then is <span
class="math display">\[H(Y|X)=\sum_{i=1}^{2}p_i
H(Y|x_i)=\frac{7}{15}\times 0.98523+\frac{8}{15}\times
0.95443=0.96880\]</span> #### Information Gain Just as its name implies,
Information Gain means the information we have gained after adding some
features. That is, we can vanish some uncertainty when we add some
information. For example, I want you to guess an NBA player, the
uncertainty is very high, however, there are only several persons in the
list if I tell you that he is a Chinese. You gained information after
knowing the Chinese feature to decrease the uncertainty. The calculation
of Information Gain is <span class="math display">\[IG(Y, X)=
H(Y)-H(Y|X)\]</span> Here, we want to decide <span
class="math inline">\(Y\)</span> with feature <span
class="math inline">\(X\)</span>. It is easy, just Entropy of <span
class="math inline">\(Y\)</span> minus Conditional Entropy <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(X\)</span>. The meaning is obvious too: <span
class="math inline">\(H(Y)\)</span> represents uncertainty, <span
class="math inline">\(H(Y|X)\)</span> represents uncertainty of <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(X\)</span>, the difference is the Information
Gain. #### Information Gain and Iris Data In this section, I will apply
Information Gain equations to the whole Iris data. First of all, let
<span class="math inline">\(Y\)</span> represent categories of iris, and
<span class="math inline">\(X_1,X_2,X_3, X_4\)</span> represent sepal
length, sepal width, petal length petal width respectively.</p>
<p>We have computed that <span
class="math inline">\(H(Y)=1.0986\)</span>, next, we will calculate 4
Conditional Entropy <span
class="math inline">\(H(Y|X_1),H(Y|X_2),H(Y|X_3),H(Y|X_4)\)</span>. In
light of continuousness of <span class="math inline">\(X\)</span>, we
divide them by mean of each feature. Then <span
class="math display">\[\overline{X_1}=5.8433,\,\overline{X_2}=3.0540,\,\overline{X_3}=3.7587,\,\overline{X_4}=1.1987\]</span></p>
<p><span class="math display">\[H(Y|X_1)=-\sum_{i=1}^3 p_i
H(Y|X_{1i})=-(\frac{70}{150}(\frac{0}{70}{\rm
log}_2\frac{0}{70}+\frac{26}{70}{\rm log}_2\frac{26}{70}
+\frac{44}{70}{\rm log}_2\frac{44}{70})+\frac{80}{150}(\frac{50}{80}{\rm
log}_2\frac{50}{80}+\frac{24}{80}{\rm
log}_2\frac{24}{80}+\frac{6}{80}{\rm
log}_2\frac{6}{80}))=1.09757\]</span></p>
<p><span class="math display">\[H(Y|X_2)=-\sum_{i=1}^3 p_i
H(Y|X_{2i})=-(\frac{67}{150}(\frac{42}{67}{\rm
log}_2\frac{42}{67}+\frac{8}{67}{\rm
log}_2\frac{8}{67}+\frac{17}{67}{\rm
log}_2\frac{17}{67}+\frac{83}{150}(\frac{8}{83}{\rm
log}_2\frac{8}{83}+\frac{42}{83}{\rm
log}_2\frac{42}{83}+\frac{33}{83}{\rm
log}_2\frac{33}{83}))=1.32433\]</span></p>
<p><span class="math display">\[H(Y|X_3)=-\sum_{i=1}^3 p_i
H(Y|X_{3i})=-(\frac{93}{150}(\frac{0}{93}{\rm
log}_2\frac{0}{93}+\frac{43}{93}{\rm
log}_2\frac{43}{93}+\frac{50}{93}{\rm
log}_2\frac{50}{93}+\frac{57}{150}(\frac{50}{57}{\rm
log}_2\frac{50}{57}+\frac{7}{57}{\rm log}_2\frac{7}{57}+\frac{0}{57}{\rm
log}_2\frac{0}{57}))=0.821667\]</span></p>
<p><span class="math display">\[H(Y|X_4)=-\sum_{i=1}^3 p_i
H(Y|X_{4i})=-(\frac{90}{150}(\frac{0}{90}{\rm
log}_2\frac{0}{90}+\frac{40}{90}{\rm
log}_2\frac{40}{90}+\frac{50}{90}{\rm
log}_2\frac{50}{90}+\frac{60}{150}(\frac{50}{60}{\rm
log}_2\frac{50}{60}+\frac{10}{60}{\rm
log}_2\frac{10}{60}+\frac{0}{60}{\rm log}_2\frac{0}{60}))=0.854655
\]</span> Information Gains is easy to get <span
class="math display">\[IG(Y,
X_1)=H(Y)-H(Y|X_1)=1.5850-1.09757=0.487427\]</span></p>
<p><span class="math display">\[IG(Y,
X_2)=H(Y)-H(Y|X_2)=1.5850-1.32433=0.260669\]</span></p>
<p><span class="math display">\[IG(Y,
X_3)=H(Y)-H(Y|X_3)=1.5850-0.821667=0.763333\]</span></p>
<p><span class="math display">\[IG(Y,
X_4)=H(Y)-H(Y|X_4)=1.5850-0.854655=0.730345\]</span> By now, we find
that <span class="math inline">\(IG(Y, X_3)\)</span> is bigger than
others, which means feature <span class="math inline">\(X_3\)</span>
supplies more information.</p>
<h2 id="id3iterative-dichotomiser-3">ID3(Iterative Dichotomiser 3)</h2>
<p>ID3 algorithm was developed by Ross Quinlan in 1986, which is a very
classic algorithm as well as C4.5 and CART. We First apply Information
Gain of each feature with respect to iris data. Then to choose the
maximum to divide data into 2 parts. For each part we apply Information
Gain recursively until we put all parents data to one node. Now that we
have know Information Gain from the last section, obviously we choose X3
as the feature dividing data into 2 parts in the first place.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-17-2.png" /></p>
<p>Let's take a look at the first cut using feature <span
class="math inline">\(X_3\)</span>. We have 150 items at first, after
comparing if <span class="math inline">\(X_3&gt;3.7587\)</span>, we
divide data into two parts, one has 93 items, the other got 57. From the
data, we know that there is no setosa in node B, meanwhile, no virginica
in node C, which means that this feature is very good for split data due
to exclude setosa and virginica.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Node B</th>
<th>Node C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>setosa</td>
<td>0</td>
<td>50</td>
</tr>
<tr class="even">
<td>versicolor</td>
<td>43</td>
<td>7</td>
</tr>
<tr class="odd">
<td>virginica</td>
<td>50</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The end condition of the algorithm is decided by IG. When IG is less
then some threshold or if there is only one category left, we can end
the algorithm. If IG less than some value(e.g. 0.01) and more than one
category left simultaneously, we have to choose a final category to be
the leaf, the rule is to set the category having samples more than the
others.</p>
<p>Take Node H for example, we set IG threshold to 0.01 in the first
place. Then we calculate the Information Gain for each feature, the
biggest IG from feature 2(sepal width in cm), which is 0.003204 and less
than 0.01. So we have to set H as a leaf. There are 0 Iris-setosa, 25
Iris-versicolor and 44 Iris-virginica in the leaf, so we set the bigger
one(i.e. Iris-virginica) to the leaf.</p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-05-19-2.png" /></p>
<h2 id="summary">Summary</h2>
<p>Today we have talked about what is decision tree algorithm. Firstly,
I introduce three background concept Entropy, Conditional Entropy and
Information Gain. Next we apply ID3 algorithm to Iris data to build a
decision.</p>
<p>One of the most significant advantages of decision tree is that we
can explain the result. If the algorithm decided UA should beat the
their passengers, they could trace the tree to find the path of reason
chain. It is very useful to tell consumers why we recommend them
something, under such circumstance, we can use decision tree to train a
model.</p>
<p>There is a shortcoming that Information Gain tends to use feature
with more values. In order to resolve the problem, Ross Quinlan improved
the algorithm through Information Gain Rate Rather than IG. <a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Leo_Breiman">Breiman</a> introduced
CART algorithm subsequently, which can be applied to classification as
well as regression. Recently, Scientists have developed more powerful
algorithm such as Random Forest and Gradient Boosting Decision Tree
etc.</p>
<h2 id="reference">Reference</h2>
<ol type="1">
<li>《统计学习方法》，李航</li>
<li>《数学之美》，吴军</li>
<li>http://www.shogun-toolbox.org/static/notebook/current/DecisionTrees.html</li>
<li>https://en.wikipedia.org/</li>
</ol>
<h2 id="appendix-code">Appendix code</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">%% octave main function file</span><br><span class="line">%% iris data dowload link: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</span><br><span class="line">[a,b,c,d, cate] = textread(&quot;iris.data&quot;, &quot;%f%f%f%f%s&quot;,&quot;delimiter&quot;, &quot;,&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%for i=1:15</span><br><span class="line">%	x = floor(rand()*150);</span><br><span class="line">%	fprintf(&#x27;%f %s\n&#x27;, a(x), cate&#123;x&#125; );</span><br><span class="line">%end;</span><br><span class="line"></span><br><span class="line">features = [a, b, c, d];</span><br><span class="line">for i=1:length(features(1, :))</span><br><span class="line">	col = features(:, i);</span><br><span class="line">	me = mean(col);</span><br><span class="line">	disp(me);</span><br><span class="line">	feat(i).greater = find(col &gt; me);</span><br><span class="line">	feat(i).less = find(col &lt;= me);</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">total = (1:150)&#x27;;</span><br><span class="line">decision(feat, length(features(1, :)), cate, total);</span><br><span class="line">fprintf(&#x27;\n&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">%% octave: decsion tree file</span><br><span class="line">function decision(feat, feat_size, cate, total)</span><br><span class="line">	if length(total) == 0</span><br><span class="line">		return</span><br><span class="line">	end</span><br><span class="line">	fprintf(&#x27;(-%d-)&#x27;, length(total));</span><br><span class="line">	%plogp = @(x)[x*log2(x)];</span><br><span class="line">	function e = plogp(pi)</span><br><span class="line">		if pi == 0</span><br><span class="line">			e = 0;</span><br><span class="line">		else</span><br><span class="line">			e = pi*log2(pi);</span><br><span class="line">		end</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	function d = div(a, b)</span><br><span class="line">		if b == 0</span><br><span class="line">			d = 0;</span><br><span class="line">		else</span><br><span class="line">			d = a/b;</span><br><span class="line">		end</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	debug = 0;</span><br><span class="line"></span><br><span class="line">	function m = maxc(cate, cates, total)</span><br><span class="line">		maxidx = 1;</span><br><span class="line">		max_c = 0;</span><br><span class="line">		for i=1:length(cates)</span><br><span class="line">			c =find(strcmp(cate, cates&#123;i&#125;));</span><br><span class="line">			cl = length(intersect(c, total));</span><br><span class="line">			if debug == 1 fprintf(&#x27;\n%d##%d  %s###&#x27;,i, cl, char(cates&#123;i&#125;)) end</span><br><span class="line">			%if (debug == 1 &amp;&amp; cl &lt;10 &amp;&amp; cl &gt;0) disp(intersect(c, total)&#x27;) end</span><br><span class="line">			if cl &gt; max_c</span><br><span class="line">				max_c = cl;</span><br><span class="line">				maxidx = i;</span><br><span class="line">			end</span><br><span class="line">		end</span><br><span class="line">		if debug == 1 fprintf(&#x27;\n****%d    %d******\n&#x27;, maxidx, max_c) end</span><br><span class="line">		%m = cates(maxidx);</span><br><span class="line">		m = maxidx;</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	% compute h(y)</span><br><span class="line">	cates = unique(cate);</span><br><span class="line">	hx = 0;</span><br><span class="line">	for i = 1:length(cates)</span><br><span class="line">		c = find(strcmp(cate, cates&#123;i&#125;));</span><br><span class="line">		rc = intersect(c, total);</span><br><span class="line">		hx -= plogp(length(rc)/length(total));</span><br><span class="line">	end</span><br><span class="line">	%fprintf(&#x27;hx = %f\n&#x27;, hx)			</span><br><span class="line">	% compute h(y|x)</span><br><span class="line">	max_feature = 1;</span><br><span class="line">	max_ig = 0;</span><br><span class="line"></span><br><span class="line">	max_left = intersect(feat(1).greater, total);</span><br><span class="line">	max_right = intersect(feat(1).less, total);</span><br><span class="line">	for i=1:feat_size</span><br><span class="line">		hxh = 0;</span><br><span class="line">		hxl = 0;</span><br><span class="line">		feat_greater = intersect(feat(i).greater, total);</span><br><span class="line">		feat_less = intersect(feat(i).less, total);</span><br><span class="line">		ge = length(feat_greater);</span><br><span class="line">		le = length(feat_less);</span><br><span class="line"></span><br><span class="line">		if (ge+le) == 0</span><br><span class="line">			continue</span><br><span class="line">		end</span><br><span class="line"></span><br><span class="line">		for j = 1:length(cates);</span><br><span class="line">			c = find(strcmp(cate, cates&#123;j&#125;));</span><br><span class="line">			xh = length(intersect(feat_greater, c));</span><br><span class="line">			xl = length(intersect(feat_less, c));</span><br><span class="line">			hxh -= plogp(div(xh, ge));</span><br><span class="line">			hxl -= plogp(div(xl, le));</span><br><span class="line">		end</span><br><span class="line">		% compute hx - h(y|x)</span><br><span class="line">		hxy = (ge/(ge+le))*hxh + ((le)/(ge+le))*hxl;</span><br><span class="line">		ig = hx - hxy;</span><br><span class="line"></span><br><span class="line">		if ig &gt; max_ig</span><br><span class="line">			max_ig = ig;</span><br><span class="line">			max_feature = i;</span><br><span class="line">			max_left= feat_less;</span><br><span class="line">			max_right = feat_greater;</span><br><span class="line">		end</span><br><span class="line"></span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">	left = max_left;</span><br><span class="line">	right = max_right;</span><br><span class="line">	%fprintf(&#x27;feature:ig  %d %f %d %d ------ \n&#x27;, max_feature, max_ig, length(left), length(right));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	if debug == 1 printf(&quot;\033[0;32;1m-ig--%f \033[0m&quot;,  max_ig); end</span><br><span class="line">	if(max_ig &lt; 0.01)</span><br><span class="line">		%fprintf(&#x27;&lt;%s&gt;&#x27;, char(maxc(cate, cates, total)))</span><br><span class="line">		printf(&quot;\033[0;31;1m&lt;%d&gt;\033[0m&quot;,  maxc(cate, cates, total));</span><br><span class="line">		return</span><br><span class="line">	end</span><br><span class="line">	fprintf(&quot;\033[0;34;1m#%d \033[0m&quot;,  max_feature);</span><br><span class="line">	fprintf(&#x27;&#123;&#x27; )</span><br><span class="line">	decision(feat, feat_size, cate, left);</span><br><span class="line">	decision(feat, feat_size, cate, right);</span><br><span class="line">	fprintf(&#x27;&#125;&#x27;)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://chengmingbo.github.io/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingbo Cheng">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingbo">
      <meta itemprop="description" content="Mingbo">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Mingbo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/05/01/A-Tutorial-To-Singular-Value-Decomposition/" class="post-title-link" itemprop="url">A Tutorial on Singular Value Decomposition</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-05-01 21:49:00" itemprop="dateCreated datePublished" datetime="2017-05-01T21:49:00+02:00">2017-05-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Alogorithm/" itemprop="url" rel="index"><span itemprop="name">Alogorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="preface">Preface</h3>
<p>Under some circumstance, we want to compress data to save storage
space. For example, when iPhone7 was released, many were trapped in a
dilemma: Should I buy a 32G iPhone without enough free space or that of
128G with a lot of storage being wasted? I had been trapped in such
dilemma indeed. I still remember that I only had 8G storage totally when
I was using my first Android phone. What annoyed me most was my
thousands of photos. Well, I confess that I was being always a mad
picture taker. I knew that there were some technique which could
compress a picture through reducing pixel. However, it is not enough,
because, as you know, in some arbitrary position in a picture, we can
tell that the picture share the same color. An extreme Example: if we
have a pure color picture, what we just need know is the RGB value and
the size, then reproducing the picture is done without extra effort.
What I was dreaming is done perfectly by Singular Value
Decomposition(SVD).</p>
<h3 id="introduction">Introduction</h3>
<p>Before SVD, in this article, I will introduce some mathmatical
concepts in the first place which cover Linear transformation and
EigenVector&amp;EigenValue. This Background knowledge is meant to make
SVD straightforward. You can skip if you are familar with this
knowledge.</p>
<h3 id="linear-transformation">Linear transformation</h3>
<p>Given a matrice <span class="math inline">\(A\)</span> and vector
<span class="math inline">\(\vec{x}\)</span>, we want to compute the
mulplication of <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span></p>
<p><span
class="math display">\[\vec{x}=\begin{pmatrix}1\\3\end{pmatrix}\qquad
A=\begin{pmatrix}2 &amp; 1 \\\\ -1 &amp; 1
\end{pmatrix}\qquad\vec{y}=A\vec{x}\]</span></p>
<p>But when we do this multiplication, what happens? Acutually, when we
multiply <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span>, we are changing the coordinate
axes of the vector <span class="math inline">\(x\)</span> to another new
axes. Begin with a simpler example, we let</p>
<p><span class="math display">\[A=\begin{pmatrix}1 &amp; 0\\\\ 0
&amp;1\end{pmatrix}\]</span></p>
<p>then we have <span class="math display">\[A\vec{x}=\begin{pmatrix}1
&amp; 0\\\\ 0
&amp;1\end{pmatrix}\begin{pmatrix}1\\3\end{pmatrix}=\begin{pmatrix}1\\3\end{pmatrix}\]</span></p>
<p>You may have noticed that we can always get the same <span
class="math inline">\(\vec{x}\)</span> after left multiply by A. In this
case, we use coordinate axes <span
class="math inline">\(i=\begin{pmatrix}1 \\\\ 0\end{pmatrix}\)</span>
and <span class="math inline">\(j=\begin{pmatrix}0 \\\\
1\end{pmatrix}\)</span> as the figure below demonstrated. That is, if we
want to represent <span
class="math inline">\(\begin{pmatrix}1\\3\end{pmatrix}\)</span> under
the coordination, we can calculate the transformation as followed:</p>
<p><span class="math display">\[\begin{align} A\vec{x}=1\cdot i + 3\cdot
j = 1\cdot \begin{pmatrix}1 \\\\ 0\end{pmatrix} + 3\cdot
\begin{pmatrix}0 \\\\
1\end{pmatrix}=\begin{pmatrix}1\\3\end{pmatrix}\end{align}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-073041.jpg" /></p>
<p>As we know, we can put a vector to anywhere in space, and if we want
to calculate sum of two vectors, the simplest way is to connect the to
vector from one's head to the other's tail. Our example, we compute
<span class="math inline">\(A\vec{x}\)</span> means add two vector(green
imaginary lines) up. And the answer is still <span
class="math inline">\(\begin{pmatrix}1\\3\end{pmatrix}\)</span>.</p>
<p>Now we change <span class="math inline">\(i=\begin{pmatrix}2\\\\
-1\end{pmatrix}\)</span> and <span
class="math inline">\(j=\begin{pmatrix}1\\1\end{pmatrix}\)</span> as the
coordinate axes(the red vectors), which means <span
class="math inline">\(A=\begin{pmatrix}2 &amp; 1 \\\\ -1 &amp;
1\end{pmatrix}\)</span>. I put vectors(black ones) to this figure as
well. We can see what happens when we change a new coordinate axes.</p>
<p>First of all, we multiply <span class="math inline">\(j\)</span> by
<span class="math inline">\(3\)</span> and <span
class="math inline">\(i\)</span> by 1. Then we move vector j and let the
head of <span class="math inline">\(i\)</span> connect the tail of <span
class="math inline">\(3\cdot j\)</span>. We can now find what is the
coordination of <span class="math inline">\(1\cdot i+3\cdot
j\)</span>(the blue one). We now verify the result using mutiplication
of <span class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span>:</p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}2 &amp; 1 \\\\
-1 &amp; 1\end{pmatrix}\begin{pmatrix}1\\3\end{pmatrix}=1\cdot
\begin{pmatrix}2 \\\\ -1\end{pmatrix} + 3\cdot  \begin{pmatrix}1 \\\\
1\end{pmatrix}=\begin{pmatrix}5\\2\end{pmatrix}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-013710.jpg" />
Here, you can imagine that matrice <span
class="math inline">\(A\)</span> is just like a function <span
class="math inline">\(f(x)\rightarrow y\)</span>, when you subsitute
<span class="math inline">\(x\)</span>, we get the exact <span
class="math inline">\(y\)</span> using the principle <span
class="math inline">\(f(x)\rightarrow y\)</span>. In fact, the
multiplication is tranform the vector from one coordination to
another.</p>
<h4 id="exercise">Exercise</h4>
<ol type="1">
<li><span class="math inline">\(A=\begin{pmatrix}1 &amp; 2 \\\\ 3 &amp;
4\end{pmatrix}\)</span>, draw the picture to stretch and rotate <span
class="math inline">\(x=\begin{pmatrix}1\\3\end{pmatrix}\)</span>.</li>
<li>Find a <span class="math inline">\(A\)</span> matrix to rotate <span
class="math inline">\(\vec{x}=\begin{pmatrix}1\\3\end{pmatrix}\)</span>
to <span class="math inline">\(90^{\circ}\)</span> and <span
class="math inline">\(180^{\circ}\)</span>.</li>
<li>what if <span class="math inline">\(A=\begin{pmatrix}1 &amp; 2 \\\\
2 &amp; 4\end{pmatrix}\)</span>.</li>
</ol>
<h3 id="eigenvector-and-eigenvalue">EigenVector and EigenValue</h3>
<p>EigenVector and EigenValue is an extremely important concept in
linear algebra, and is commonly used everywhere including SVD we are
talking today. However, many do not know how to interpret it. In fact,
EigenVector and EigenValue is very easy as long as we know about what is
linear transformation.</p>
<h4 id="a-problem">A Problem</h4>
<p>Before start, let's take a look at a question: if we want to multiply
matrices for 1000 times, how to calculate effectively? <span
class="math display">\[AAA\cdots A= \begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp; 2\end{pmatrix}\cdots
\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\]</span></p>
<h4 id="intuition">Intuition</h4>
<p>Last section, we have talked that if we multiply a vector by a matrix
<span class="math inline">\(A\)</span>, means that we use <span
class="math inline">\(A\)</span> to stretch and rotate the vector in
order to represent the vector in a new coordinate axes. However, there
are some vectors for <span class="math inline">\(A\)</span>, they can
only be stretched but can not be rotated. Assume <span
class="math inline">\(A=\begin{pmatrix}3 &amp; 1 \\\\ 0 &amp;
2\end{pmatrix}\)</span>, let <span
class="math inline">\(\vec{x}=\begin{pmatrix}1 \\\\
-1\end{pmatrix}\)</span>. When we multiply <span
class="math inline">\(A\)</span> and <span
class="math inline">\(\vec{x}\)</span></p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}3 &amp; 1 \\\\ 0
&amp; 2\end{pmatrix}\begin{pmatrix}1 \\\\
-1\end{pmatrix}=\begin{pmatrix}2 \\\\ -2\end{pmatrix}=2\cdot
\begin{pmatrix}1 \\\\ -1\end{pmatrix}\]</span></p>
<p>It turns out we can choose any vector along <span
class="math inline">\(\vec{x}\)</span>, the outcome is the same, for
example:</p>
<p><span class="math display">\[A\vec{x}=\begin{pmatrix}3 &amp; 1 \\\\ 0
&amp; 2\end{pmatrix}\begin{pmatrix}-3 \\\\
3\end{pmatrix}=\begin{pmatrix}-6 \\\\ -6\end{pmatrix}=2\cdot
\begin{pmatrix}-3 \\\\ 3\end{pmatrix}\]</span></p>
<p><img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-25-052135.jpg" /></p>
<p>We name vectors like <span class="math inline">\(\begin{pmatrix}-3
\\\\ 3\end{pmatrix}\)</span> and <span
class="math inline">\(\begin{pmatrix}1 \\\\ -1\end{pmatrix}\)</span>
<strong>EigenVectors</strong> and 2 the conresponse
<strong>EigenValues</strong>. In practice, we usually choose unit
eigenvectors(length equals to 1) given that there are innumerable
EigenVectors along the line.</p>
<p>I won't cover how to compute these vectors and vaules and just list
the answer as followed</p>
<p><span class="math display">\[\begin{align}&amp;\begin{pmatrix}3 &amp;
1 \\0&amp; 2\end{pmatrix}
\begin{pmatrix}{-1}/{\sqrt(2)} \\\\ {1}/{\sqrt(2)}\end{pmatrix}=
2\begin{pmatrix}{-1}/{\sqrt(2)} \\\\ {1}/{\sqrt(2)}\end{pmatrix}
\qquad\qquad\vec{x_1}=\begin{pmatrix}{-1}/{\sqrt(2)} \\\\
{1}/{\sqrt(2)}\end{pmatrix} &amp;\lambda_1=2\\
&amp;\begin{pmatrix}3 &amp; 1 \\0&amp; 2\end{pmatrix}
\begin{pmatrix}1 \\\\ 0\end{pmatrix}\qquad=\qquad
3\begin{pmatrix}1 \\\\
0\end{pmatrix}\qquad\qquad\quad\,\,\,\vec{x_2}=\begin{pmatrix}1 \\\\
0\end{pmatrix}
&amp;\lambda_2=3
\end{align}\]</span> Notice that <span
class="math inline">\(|\vec{x_1}|=1\)</span> and <span
class="math inline">\(|\vec{x_2}|=1\)</span> #### EigenValue
Decomposition If we put two EigenVectors and corresponding EigenValues
together, we can get the following equation: <span
class="math display">\[AQ=\begin{pmatrix}3 &amp; 1 \\0&amp;
2\end{pmatrix}
\begin{pmatrix}
{-1}/{\sqrt(2)}&amp;1\\
{1}/{\sqrt(2)}&amp;0
\end{pmatrix}=
\begin{pmatrix}
{-1}/{\sqrt(2)}&amp;1 \\\\ {1}/{\sqrt(2)}&amp;0
\end{pmatrix}
\begin{pmatrix}
2 &amp; 0\\
0 &amp; 3
\end{pmatrix}=Q\Lambda
\]</span> Then we have <span class="math inline">\(AQ=Q\Lambda\)</span>,
the conclusion is still right if we introduce more dimensions, that is
<span class="math display">\[\begin{align}
A\vec{x_1}=\lambda\vec{x_1}\\
A\vec{x_2}=\lambda\vec{x_2}\\
\vdots\qquad\\
A\vec{x_k}=\lambda\vec{x_k}
\end{align}\]</span></p>
<p><span class="math display">\[Q=
\begin{pmatrix}
    x_{11}&amp; x_{21} &amp;\cdots x_{k1}&amp;\\
    x_{12}&amp; x_{22} &amp;\cdots x_{k2}&amp;\\
    &amp;\vdots&amp;&amp;\\
    x_{1m}&amp; x_{22} &amp;\cdots x_{km}&amp;
\end{pmatrix}
\qquad\Lambda=
\begin{pmatrix}
\lambda_1 &amp; 0 &amp; \cdots&amp;0\\
0 &amp;\lambda_2&amp;\cdots&amp;0\\
\vdots&amp;\vdots&amp;\ddots\\
0&amp;\cdots&amp;\cdots&amp;\lambda_k
\end{pmatrix}\]</span></p>
<p>If we do something on the equation <span
class="math inline">\(AQ=Q\Lambda\)</span>, then we have: <span
class="math display">\[AQQ^{-1}=A=Q\Lambda Q^{-1}\]</span> It is
EigenVaule Decomposition. #### Resolution Now, Let's look at the
question in the beginning of this section <span
class="math display">\[\begin{align}
AAA\cdots A&amp;= \begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\begin{pmatrix}3&amp; 1 \\0 &amp; 2\end{pmatrix}\cdots
\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}=\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}\\
AAA\cdots A &amp;= Q\Lambda Q^{-1}Q\Lambda Q^{-1}Q\Lambda Q^{-1}\cdots
Q\Lambda Q^{-1}=Q\prod_{i=1}^{1000}\begin{pmatrix}3&amp; 1 \\0 &amp;
2\end{pmatrix}Q^{-1}\\
AAA\cdots A &amp;=Q\Lambda\Lambda\cdots \Lambda Q^{-1}=
Q\begin{pmatrix}2^{1000} &amp; 0 \\\\0 &amp; 3^{1000}\end{pmatrix}Q^{-1}
\end{align}\]</span> The calculation is extremely simple using EVD.</p>
<h4 id="exercise-1">Exercise</h4>
<ol type="1">
<li>Research how to compute EigenVectors and EigenValues, then
compute<span class="math inline">\(\begin{pmatrix}1 &amp; 2 &amp; 3\\4
&amp; 5 &amp;6\\7 &amp; 8 &amp; 9\end{pmatrix}\)</span>.</li>
<li>Think about the decisive factor affects how many EigenValues we can
get.</li>
</ol>
<h3 id="singular-value-decompositon">Singular Value Decompositon</h3>
<p>Notice that EigenVector Decomposition is applied to decompose square
matrices. Is there any approach to decompose non-square matrices? The
answer is a YES, and the name is Singular Value Decompositon.</p>
<h4 id="intuition-1">Intuition</h4>
<p>First of all, let's take a look at what SVD looks like <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-rectangle1.png" />
From the picture, we can find that matrice <span
class="math inline">\(A\)</span> is decomposed to 3 components: <span
class="math inline">\(U\)</span>, <span
class="math inline">\(\Sigma\)</span> and <span
class="math inline">\(V^{T}\)</span>. <span
class="math inline">\(U\)</span> and<span
class="math inline">\(V^T\)</span> are both sqaure matrices and <span
class="math inline">\(\Sigma\)</span> has the same size as <span
class="math inline">\(A\)</span>. Still, I want to emphasize that <span
class="math inline">\(U\)</span> and<span
class="math inline">\(V^T\)</span> are both unitary matrix, which means
the Determinant of <span class="math inline">\(U\)</span> and <span
class="math inline">\(V^T\)</span> is 1 and <span
class="math inline">\(U^T=U^{-1}\quad V^T=V^{-1}\)</span>.</p>
<h4 id="deduction">Deduction</h4>
<p>In the Linear Transformation section, we can transform a vector to
another coordinate axes. Assume you have a non-square matrice, and you
want to transform A from vectors <span
class="math inline">\(V=(\vec{v_1},
\vec{v_2},\cdots,\vec{v_n})^T\)</span> to antoher coordinate axes which
is <span class="math inline">\(U=(\vec{u_1},
\vec{u_2},\cdots,\vec{u_n})^T\)</span>, the thing is, <span
class="math inline">\(\vec{v_i}\)</span> and <span
class="math inline">\(\vec{u_i}\)</span> have unit length, and all
directions are perpendicular, that is, each of <span
class="math inline">\(\vec{v_i}\)</span> are at right angles to other
<span class="math inline">\(\vec{v_j}\)</span>, we name such matrices as
orthogonal matrices. In addition, I need add a factor <span
class="math inline">\(\Sigma=(\sigma_1,\sigma_2,
\sigma_3,\cdots,\sigma_n)\)</span> which represent the times of each
direction of <span class="math inline">\(\vec{u_i}\)</span>, i.e., We
need transform A from <span class="math inline">\(V=(\vec{v_1},
\vec{v_2},\cdots,\vec{v_n})^T\)</span> to <span
class="math inline">\((\sigma_1 \vec{u_1},\sigma_2 \vec{u_2}, \sigma_3
\vec{u_3},...\sigma_n \vec{u_n})^T\)</span>. From the picture below we
can find that we want to transform from the circle coordinate axes to
the ellipse axes. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-26-circle.png" />
<span class="math display">\[\begin{align}
\vec{v_1} \vec{v_2} \vec{v_3},...\vec{v_n} \qquad\rightarrow \qquad
&amp;\vec{u_1},\vec{u_2},\vec{u_3},...\vec{u_n}\\
&amp;\sigma_1,\sigma_2, \sigma_3,...\sigma_n
\end{align}\]</span></p>
<p>Recall that we can transform <span class="math inline">\(A\)</span>
at every direction, then generate another direction as new coordinate
direction. So we have <span class="math display">\[ A \vec{v_1}=\sigma_1
\vec{u_1}\\
A \vec{v_2}=\sigma_2 \vec{u_2}\\
\vdots\\
A \vec{v_j}=\sigma_j \vec{u_j}\]</span></p>
<p><span class="math display">\[\begin{align}
&amp;\begin{pmatrix}\\\\A\\\\\end{pmatrix}\begin{pmatrix}\\\\
\vec{v_1},\vec{v_2},\cdots,\vec{v_n}\\\\\end{pmatrix}=\begin{pmatrix}\\\\
\vec{u_1}, \vec{u_2},\cdots,\vec{u_n}\\\\ \end{pmatrix}\begin{pmatrix}
\sigma_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_n
\end{pmatrix}\\
&amp;C^{m\times n}\qquad\quad C^{n\times n}\qquad\qquad\qquad C^{m\times
n}\qquad \qquad \qquad C^{n\times n}
\end{align}\]</span> Which is <span class="math display">\[A_{m\times
n}V_{n\times n} = \hat{U}_{m\times n}\hat{\Sigma}_{n\times
n}\]</span></p>
<p><span class="math display">\[\begin{align}
A_{m\times n}V_{n\times n} &amp;= \hat{U}_{m\times
n}\hat{\Sigma}_{n\times n}\\
(A_{m\times n}V_{n\times n}V_{n\times n}^{-1} &amp;= \hat{U}_{m\times
n}\hat{\Sigma}_{n\times n}V_{n\times n}^{-1}\\
A_{m\times n}&amp;=\hat{U}_{m\times n}\hat{\Sigma}_{n\times n}V_{n\times
n}^{-1}\\&amp;=\hat{U}_{m\times n}\hat{\Sigma}_{n\times n}V_{n\times
n}^{T}
\end{align}\]</span></p>
<p>We need do something to the equation in order to continue the
deduction. First we stretch matrice <span
class="math inline">\(\hat{\Sigma}\)</span> vertically to <span
class="math inline">\(m \times n\)</span> size. Then stretch <span
class="math inline">\(\hat{U}\)</span> horizonly to <span
class="math inline">\(m\times m\)</span>, we can set any value to the
right entries. <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-27-RSVD.png" /></p>
<p>Due to the fact we need calculate <span
class="math inline">\(U^{-1}\)</span> and <span
class="math inline">\(V^{-1}\)</span>, the equation is adjusted to <span
class="math display">\[A_{m\times n} = U_{m\times m}\Sigma_{m\times
n}V^T_{n\times n}\]</span> For furture convenience, we need sort all
<span class="math inline">\(\sigma s\)</span>, which means: <span
class="math display">\[\sigma_1\geq\sigma_2\geq\sigma_3 \geq\cdots\geq
\sigma_m\]</span>. #### How to calculate <span
class="math inline">\(U\)</span>, <span
class="math inline">\(V^T\)</span> and <span
class="math inline">\(\Sigma\)</span> To Decompose matrice <span
class="math inline">\(A\)</span>, we need calculate <span
class="math inline">\(U\)</span>, <span
class="math inline">\(V^T\)</span> and <span
class="math inline">\(\Sigma\)</span>. Remember that <span
class="math inline">\(U^T = U^{-1}\)</span> and <span
class="math inline">\(V^T = V^{-1}\)</span>, we will use the property
next.</p>
<p><span class="math display">\[\begin{align}
A &amp;= U\Sigma V^T\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
AA^T&amp;=U\Sigma V^T(U\Sigma V^T)^T\\
&amp;=U\Sigma V^TV\Sigma^T U^T\\
&amp;=U\Sigma V^{-1}V\Sigma^T U^T\\
&amp;=U\Sigma I\Sigma^T U^T\\
&amp;=U\Sigma^2 U^T
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
(AA^T)U&amp;=(U\Sigma^2 U^T)U\\
&amp;=(U\Sigma^2 )U^{-1}U\\
&amp;=U\Sigma^2
\end{align}\]</span></p>
<hr />
<p><span class="math display">\[\begin{align}
A^TA
&amp;=(U\Sigma V^T)^TU\Sigma V^T\\
&amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T U^TU\Sigma V^T\\
&amp;=V\Sigma^T U^{-1}U\Sigma V^T\\
&amp;=V\Sigma^T I\Sigma V^T\\
&amp;=V\Sigma^2 V^T\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}(A^TA)V&amp;=(V\Sigma^2
V^T)V\\
&amp;=(V\Sigma^2)V^{-1}V\\
&amp;=V\Sigma^2
\end{align}\]</span></p>
<h3 id="image-compression">Image Compression</h3>
<p>Firstly, let's look at the process of compressing a picture, the left
picture is original grayscale image. On the right, under different
compress rate, we can see pictures after reproducing. Before compress,
the size of the picture is 1775K byte. Then the picture is almost the
same, when we compress which into 100K byte size, which means we can
save 90% storage space <img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-29-image.gif" /></p>
<p>To compress a piture, you just decompose the matrice through SVD,
then instead of using the original <span
class="math inline">\(U_{m\times m}\)</span>, <span
class="math inline">\(\Sigma_{m\times n}\)</span> and <span
class="math inline">\(U_{n\times n}\)</span>, we shrink every matrice to
new size <span class="math inline">\(U_{m\times r}\)</span>, <span
class="math inline">\(\Sigma_{r\times r}\)</span> and <span
class="math inline">\(U_{r\times n}\)</span>. The final <span
class="math inline">\(size(R)\)</span> is still <span
class="math inline">\(m\times n\)</span>, but we abandon some entries
since these entries are not so important than these we have reserved.
<img
src="http://cmb.oss-cn-qingdao.aliyuncs.com/2017-04-30-rect.png" /></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%% octave: core code of svd compressions</span><br><span class="line">X = imread(filename);  </span><br><span class="line">[U S V] = svd(double(X));</span><br><span class="line">R = U(:,1:r)*S(1:r,1:r)*V(:,1:r)&#x27;;    </span><br></pre></td></tr></table></figure>
<h3 id="summary">Summary</h3>
<p>Today we have learned mathmatics backgroud on SVD, including linear
transformation and EigenVector&amp;EigenVaule. Before SVD, we first
talked about EigenValue Decomposition. Finally, Singular Vaule
Decomposition is very easy to be deduced. In the last section, we took
an example see how SVD be applied to image compression field.</p>
<p>Now, it comes to the topic how to save our storage of a 32G iPhone7,
the coclusion is obvious: using SVD compress image to shrink the size of
our photos.</p>
<h3 id="reference">Reference</h3>
<ol type="1">
<li>https://www.youtube.com/watch?v=EokL7E6o1AE</li>
<li>https://www.youtube.com/watch?v=cOUTpqlX-Xs</li>
<li>https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw</li>
<li>https://yhatt.github.io/marp/</li>
<li>https://itunes.apple.com/cn/itunes-u/linear-algebra/id354869137</li>
<li>http://www.ams.org/samplings/feature-column/fcarc-svd</li>
<li>https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class=""></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Mingbo Cheng</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"chengmingbo/gitment-comments","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
